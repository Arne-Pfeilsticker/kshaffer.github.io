{
    "db": [
        {
            "data": {
                "posts": [
                    {
                        "id": 0,
                        "title": "Hypothes.is as a public research notebook",
                        "slug": "hypothesis-public-research-notebook",
                        "markdown": "\nCan we do scholarly work in public without relying on for-profit \"social\" platforms?\n\nThis is a big question I've been mulling over lately, especially as my new position at the University of Mary Washington will involve significant work on the [Domain of One's Own](http://umw.domains/) project. I won't tackle all the implications of that question in a single blog post, but I do want to address one aspect of it: can we share things of value we discover \u2015 and our thoughts about them \u2015 without providing free labor or content to for-profit tech corporations? Even better, can we do so without everyone becoming a coder, web designer, and server administrator?\n\n## hypothes.is\n\nOne tool that can help us accomplish this is [hypothes.is.](https://hypothes.is) Hypothes.is is an open annotation tool for the web, allowing anyone to highlight, annotate, or comment on any webpage via a Chrome plugin (web developers can also install it on their sites, like I have with my blog). It's similar to how Medium users can annotate and highlight blog posts on that platform, but you can use the hypothes.is plugin on any website. It's important to note that hypothes.is users don't alter the original website. Rather the hypothes.is plugin adds an annotation/highlighting layer *over* the webpage that only hypothes.is users can see, and hypothes.is users can toggle that annotation/highlighting layer on and off as they like. Here's a video intro to the hypothes.is project:\n\n<iframe src=\"https://player.vimeo.com/video/29633009\" width=\"640\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n\nMost discussion I've seen about hypothes.is has centered around this feature of page annotation, often discussing ways in which, say, [students in a class can collaboratively annotate](https://hypothes.is/quick-start-guide/) a single page [as they study it together](http://acdigitalpedagogy.org/category/hypothes-is/). However, hypothes.is has other, lesser known features that, in my mind, are both more interesting and more powerful.\n\n## The hypothes.is stream\n\nWhat I think makes hypothes.is really interesting is the [stream.](https://hypothes.is/stream) Here you can find every public annotation in reverse time order (like Twitter). There is even an RSS feed for this stream. Of course, most people don't want to read *every* annotation, so you can limit the results by user, tag, or search content. You can also send it simple queries in the URL. (Read more about these queries [here.](https://hypothes.is/for-publishers/))\n\nFor example, here are [all of my public annotations](https://hypothes.is/stream?q=user:kris.shaffer). And here are all public annotations tagged [IndieWeb](https://hypothes.is/stream?q=tag:IndieWeb).\n\nThis means that scholars and students working in public can share the online resources they've found valuable, as well as their reactions to it, while using an \"indie\" tool, and without installing anything more complicated than a browser plugin. (Though, at least for now, that plugin means relying on Google, but one step at a time.) It also means that we can follow each other's work, as well as connect with others working on similar topics (via tags).\n\n\n## The hypothes.is API\n\nThere are some limitations to the hypothes.is stream. Which, of course, is understandable given how young the tool is, and how much focus the annotation of individual pages has received, rather than the potential of the stream.\n\nUsers cannot directly download their annotations. As far as I can tell, they all live on the hypothes.is server. It also seems as if it's not possible to combine search queries in the stream... say, all annotations by me *and* tagged IndieWeb. Or all annotations tagged with a class tag and a topical tag. (Someone please correct me if I'm wrong about these things, as I'd love to be able to do them!)\n\nHowever, since hypothes.is is committed to supporting open work and seems to have an interest in collaborating with other developers and researchers, they have a fairly robust API (application programming interface). While most users will not tap into the API directly, it is possible for other IndieWeb and IndieEdTech developers to create tools that interact with and build on hypothes.is. Perhaps creating a WordPress plugin that will take all of a user's annotations and collect them in a page on their website, integrated with the site's theming. Or creating a program that will automatically publish all of a user's annotations as bookmarks on their [Known](https://withknown.com) site, and maybe even cross-post those to Twitter. Things like these would make it even more powerful for non-coders interested in indie and non-profit tools. (These are some of the projects I hope to work on in my new role at UMW.)\n\nOne common drawback to IndieWeb and open-source tools is that they tend to require a significant technological background in order to use. So I like it a lot when someone makes a tool that is easy for most people to use, but also readily \"hackable\" for those of us who want to get under the hood and see how we might extend it. I'm excited to see what else it can do, and what cool things I can do with its API.\n\nI'll keep you posted... :)\n\n## Update\n\nSince writing this post, I've been digging into the hypothes.is API and doing some work. I've created a Python module called [Pypothesis](https://github.com/kshaffer/pypothesis), which makes for fairly easy interaction with the hypothes.is API in a programming environment \u2015 at least the GET part of the API, which fetches public annotations without needing to authenticate. Also included with Pypothesis is [a script](https://github.com/kshaffer/pypothesis/blob/master/hypothesisToJekyll.py) that will download a user's annotations, annotations with a particular tag, or both, then extract the important information from them and write them to a simple MarkDown format. The output is a well-formed [Jekyll-friendly](https://jekyllrb.com/) page, which makes for easy use if you have [a blog hosted on GitHub](https://pages.github.com/). But the MarkDown can also be converted to HTML with a program like [PanDoc](http://pandoc.org/) or [MultiMarkDown](http://fletcherpenney.net/multimarkdown/), or pasted right into WordPress, [if you enable MarkDown](https://en.support.wordpress.com/markdown/). I'll be blogging in more detail about this Python module and future derivative projects soon, but for now, check it out on GitHub and [read the (fairly detailed) documentation](https://github.com/kshaffer/pypothesis/blob/master/README.md) if you'd like to give it a whirl.\n",
                        "html": "",
                        "image": "/content/images/trooperbook.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Hypothes.is as a public research notebook",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-04-30 16:25:00 -0600",
                        "created_by": 1,
                        "updated_at": "2016-04-30 16:25:00 -0600",
                        "updated_by": 1,
                        "published_at": "2016-04-30 16:25:00 -0600",
                        "published_by": 1,
                        "og_title": "Hypothes.is as a public research notebook",
                        "twitter_title": "Hypothes.is as a public research notebook",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/trooperbook.jpg",
                        "twitter_image": "/content/images/trooperbook.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Academic freedom is for students, too",
                        "slug": "academic-freedom-is-for-students",
                        "markdown": "\nWho decides what teachers teach? More importantly, who decides what students learn?\n\n[Cedar Riener\u2019s recent article for EML](http://modernlearners.com/beyond-academic-freedom-and-dignity/) unpacks some of the implications of academic freedom in the classroom. He cites the [AAUP\u2019s statement on academic freedom](http://www.aaup.org/report/1940-statement-principles-academic-freedom-and-tenure), which states that \u201c[t]he common good depends upon the free search for truth and its free exposition.\u201d Riener writes, \u201cAcademic freedom is not only freedom to pursue controversial topics, but it defines my job as a creative craft.\u201d This creativity and the experimentation that comes with it necessarily lead to inefficiency and a lack of standardization. But the positive side is that providing instructors freedom in their choice of content and pedagogical approach leads to attraction of better teachers, more improvement on the part of the experimenting teachers, and increased student engagement with a teacher motivated by his or her instructional freedom.\n\nThese are excellent points, and I wholeheartedly support the idea that academic freedom is about teaching as least as much as it is about research. As the [teacher-accountability movement](https://www.insidehighered.com/news/2014/11/19/performance-based-funding-provokes-concern-among-college-administrators) is [turning its gaze on higher education](https://www.insidehighered.com/news/2014/12/05/second-higher-ed-summit-obama-administration-mixes-praise-and-accountability), and the US Department of Education is proposing a one-size-fits-all [ranking system](https://www.insidehighered.com/sites/default/server_files/files/ratings%20framework%20draft.pdf) for effective and efficient universities, these are important ideas to fight for.\n\nBut this view of academic freedom affecting research and teaching still stops short of where I believe it should extend. Riener\u2019s view, which is supported by many thoughtful pedagogues, is still faculty-centered. But in my view, the most important aspect of pedagogical academic freedom is the freedom of the student.\n\nDuring a Twitter chat among a number of university faculty last year, the topic of academic freedom came up. One participant asked how we thought our students would define academic freedom. So I asked some of my students. Their responses were eye-opening.\n\nOne student tweeted back that academic freedom means \u201cbeing able to learn in an environment that promotes individual progress so as to be able to move together as a class.\u201d\n\nAnother defined academic freedom as \u201cthe ability to TEACH in a unique and fulfilling way, so that your students can learn in a way that benefits them most directly. Academia should be about enhancing the process of learning & teaching, not just curriculum.\u201d\n\nStill another: \u201cthe ability to learn rather than cram and forget before a test. Freedom to learn rather than focus on grades.\u201d\n\nFinally: \u201cI consider it a privilege and NOT something to be taken for granted. It\u2019s free from \u2018standardization.\u2019\u201d\n\nNote several trends in these responses.\n\nFirst, *not a single student mentioned faculty research*. Of course, these were students of mine from my time at a teaching-oriented university, so that might be different at an research-based institution. But I would wager that few undergraduates from research universities would think much differently about academic freedom from undergraduates at a teaching university.\n\nSecond, where the freedom of faculty is mentioned, it is with the goal of learner benefits in mind. Those benefits are not a by-product of instructor freedom.\n\nThird, academic freedom is not simply about course content. It is about a \u201cprocess of learning,\u201d and a lack of standardization, not an optimal selection and ordering of curricular topics. In fact, as the third student noted, academic freedom means freedom from the tyranny of content, or more specifically, freedom from the short-term memorization of instructor-chosen information under the threat of grade-based punishment.\n\nLastly, and most importantly, while faculty freedom is mentioned, it is the freedom of the *student* that underlies all of these comments. There was absolutely no hint in these comments that academic freedom would only apply to faculty. And why should it?\n\nOf course, these are only responses from four students. And they are four students who took between 8 and 16 credit hours with a budding critical pedagogue who gave them an increasing amount of freedom over the course of their studies. But the idea that academic freedom applies to students is not a unique one, even if it is often left out of contemporary discussions on education.\n\nCritical pedagogues like Paulo Freire, Henry Giroux, bell hooks, and a wide variety of authors at [Hybrid Pedagogy](http://www.hybridpedagogy.com/) emphasize the freedom of the student. In fact, the philosophy of critical pedagogy holds that student freedom is not a means to an end of \u201ceffective\u201d learning. For a critical pedagogue, student freedom, and the ability to wield it mindfully, is the *goal* of education.\n\nThe Center for Applied Special Technology also advocates student freedom in their expression of the principles of [Universal Design for Learning](http://www.cast.org/udl/). Assuming a diversity of student backgrounds, intelligences, motivations, goals, and even neurological structures, UDL advocates for an educational environment in which multiple means of representation, action, expression, engagement, and assessment is the norm, with support for student choice.\n\nBut student freedom is an old idea, as well. The Greek origin of *school* is *schole*, meaning leisure. This reflects an older idea of school as a place where students were given not only instruction, but time and space in which to contemplate the good. Recent American models of school, however, have shifted from Greek *schole* of leisure and freedom to the Middle Dutch *schole*. This *schole*, from which we get the English *shoal* or *school* of fish, means *troop*, and applies to a homogenous group. Our modern schools often take on this homogenous, industrialist, even militaristic model, at the expense of student freedom and diversity. While it may be more \u201cefficient\u201d at communicating facts and meeting standards, the lack of intellectual freedom and diversity harms students \u2014 and therefore society \u2014 in the long run.\n\nWhatever one\u2019s philosophy, it is clear that the three benefits Riener ascribes to *faculty* academic freedom apply to students as well. Just as pedagogical freedom makes teaching positions more attractive, intellectual freedom makes academic study more attractive \u2014 something that can lead both to more engaged learners and, as Riener emphasizes in regards to faculty, easier recruitment of students in an admissions setting. Second, Riener writes that \u201cfreedom is necessary for long-term improvement and excellence.\u201d This is as true for students as it is for teachers. And finally, Riener points out the increased passion and inspiration that a teacher brings to their teaching when they act \u201cof their own free will\u201d and make their own pedagogical decisions. This is no less true of students. As a music professor, I see regularly the passion and focus with which student performers, for example, engage their vocal and instrumental practice \u2014 often in striking disproportion to their studies for institutionally required courses, even when those courses are worth more credit hours and/or are harder places to achieve high grades than their lessons. This is not a student flaw, it is a human trait \u2014 one we as faculty and administrators can do a better job of harnessing.\n\n[Jennifer Hardwick writes](http://www.hybridpedagogy.com/journal/safe-space-dangerous-ideas-dangerous-space-safe-thinking/), \u201cone must be willing to leave the security of assuredness and embrace the fact that learning can be a difficult and even painful process that shakes your foundations, changes you, and transforms the way you see the world.\u201d In light of this, she asks, \u201chow can I create a safe space for dangerous ideas, and a dangerous place for safe thinking?\u201d If we want education to transform the way our students see the world, we must set aside the safe, but flawed, thinking of curriculum as content \u2014 pedagogy as the careful, logical ordering of facts through which we lead our students. Transformative education is dangerous. It allows teachers, and students, the freedom to fail\u2026and the freedom to be brilliant. This is education. Not the universal meeting of a preset standard, nor the study of the brilliance of dead celebrities. Rather, education is the empowering of students to push against the standards, to discover and cultivate their own brilliance. To learn what it means to be free.\n\n*This article was originally published in [Educating Modern Learners](http://www.modernlearners.com/).*\n",
                        "html": "",
                        "image": "/content/images/freedom.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Academic freedom is for students, too",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-04-13 20:39:54 -0600",
                        "created_by": 1,
                        "updated_at": "2015-04-13 20:39:54 -0600",
                        "updated_by": 1,
                        "published_at": "2015-04-13 20:39:54 -0600",
                        "published_by": 1,
                        "og_title": "Academic freedom is for students, too",
                        "twitter_title": "Academic freedom is for students, too",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/freedom.jpg",
                        "twitter_image": "/content/images/freedom.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Educational fallacies: post hoc, propter hoc",
                        "slug": "educational-fallacies-post-hoc-propter-hoc",
                        "markdown": "\n*This weekend, I read [a recent blog post at the Chronicle of Higher Education](http://chronicle.com/blogs/conversation/2014/08/05/the-rise-of-the-helicopter-teacher/) by Steven Conn on the \"helicopter teacher.\" Conn's post is primarily a rant about what many might call \"soft\" teaching practices. It is filled with hyperbole and fallacies, and lacking both in nuance and in knowledge of the scholarly literature on teaching. I've decided not to post a direct response to such an argument. However, Conn's post, and many of the comments and discussions that have come in response to his post, expose a number of rampant fallacies in the way that we think about teaching, especially in higher education. So I've decided to write a series of blog posts on educational fallacies\u2014things like \"I covered it, I assessed it, therefore my students learned it,\" \"a hard class is a rigorous class is a good class,\" or \"what's good for the average is good for all.\" This is my first post in this series, addressing the fallacy:*\n\n> \"I did something; students learned something. Therefore, what I did caused the learning.\"\n\nThis fallacy, as I've heard it, usually comes from \"traditional\" faculty resistant to new methods, even when demonstrated in formal, peer-reviewed research. It goes something like \"I've been teaching this way for *x* years, and my students turn out just fine.\" (Note the hint of survivor's bias.)\n\nNow, informal experimentation happens all the time in teaching. It has to. And to some extent, academic freedom is about such curriculural experimentation. But that isn't what I'm talk about here.\n\nIn fact, I'm all for making changes semester-to-semester, year-to-year, and gauging the results. We can discover many wonderful new methods without an IRB-approved study by simply trying something new, and comparing the results to what we did the year before. More experimental rigor may be required before making large, generalizable claims. But most thoughtful teachers have improved their craft and their service to their students quite legitimately through such experimentation in their teaching.\n\nHowever, the resistance to try something new simply because what we've always been doing produces good results is fallacious. Specifically, this is *post hoc, propter hoc* logic\u2014the idea that when one thing is followed by another thing, that first occurrence caused the second. Simply because I teach in a certain way and my students demonstrate significant knowledge at the end of the course does not mean that my teaching led to their learning. We can all think of times as students when we learned the content of a course in a deep way *in spite of* the poor instruction we received. (We simply cared too much not to work hard enough to teach ourselves and/or learn from our peers.) We need the humility to recognize this possibility as teachers.\n\nAs in most examples of *post hoc, propter hoc*, there is more than one potential contributing factor to the result of student learning: the instructor's efforts, of course, but also the students' efforts, the textbook (and/or supporting media), the students' motivation, the level of interest and intelligence of fellow students, the location and time of the class, and\u2014of course\u2014the knowledge and abilities of students when they *enter* the class. Without accounting for these other factors, we cannot claim a causal relationship without major caveats and equivocations.\n\nSo when we find ourselves saying something like \"I've been doing it this way for a long time, and it always gets good results,\" or \"this is how my professor did it, and it worked great for me,\" let's check ourselves. Do we know the real cause of our students' success? Have we accounted for other factors?\n\nOr are we arrogantly assuming that our students' learning is primarily due to *our* instructional prowess?\n",
                        "html": "",
                        "image": "",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Educational fallacies: post hoc, propter hoc",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2014-08-11 09:50:16 -0600",
                        "created_by": 1,
                        "updated_at": "2014-08-11 09:50:16 -0600",
                        "updated_by": 1,
                        "published_at": "2014-08-11 09:50:16 -0600",
                        "published_by": 1,
                        "og_title": "Educational fallacies: post hoc, propter hoc",
                        "twitter_title": "Educational fallacies: post hoc, propter hoc",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "",
                        "twitter_image": ""
                    },
                    {
                        "id": 0,
                        "title": "The Flipped Classroom: reading list",
                        "slug": "the-flipped-classroom-reading-list",
                        "markdown": "\nHere is the reading list for [The Flipped Classroom](http://www.digitalpedagogylab.com/blog/course/the-flipped-classroom/), a three-week, intensive, online course I'm teaching for Digital Pedagogy Lab beginning on July 19. I'm still building the private Canvas site and discussion forum that we'll be using for the course, so I'm offering the reading list here for participants who want to get an early start, or for others who simply want to see what we'll be reading. For more information about the course or to register, see [the course web page](http://www.digitalpedagogylab.com/blog/course/the-flipped-classroom/). I've also posted [a draft syllabus](http://kris.shaffermusic.com/2015/06/syllabus-for-the-flipped-classroom-at-hybrid-pedagogy-courses/) here on my blog.\n\nWhile this looks like a lot (!), please note that this list contains many short pieces. The estimated total is around 65\u201370 pages per week, not counting the optional readings. (Not all web content is paginated.)\n\nAlso, note that some of these resources are behind a paywall. I am happy to provide a copy of any paywalled resources for study purposes to enrolled participants in the course, if you do not have institutional access. Simply email me at [kris@hybridpedagogy.org](mailto:kris@hybridpedagogy.org) to request a PDF. I'll also post them to our Canvas site once I have that up and running.\n\nIf you have questions about the course, please email me or \u2014 better yet \u2014 send me a tweet at [@krisshaffer](http://twitter.com/krisshaffer). Feel free to use the course hashtag: #DPLFlipClass.\n\n# Week 1 readings/videos\n<hr/>\n\n<iframe src=\"https://player.vimeo.com/video/133381238\" width=\"500\" height=\"281\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> <p><a href=\"https://vimeo.com/133381238\">The Flipped Classroom - introduction</a> from <a href=\"https://vimeo.com/user11692346\">Kris Shaffer</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>\n\n## Overview\n\n- Schell, Julie. 2013. [\"What is a flipped classroom? (in 60 seconds).\"](http://blog.peerinstruction.net/2013/04/22/what-is-a-flipped-classroom-in-60-seconds/) *Turn to Your Neighbor: The Official Peer Instruction Blog.*  \n- Schell, Julie. [\"Student-Centered University Learning.\"](http://revista.drclas.harvard.edu/book/student-centered-university-learning) In *ReVista: Harvard Review of Latin America.*  \n- Bruff, Derek. 2012. [\"The Flipped Classroom FAQ.\"](http://www.cirtl.net/node/7788) The Center for the Integration of Research, Teaching and Learning.  \n- Lage et al. 2000. [\"Inverting the Classroom: A Gateway to Creating an Inclusive Learning Environment.\"](http://www.jstor.org/stable/1183338) In *The Journal of Economic Education* 31/1, 30\u201343.  \n- Shaffer, Kris and Bryn Hughes. 2013. [\"Flipping the Classroom: Three Methods.\"](http://flipcamp.org/engagingstudents/shafferintro.html) In *Engaging Students: Essays in Music Pedagogy* 1.  \n- Duker et al. 2015. [\"Hacking the Music Theory Classroom: Standards-Based Grading, Just-in-Time Teaching, and the Inverted Class.\"](http://www.mtosmt.org/issues/mto.15.21.1/mto.15.21.1.duker_gawboy_hughes_shaffer.html) In *Music Theory Online* 21/1. *Sections 3.1\u20133.6 only.*  \n- Vliet, et al. 2015. [\"Flipped-Class Pedagogy Enhances Student Metacognition and Collaborative-Learning Strategies in Higher Education But Effect Does Not Persist.\"](http://www.lifescied.org/content/14/3/ar26.full) In *Life Sciences Education* 14, 1\u201310.\n\n### Optional further reading\n\n- Educause, [\"7 things you should know about flipped classrooms\"](https://net.educause.edu/ir/library/pdf/eli7081.pdf)  \n- Shaffer, Kris. 2015. \"The flip that matters.\" (not yet published; GDrive link will be provided)  \n- Shaffer, Kris. 2015. [\"The flipped classroom: six myths.\"](http://kris.shaffermusic.com/2015/07/the-flipped-classroom-six-myths/)  \n\n## Microlectures\n\n- Educause, [\"7 things you should know about screencasting.\"](https://net.educause.edu/ir/library/pdf/ELI7012.pdf)  \n- Talbert, Robert. 2011. [\"How I make screencasts: Chapter 0.\"](http://chronicle.com/blognetwork/castingoutnines/2011/02/28/how-i-make-screencasts-chapter-0/) *Casting Out Nines* blog on the *Chronicle of Higher Education* Blog Network.  \n- Talbert, Robert. 2011. [\"How I make screencasts: The planning phase.\"](http://chronicle.com/blognetwork/castingoutnines/2011/03/02/how-i-make-screencasts-the-planning-phase/) *Casting Out Nines* blog on the *Chronicle of Higher Education* Blog Network.  \n- Gawboy, Anna. 2014. [Notes on video microlectures.](http://hackingmusictheory.github.io/gawboy.html) *Please stop at \"Theory I Sample Module.\"*  \n\n### Optional further reading\n\n- Talbert, Robert. [Posts tagged \"screencasting.\"](http://chronicle.com/blognetwork/castingoutnines/category/technology/screencasts/) *Casting Out Nines* blog on the *Chronicle of Higher Education* Blog Network.  \n\n\n## Just-in-time teaching\n\n- Duker et al. 2015. [\"Hacking the Music Theory Classroom: Standards-Based Grading, Just-in-Time Teaching, and the Inverted Class.\"](http://www.mtosmt.org/issues/mto.15.21.1/mto.15.21.1.duker_gawboy_hughes_shaffer.html) In *Music Theory Online* 21/1. *Sections 4.1\u20134.10 only.*  \n- Dobson, John L. 2008. [\"The use of formative online quizzes to enhance class preparation and scores on summative exams.\"](http://advan.physiology.org/content/32/4/297) In *Advances in Physiology Education* 32/4, 297\u2013302.  \n- Kibble, Jonathan. 2007. [\"Use of unsupervised online quizzes as formative assessment in a medical physiology course: effects of incentives on student participation and performance.\"](http://advan.physiology.org/content/31/3/253) In *Advances in Physiology Education* 31/3, 253\u201360.  \n\n### Optional further reading\n\n- Kibble, Jonathan. 2010. [\"Voluntary participation in online formative quizzes is a sensitive predictor of student success.\"](http://advan.physiology.org/content/35/1/95) In *Advances in Physiology Education* 35/1, 95\u201396.  \n\n\n\n# Week 2 readings\n<hr/>\n\n## Peer instruction & clickers\n\n- Mazur, Eric. [\"Confessions of a Converted Lecturer.\"](https://www.youtube.com/watch?v=WwslBPj8GgI)  \n- Schell, Julie. 2013. [\"From Flipped Classrooms to Flipping with Peer Instruction.\"](http://blog.peerinstruction.net/2013/11/04/from-flipped-classrooms-to-flipping-with-peer-instruction/) In *Turn to Your Neighbor: The Official Peer Instruction Blog*.  \n- Mazur, Eric and Jessica Watkins. 2009. [\"Just-in-Time Teaching and Peer Instruction.\"](http://mazur.harvard.edu/publications.php?function=display&rowid=634) In *Just-in-time Teaching: Across the Disciplines, Across the Academy*, 39\u201362.  \n- Deslauriers, et al. 2011. [\"Improved Learning in a Large-Enrollment Physics Class.\"](http://www.sciencemag.org/content/332/6031/862.abstract) In *Science* 332/6031, 862\u201364.  \n- Caldwell, Jane E. 2007. [\"Clickers in the Large Classroom: Current Research and Best-Practice Tips.\"](http://www.lifescied.org/content/6/1/9.full) In *Life Sciences Education* 6/1, 9\u201320.  \n- Crouch, Catherine H. and Eric Mazur. 2001. [\"Peer Instruction: Ten years of experience and results.\"](http://web.mit.edu/jbelcher/www/TEALref/Crouch_Mazur.pdf) In *American Journal of Physics* 69/9, 970\u201377.  \n\n\n### Optional further reading\n\n- Bunce, et al. 2010. [\"How Long Can Students Pay Attention in Class? A\nStudy of Student Attention Decline Using Clickers.\"](http://pubs.acs.org/doi/abs/10.1021/ed100409p) In *Journal of Chemical Education* 87/12, 1438\u201343.  \n- [\"Clicker Resource Guide: An Instructors Guide to the Effective Use of Personal Response Systems (Clickers) in Teaching\"](http://www.colorado.edu/sei/documents/clickeruse_guide0108.pdf) from CU & UBC.  \n\n\n## Inquiry-driven learning\n\n- Musallam, Ramsey. 2013. [\"A pedagogy-first approach to the flipped classroom.\"](http://www.cyclesoflearning.com/learning--instruction/a-pedagogy-first-approach-to-the-flipped-classroom) In *Cycles of Learning* blog.  \n- Colletti, Carla R. 2013. [\"The Silent Professor: enhancing Student Engagement through the Conceptual Workshop.\"](http://flipcamp.org/engagingstudents/colletti.html) In *Engaging Students: Essays in Music Pedagogy* 1.  \n\n### Optional further reading\n\n- Karplus, Robert. 1977. [\"Science Teaching and the Development of Reasoning.\"](http://onlinelibrary.wiley.com/doi/10.1002/tea.3660140212/abstract) In *Journal of Research in Science Teaching* 14/2, 169\u201375.\n\n## Problem-based learning\n\n- Vega, Vanessa. 2012. [\"Project-Based Learning Research Review: Evidence-Based Components of Success.\"](http://www.edutopia.org/pbl-research-evidence-based-components) In *Edutopia*.  \n- Donaldson, Jonan. 2014. [\"The Maker Movement and the Rebirth of Constructionism.\"](http://www.hybridpedagogy.com/journal/constructionism-reborn/) In *Hybrid Pedagogy*.  \n- Shaffer, Kris. 2014. [\"Assessing Problem-Based Learning.\"](http://flipcamp.org/engagingstudents2/essays/shaffer.html) In *Engaging Students: Essays in Music Pedagogy*.  \n- Strobel, Johannes and Angela van Barneveld. 2009. [\"When is PBL More Effective? A Meta-synthesis of Meta-analyses Comparing PBL to Conventional Classrooms.\"](http://docs.lib.purdue.edu/ijpbl/vol3/iss1/4/) In *Interdisciplinary Journal of Problem-Based Learning* 3/1, 44\u201358.  \n- Albanese, Mark A. and Susal Mitchell. 1993. [\"Problem-based Learning: A Review of Literature on Its Outcomes and Implementation Issues.\"](http://journals.lww.com/academicmedicine/Abstract/1993/01000/Problem_based_Learning__A_Review_of_Literature_on.20.aspx) In *Academic Medicine* 68/1, 52\u201381.  \n- Berkson, Laeora. 1993. [\"Problem-Based Learning: Have the Expectations Been Met?\"](http://journals.lww.com/academicmedicine/Abstract/1993/10000/Problem_based_learning__have_the_expectations_been.53.aspx) In *Academic Medicine* 68/10, 579\u201388.\n\n### Optional further reading\n\n- [\"Resources for Problem-Based Learning\"](http://www.udel.edu/inst/resources/index.html), University of Delaware, Institute for Transforming Undergraduate Education.  \n- Duker et al. 2014. [\"Problem-Based Learning in Music: A Guide for Instructors.\"](http://flipcamp.org/engagingstudents2/essays/dukerShafferStevens.html)  \n",
                        "html": "",
                        "image": "/content/images/bookbind.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "The Flipped Classroom: reading list",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-07-10 10:59:55 -0600",
                        "created_by": 1,
                        "updated_at": "2015-07-10 10:59:55 -0600",
                        "updated_by": 1,
                        "published_at": "2015-07-10 10:59:55 -0600",
                        "published_by": 1,
                        "og_title": "The Flipped Classroom: reading list",
                        "twitter_title": "The Flipped Classroom: reading list",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/bookbind.jpg",
                        "twitter_image": "/content/images/bookbind.jpg"
                    },
                    {
                        "id": 0,
                        "title": "A journey through API programming \u2015 Part 1: What is an\u00a0API?",
                        "slug": "journey-through-api-programming-1",
                        "markdown": "\nAn API is one of the most powerful and essential tools for web programming today. And yet, as I\u2019ve been building my skills as a web developer lately, I\u2019ve had a hard time finding a decent tutorial on how to use APIs, especially when authentication is involved. After a lot of digging \u2015 and banging my head against various metaphorical walls! \u2015 I\u2019ve got a handle on basic API programming. I even built [a WordPress plugin](http://umwdtlt.com/hypothesis-aggregator-wordpress-plugin/) that makes use of [an external API](https://h.readthedocs.io/en/latest/api/) to collect its content.\n\nNow I\u2019m exploring some of the more intermediate and advanced aspects of API programming, and I\u2019ve decided to blog my way through it, in the hopes that it will help others who want to go down this path. (It will also probably help me solidify concepts in my own mind, as well as force myself to document the process in a more organized way, which will help me down the road.)\n\nSpecifically, I\u2019m going to explore using [Medium\u2019s public API](https://medium.com/blog/the-medium-api-is-now-open-to-everyone-3f4642e5c850#.tuff77m2s) and integrating it into some of my current/recent ed-tech projects. If that sounds like something you\u2019re interested in exploring, feel free to follow along! And please feel free to annotate these posts (using Medium\u2019s native annotation tools, or the non-profit annotation tool [hypothes.is](https://hypothes.is/) \u2015 also the basis of that plugin I mentioned!), especially if you have links or tips to share!\n\nI\u2019m going to aim this series of posts at readers who are relatively new to programming. APIs, like frameworks, can allow a relatively novice coder to do some really powerful things. In fact, I think APIs are a great way [to introduce some really cool things you can do with code](http://kris.shaffermusic.com/codingforteachers/) to people who are new to coding, while giving them both motivation and a framework of understanding for those foundational concepts like variables, objects, loops, and functions that they may still be working on mastering. I also want to recognize that even people who have been coding for a while may be not be an expert in *my* language of choice. So I don\u2019t want to use examples in Python, JavaScript, or whatever that are way beyond graspability for that PHP master who is still a JavaScript novice. That said, if you\u2019re a coder who is still trying to figure out this API thing, hopefully my tutorials will be of use.\n\nI imagine this will be a several-part series of posts, exploring APIs in general, requesting data from APIs (and doing something with that data), authenticating with an API server, posting data over an API, and putting it all together into an API-based web app. And please feel free to leave a comment if you have suggestions or questions about other topics.\n\nLet\u2019s get started!\n\n## So what is an API,\u00a0anyway?!\n\nAPI stands for Application Programming Interface. Let\u2019s use Medium as an example. All of the content on Medium exists on a server (or, probably, a big bank of networked servers), and a server is just a computer that serves up data over the internet. When I want to read something on Medium, I open my browser, go to Medium.com, and use their Graphical User Interface (GUI, often pronounced \u201cgooey\u201d) to interact with the server \u2015 getting, posting, editing, and deleting data.\n\nHowever, if I write a *program* that I want to interact with my data on Medium, I don\u2019t tell it to open the browser, click here and there, and read/post/edit/publish. Instead, I write code that interacts with the server via the API. The API allows for the same kinds of interactions (\u201cmethods\u201d) \u2015 GETting, POSTing, DELETEing, etc. \u2015 but via an interface that is designed for interaction with another app, not with a human user.\n\n<img src=\"/content/images/apiIllustration.png\" style=\"width: 100%;\" alt=\"A GUI is for humans, an API is for\u00a0apps.\" />\n\n*A GUI is for humans, an API is for\u00a0apps.*\n\n\nThat\u2019s pretty much it. At least on a conceptual level. All the details about how APIs work just come out of a desire to help apps on different computers/servers talk to each other efficiently. Writing code that makes use of APIs simply requires figuring out how to use an API to connect your app with another that already exists.\n\nNext up,\u00a0\u2026 [**Why APIs?**](http://kris.shaffermusic.com/2016/08/journey-through-api-programming-2/)\n\n*Header photo by* [*r2hox*](https://www.flickr.com/photos/rh2ox/) *(CC BY\u2013SA).*\n",
                        "html": "",
                        "image": "/content/images/apiHeader1.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "A journey through API programming \u2015 Part 1: What is an\u00a0API?",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-08-30 10:02:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-08-30 10:02:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-08-30 10:02:00 -0400",
                        "published_by": 1,
                        "og_title": "A journey through API programming \u2015 Part 1: What is an\u00a0API?",
                        "twitter_title": "A journey through API programming \u2015 Part 1: What is an\u00a0API?",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/apiHeader1.jpg",
                        "twitter_image": "/content/images/apiHeader1.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Educational fallacies: Age is a proxy for intellectual development",
                        "slug": "educational-fallacies-age-is-a-proxy-for-intellectual-development",
                        "markdown": "\n*This is the third in my series of posts on educational fallacies. You can read all of them [here](http://kris.shaffermusic.com/tags/#educational%20fallacies).*\n\nAlmost all educational settings in the US (and much of the rest of the world) group students by age and then teach those age-grouped students as if they were at the same stage in their intellectual and social development. Most of the time these age groups, in K\u201312 at least, are divided such that students are within one year of each other in age. Even private school and homeschool curricula assume that \"first grade\" is a thing and teach to that thing. But does a group of students being the same age mean they are at the same stage in their intellectual development? And do we educate better when students are surrounded by others in the same stage of development anyway?\n\nAs soon as we question these assumptions they fall apart. When we question them, we realize that they have no basis in evidence or in logic. And when we explore alternatives, we quickly find that we are likely hindering our students' education when we attempt to group them by similar developmental stage, and when we use age as a proxy for that development.\n\nThere are numerous factors that contribute to \"developmental appropriateness\" for a particular intellectual or physical task, making age a poor proxy for a student's developmental level: a student's biology and physiology, family context, socio-economic background, medical situation, (multi)linguistic context, past educational experience, classroom environment, adult and peer support, etc. None of these, not even the physiological, will be consistent across an age span, and it is *impossible* that *all* of them would be consistent for a class full of students. Even the ideal route through the material differs for individual students. Thus, taking dozens (hundreds, thousands, millions) of students through the same educational experiences, in the same order, at the same time, according to the number of years (+/\u2013 .5) since exiting the womb, is kind of ridiculous. (Of course, decisions to educate in this way were never based in pedagogical effectiveness, but only in the expediency of industrialized educational institutions \"delivering\" education to the masses.)\n\nUnfortunately, this age-based model not only rules American public education, but private schools, homeschool networks and curricula, and even religious education have adopted this student-as-fast-food-hamburger approach, following the public school model.\n\nBut this use of age as a proxy for intellectual and social development is only an issue because *we assume that people should be educated alongside others at the same stage of development*. This idea is equally baseless, both in evidence and logic.\n\n[As proponents of *peer instruction* (PI) have found](http://mazur.harvard.edu/research/detailspage.php?rowid=8), students often learn concepts better when the explanation comes from someone who recently learned the concept than from someone who learned it years, or decades, ago (their teacher or textbook author). Similarly, students solidify and deepen their understanding of a concept when they are put in a position to explain it to someone else who doesn't yet understand.\n\nThese phenomena are not only the basis of recent developments in PI, but they form part of the pedagogical core of [Maria Montessori's method](http://digital.library.upenn.edu/women/montessori/method/method.html), which has been employed in thousands of schools for the last 100 years. In a Montessori classroom (like the one my oldest son attends), students are grouped in three-year segments. Students are given more flexibility regarding when they engage certain concepts. In fact, that flexibility can extend beyond the three-year groupings, as a \"second grader\" in a class for 6\u20139-year-olds is free to engage \"fifth-grade math\" if she is interested, motivated, and ready for those concepts. Further, the wider age groupings means that more advanced or experienced students in the class\u2014who are usually, but not always, the older students\u2014can solidify their learning as they help the younger ones, and the younger ones can learn by receiving not only instruction from their teacher, but the modeling and instruction of their more experienced peers, as well. (Though these techniques were originally based on Montessori's own experience, many have since been verified by [empirical studies in education research](https://openlibrary.org/works/OL8004702W/Montessori).)\n\nMuch can be said about the power relations that both motivate and grow out of the industrialized, divide-and-conquer, students-as-fast-food, teachers-as-line-cooks model of education. And perhaps I'll follow up in the future on some of those. But for now, I simply want to point out the fallaciousness of these core ideas of our educational system: that age can stand as a proxy for intellectual and social development, and that students should be taught in groups according to their developmental progress. These ideas are pedagogically baseless expediencies for an industrial model of education, and time is past due for us to rethink and replace them.\n",
                        "html": "",
                        "image": "/content/images/gradeOne.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Educational fallacies: Age is a proxy for intellectual development",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2014-08-30 15:26:14 -0600",
                        "created_by": 1,
                        "updated_at": "2014-08-30 15:26:14 -0600",
                        "updated_by": 1,
                        "published_at": "2014-08-30 15:26:14 -0600",
                        "published_by": 1,
                        "og_title": "Educational fallacies: Age is a proxy for intellectual development",
                        "twitter_title": "Educational fallacies: Age is a proxy for intellectual development",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/gradeOne.jpg",
                        "twitter_image": "/content/images/gradeOne.jpg"
                    },
                    {
                        "id": 0,
                        "title": "#MacronLeaks - how disinformation spreads",
                        "slug": "macronleaks-timeline",
                        "markdown": "\nA lot happened online on May 5, 2017. Ben Starling, CE Carey, and I found evidence of <a href=\"https://medium.com/data-for-democracy/democracy-hacked-a46c04d9e6d1\" target=\"blank_\">a massive disinformation campaign taking place on Twitter and 4chan</a>, in an attempt to swing the French presidential election in favor of Marine Le Pen. As we were putting the finishing touches on our article about this campaign, we saw <a href=\"https://twitter.com/wikileaks/status/860577607670276096\" target=\"blank_\">the tweet from WikiLeaks</a>, announcing that a 9GB dump of documents from the Macron campaign (dubbed #MacronLeaks) had been posted online. Then the Macron campaign confirmed that they had been the target of hackers, <a href=\"http://www.reuters.com/article/us-france-election-macron-leaks-idUSKBN1812AZ?utm_source=twitter&amp;utm_medium=Social\" target=\"blank_\">possibly with Russian connections</a>.\n\nWe continued to collect tweets related to the French election until two days after the election, in addition to ongoing monitoring of content on 4chan's *politically incorrect* board \u2015 known simply as /pol/. In the aftermath of the election, we analyzed how the #MacronLeaks news spread through 4chan and Twitter. What we found was a pattern that keeps arising in our (and others') study of the spread of disinformation online:\n\n> An anonymous user dumps information on 4chan, and then a small number of \"catalyst\" accounts bring the disinformation to a more mainstream platform like Twitter, where an army of bots, sockpuppets, \"$hitposters\", and unsuspecting individuals amplify the signal until it \"trends\" and a celebrity account brings it to the attention of mainstream media.\n\n**It's this combination of what we're calling *catalyst* accounts and the army of *signal boosters* \u2015 a number of which are bots and botnets \u2015 that allows the disinformation to spread quickly and reach the mainstream.**\n\n## #MacronLeaks timeline\n\nHere is a <a href=\"https://timeline.knightlab.com/\" target=\"blank_\">timeline</a> of how the MacronLeaks campaign progressed, beginning with early signs that a data dump might be coming, through the Macron campaign's announcement confirming a hack.\n\n<iframe src='https://cdn.knightlab.com/libs/timeline3/latest/embed/index.html?source=16WSJUwmVstsjH6R6l1mdext1P9vnXeHu-J88fCWeONs&font=Default&lang=en&initial_zoom=2&height=650' width='150%' height='650' style=\"margin-left: -25%\" webkitallowfullscreen mozallowfullscreen allowfullscreen frameborder='0'></iframe>\n\nThere are a few key takeaways from this timeline:\n\n<ul><li>Disobedient Media is a source worth a closer investigation. Not only were they a major early source of information about the leak, but they tweeted that it might be coming before the leak was posted to pastebin.com. <br></li><li>This is not the first time that a false leak has been posted to 4chan before more mainstream (social) media. In previous (unpublished) research, we found the #SyriaHoax campaign also got an early start there before moving to Twitter. <a href=\"https://twitter.com/JackPosobiec/status/860567142965575681\" target=\"blank_\">Jack Posobiec claims</a> that 4chan's /pol/ is \"the new Wikileaks\". But so far, it seems to be the place to dump <i>false</i> information that someone wants to spread <i>quickly</i> \u2015 likely to have a maximum impact before people have a chance to verify its veracity and authenticity. This is worth keeping in mind for future campaigns.</li><li><a href=\"https://twitter.com/JackPosobiec/\" target=\"blank_\">Jack Posobiec</a> and <a href=\"https://twitter.com/BasedMonitored\" target=\"blank_\">Based Monitored</a> were important links from 4chan to the Twitter community.</li><li>While Jack Posobiec linked to 4chan's thread, Based Monitored linked directly to the leak. This skirting of 4chan may have led to Based Monitored being the most-retweeted account before WikiLeaks?</li><li>It wasn't until WikiLeaks tweeted a link that knowledge of the leak came into the mainstream. Even though Wikileaks claimed it may have been a \"<a href=\"https://twitter.com/wikileaks/status/860577607670276096\" target=\"blank_\">4chan practical joke</a>\", they brought significant visibility to the leak and were instrumental in its achieving mainstream status.</li></ul>\n\nI think it's also important to note the absence of some more well known players in this narrative: Breitbart, InfoWars, Fox News, ... none of the established media sources on the far-right or center-right played a role in this, including those known for fake-news conspiracy theories. Whoever was behind this disinformation campaign knew how to get the message front-and-center quickly, at a crucial time \u2015 just a few hours before the campaign news blackout in France. \n\nAnd while the campaign was not successful in swinging the election in favor of Le Pen, it was absolutely successful in terms of controlling the mainstream media narrative at the most pivotal moment of media attention in advance of the election. They had the last word. And *if* (and that's a big if right now) the architect of this campaign is the same as the one behind #MacronGate, they were also successful in <a href=\"http://www.telegraph.co.uk/news/2017/05/04/emmanuel-macron-files-defamation-complaint-marine-le-pen-offshore/\" target=\"blank_\">shifting the focus of the last French presidential debate</a>. If it's not perpetrated by the same actor(s), then the #MacronLeaks campaign certainly capitalized on the success of #MacronGate, and was likely aided by its priming of media attention.\n\nWe're seeing disinformation campaign tactics evolve quickly, but one key trend seems to recur: **use catalysts and amplifiers to bring propaganda to the attention of the public at large, with the goal of getting a major influencer *outside your community* to boost it into mainstream media or campaign activities.** This time that was Wikileaks, in the past it's been <a href=\"http://www.politico.com/magazine/story/2017/03/memes-4chan-trump-supporters-trolls-internet-214856\" target=\"blank_\">the Trump campaign itself</a>.\n\nThe UK election is coming soon, as is Germany. We'll keep studying these campaigns, along with our <a href=\"http://datafordemocracy.org/\" target=\"blank_\">Data for Democracy</a> colleagues, and we'll keep you posted.",
                        "html": "",
                        "image": "/content/images/disinfo.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "#MacronLeaks - how disinformation spreads",
                        "meta_description": "It's the combination of catalyst accounts and an army of signal boosters \u2015 a number of which are bots \u2015 that allows disinformation to spread quickly.",
                        "author_id": 1,
                        "created_at": "2017-05-19 10:54:16 -0400",
                        "created_by": 1,
                        "updated_at": "2017-05-19 10:54:16 -0400",
                        "updated_by": 1,
                        "published_at": "2017-05-19 10:54:16 -0400",
                        "published_by": 1,
                        "og_title": "#MacronLeaks - how disinformation spreads",
                        "twitter_title": "#MacronLeaks - how disinformation spreads",
                        "og_description": "It's the combination of catalyst accounts and an army of signal boosters \u2015 a number of which are bots \u2015 that allows disinformation to spread quickly.",
                        "twitter_description": "It's the combination of catalyst accounts and an army of signal boosters \u2015 a number of which are bots \u2015 that allows disinformation to spread quickly.",
                        "og_image": "/content/images/disinfo.jpg",
                        "twitter_image": "/content/images/disinfo.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Hypothes.is Aggregator \u2015 A WordPress Plugin",
                        "slug": "hypothesis-aggregator",
                        "markdown": "\n*This is a cross-post from the [UMW Division of Teaching and Learning Technologies blog](http://umwdtlt.com/hypothesis-aggregator-wordpress-plugin/).*\n\nI\u2019ve been working and writing a lot lately about using the web annotation tool\u00a0<a href=\"https://hypothes.is/\">hypothes.is</a> for public scholarship. It has a lot of cool uses \u2015 not only the collaborative annotation of individual web pages, but also the creation of a\u00a0<a href=\"http://kris.shaffermusic.com/2016/04/hypothesis-public-research-notebook/\">public research notebook</a>, and the possibility of linking hypothes.is with other apps <a href=\"http://kris.shaffermusic.com/2016/05/getting-started-with-the-hypothesis-api/\">through the use of their open API</a>.\n\nBased on that work, I\u2019ve created a few tools to help people make fuller use of hypothes.is in their work as public scholars. The first is <a href=\"http://kris.shaffermusic.com/2016/06/introducing-pypothesis-1/\">a Python script</a> that collects annotations (by user, by tag, or both) and converts them to clean MarkDown text, for use in a blog. The second is <a href=\"http://kris.shaffermusic.com/2016/06/introducing-pypothesis-2/\">Pypothesis</a>, a Python module for writing programs that interact with the hypothes.is API.\n\nMore recently, I've created a WordPress plugin called <a href=\"https://github.com/kshaffer/hypothesis_aggregator\">Hypothes.is Aggregator</a>, which will allow WordPress users \u2015 bloggers, teachers, and students alike \u2015 to collect their own annotations, annotations on a topic of interest, or annotations from/about a class, and present them in a page or post on the WordPress platform. It's easy to install, easy to use, and (I hope) will be of value to students, scholars, teachers, and writers.\n<h2>How it works</h2>\nHypothes.is aggregator is super-simple. Create a new page or post in WordPress, and as you write, include the following \"shortcode\":\n\n~~~\n[hypothesis]\n~~~\n\nNow, that alone won't do anything. You need to feed it some search terms, like one of the following:\n\n~~~\n[hypothesis user = 'kris.shaffer']\n~~~\n\n~~~\n[hypothesis tags = 'IndieWeb']\n~~~\n\n~~~\n[hypothesis text = \"Domain of One's Own\"]\n~~~\n\n~~~\n[hypothesis user = 'kris.shaffer' tags = 'IndieEdTech']\n~~~\n\nHypothes.is Aggregator accepts <em>user</em>, <em>tags</em>, and <em>text</em> search parameters, on their own or in combination with each other. (Currently it does not support lists of users or tags, but that is in the works for a future version.)\n\nAfter adding one line of this \"shortcode,\" publish the post, and you should see on that page a list of annotations, <a href=\"http://pushpullfork.com/uncategorized/testing-the-skeleton-of-a-hypothesis-aggregator-plugin-for-wordpress/\">like on this sample page</a>. That's it!\n\n(If it doesn't work, please let me know, and I'll do my best to figure it out and release a fix.)\n\n<h2>What it's for</h2>\n\nI envision a number of possible uses for Hypothes.is Aggregator. As I write in my post on hypothes.is as a public research notebook, you can use this plugin to make a public research notebook on your WordPress site. Read something interesting, annotate it, and aggregate those annotations \u2015 perhaps organized by topic\u00a0\u2015 on your domain. They will automatically update. Just set it and leave it alone.\n\nI also see this as a tool for a class. Many instructors already use hypothes.is by assigning a reading that students will annotate together. Hyopthes.is Aggregator makes it easy to assign a <em>topic</em>, rather than a reading, and ask students to find their own readings on the web, annotate them, and tag them with the course tag. Then Hypothes.is Aggregator can collect all the annotations with the class tag in one place, so students and instructors can see and follow-up on each other's annotations. Similar activities can be done by a collaborative research group or in an unconference session.\n\n<h2>Help me kick the tires</h2>\n\nI've done a lot of small tests, but I could use some help. So if you're interested, please try it out and let me know how it goes. I'll do my best to help you out and/or add fixes and features to future versions.\n\nIt's really easy to\u00a0install and get going. First, go to <a href=\"https://github.com/kshaffer/hypothesis_aggregator\">the project page on GitHub</a>, click \"Clone or Download\" (the green button on the right), and then \"Download to ZIP.\" Then go to Plugins &gt;&gt; Add New in your WordPress dashboard. Click \"Upload Plugin.\" Then upload the ZIP file you just downloaded. Once it's uploaded and installed, click \"Activate,\" and you're ready to go!\n\nThen try it out, and let me know how it goes.\n\n<h2>Plans for the future</h2>\n\nIt can do a lot of things as-is. But I want to add more. Here's my current list (feel free to suggest more):\n<ul>\n \t<li>Search for all posts from a <em>list</em> of users (for more fool-proof class aggregation).</li>\n \t<li>Support multiple tags \u2015 in both AND and OR configurations.</li>\n \t<li>Add \"class\" attributes to the HTML objects, for more flexible visual presentation.</li>\n \t<li>The ability to embed a single annotation into a blog post (like you can with a tweet).</li>\n</ul>\nWhat else should this plugin do?\n\n<em>Photo by <a href=\"https://www.flickr.com/photos/drainrat/16786733075/\">darkday</a></em>\u00a0(CC BY 2.0).\n",
                        "html": "",
                        "image": "/content/images/oldtype.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Hypothes.is Aggregator \u2015 A WordPress Plugin",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-08-03 08:40:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-08-03 08:40:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-08-03 08:40:00 -0400",
                        "published_by": 1,
                        "og_title": "Hypothes.is Aggregator \u2015 A WordPress Plugin",
                        "twitter_title": "Hypothes.is Aggregator \u2015 A WordPress Plugin",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/oldtype.jpg",
                        "twitter_image": "/content/images/oldtype.jpg"
                    },
                    {
                        "id": 0,
                        "title": "A journey through API programming \u2015 Part 3: Retrieving data",
                        "slug": "journey-through-api-programming-3",
                        "markdown": "\n*This is part of a series of posts in which I blog through my process of learning API programming in general and* [*the Medium API*](https://medium.com/blog/the-medium-api-is-now-open-to-everyone-3f4642e5c850#.8ehvndx6y) *in particular. For the beginning of this series, see* [*Part 1*](http://kris.shaffermusic.com/2016/08/journey-through-api-programming-1/) *and* [*Part 2*](http://kris.shaffermusic.com/2016/08/journey-through-api-programming-2/)*.*\n\nThe basic element of programming with APIs is the API call, a single command in one application that contacts another application with a request to send, receive, edit, or delete data to/from that other application. API calls are particularly powerful when the \u201cdata\u201d to send is actually a set of instructions for the app to execute, with instructions for what to do with the results. But we\u2019ll start on the simple end with a simple \u201cGET\u201d request \u2015 an API call that asks the other application to send some data back to the application making the call.\n\nWhen I first started exploring API programming, I ran into a lot of documentation sites that contained something like this as a \u201csample request\u201d with no instructions for how to issue such a request within a specific programming language:\n\n~~~\nGET /v1/me HTTP/1.1\nHost: api.medium.com\nAuthorization: Bearer 181d415f34379af07b2c11d144dfbe35d\nContent-Type: application/json\nAccept: application/json\nAccept-Charset: utf-8\n~~~\n\nOf course, it\u2019s not the job of those writing API documentation to teach users how to make API calls. *But* just about everything I read, including API how-tos, assumed some level of knowledge that I didn\u2019t have, making it very difficult to learn how to make a \u201csimple\u201d API call.\n\nI\u2019m going to try and break it down.\n\nLet\u2019s start with a really simple example. The [hypothes.is API](https://h.readthedocs.io/en/latest/api/) does not require authentication for GET requests, as long as you are okay with the API returning only public data (no private annotations are included in the data it sends back), so that makes it more straightforward than, say, the Medium API.\n\nThe hypothes.is API documentation provides the following description of what such a GET request would look like. This API call returns all the annotations by the user gluejar.\n\n~~~\nGET /api/search?limit=1000&user=gluejar@hypothes.is\nHost: hypothes.is\nAccept: application/json\n~~~\n\nBut this isn\u2019t what we type into our code. We can stitch it together though. Start with the host \u2015 hypothes.is \u2015 then add the GET url \u2015 /api/search?limit=1000&user=gluejar@hypothes.is \u2015 to the end of it, and finally stick https:// in front of it:\n\n~~~\nhttps://hypothes.is/api/search?limit=1000&user=gluejar@hypothes.is\n~~~\n\nThis is the unauthenticated API call! Notice we\u2019re going to the hypothes.is server, telling it we\u2019re making an API request (notice the /api/, though not all servers do it this way), and then pass an instruction to search for annotations from the user gluejar@hypothes.is and limit the results to 1000 annotations or less.\n\nTo try this API call, just put that URL into the address bar of your browser and see what you get. This is a JSON object (JavaScript Object Notation, though it is not limited to JavaScript; many languages use and parse JSON data). If you\u2019ve worked with JSON data before, you likely recognize the format. If not, it may look like nonsense. (Though if you cut and paste it into a [JSON prettifier](https://jsonformatter.curiousconcept.com/), the structure will start to become more apparent.)\n\nFor simple GET requests to APIs that don\u2019t require authentication, it\u2019s that easy.\n\nTo incorporate such a request into your code, you need a function that will make the request without a browser, and store the results into a data structure.\n\nHere\u2019s the code that will do that in Python (assuming you have installed the requests and json modules):\n\n~~~python\nimport requests\nimport json\nraw_data = requests.get('https://hypothes.is/api/search?limit=1000&user=gluejar@hypothes.is')\nparsed_data = json.loads(raw_data.text)\n~~~\n\nNow you have a JSON object called \u2018parsed\\_data\u2019 containing the same data returned in our browser request above, ready for analysis and manipulation.\n\n## Helper\u00a0modules\n\nConstructing the exact URL for a request can be tricky, though. It\u2019s easy to see that the above search looks for 1000 or less annotations from gluejar@hypothes.is, but how would we construct a GET request for the five most recent annotations with the tag \u2018JavaScript\u2019 from the user kris.shaffer@hypothes.is? Poring over the API docs (when they\u2019re well written) can make that clear. However, they aren\u2019t always as complete or pedagogically sound as we might like. Or the docs may be in good shape, but the API is so robust that the construction of the request will be highly complex by necessity. The same goes for the data we get back \u2015 even with clear documentation, the data structure may be complex, deeply hierarchical, and unituitively structured from the perspective of an end user.\n\nFor these reasons, many developers of popular APIs create software development kits (SDKs) or helper modules for common programming languages, to make the construction of requests and the processing of data less burdensome. For example, I created [Pypothesis](https://github.com/kshaffer/pypothesis), a Python module for making and parsing unathenticated GET requests for the hypothes.is API. The Spotify API has [Spotipy](https://spotipy.readthedocs.io/en/latest/) (also, predictably, for Python).\n\nHere\u2019s example code for Pypothesis (assuming the module has been installed, or pasted into the Python script above what follows):\n\n~~~python\ns = searchurl(user = 'kris.shaffer@hypothes.is', tag = 'JavaScript')\nl = retrievelist(s)\nfor entry in l:\n    e = Annotation(entry)\n    print(e.title, e.uri)\n~~~\n\nThis is all you need to find every annotation by me about JavaScript, and then print to screen the title and URI of each article annotated. (Dig into the code for [Hypothes.is Aggregator](https://github.com/kshaffer/hypothesis_aggregator) to see how to do similar things in PHP.)\n\n## Using the Medium API\u00a0SDK\n\nBut I said this blog series would be about by journey with the Medium API! I went through hypothes.is first, because you can send a GET request *without* authenticating. Not so with Medium. **Every Medium API call requires authentication.** And that adds significant complication. So let\u2019s dig in, using the Medium API\u2019s Python SDK.\n\nFirst, we need to download and install Medium\u2019s SDK for API calls in Python. You can find it (and SDKs for several other languages) [here](https://github.com/Medium/medium-api-docs/blob/master/SDK.md). Also, I\u2019ll be mostly following Ben Werdmuller\u2019s tutorial for the Medium API ([available here](https://github.com/Medium/medium-api-docs)), but adding more detailed instructions for the less-than-fully-initiated (like I was/am).\u00a0:)\n\nThe first step is to [register a new Medium application](https://medium.com/me/applications) so you can make authenticated calls. It\u2019s fine to name it something like \u2018testing\u2019. It will ask you for a callback URL. It doesn\u2019t actually matter what this is. In fact, it doesn\u2019t even need to exist! I recommend putting your website followed by \u2018/callback\u2019. When you register successfully, it will provide you with a client ID and a client secret. Treat the client secret like a password and do *not* share it with others.\n\nThen you can run the following code in Python (replacing the X\u2019s with your client ID and client secret, and replacing \u2018https://pushpullfork.com/callback\u2019 with the callback URL you provided when you made the app). Because we\u2019re doing this in two parts, I recommend calling Python from the command line or using iPython, which will keep the session open, rather than running code from within a text editor.\n\n~~~python\nfrom medium import Client\nimport requests\n~~~\n\n~~~python\nclient = Client(application_id=\"xxxxxxxxxxx\", application_secret=\"xxxxxxxxxxxxxxxxxxxxxxxxx\")\nauth_url = client.get_authorization_url(\u201csecretstate\u201d, \u201chttps://pushpullfork.com/callback\", [\u201cbasicProfile\u201d, \u201cpublishPost\u201d])\nprint(auth_url)\n~~~\n\nIf it works, it should print to screen a really long URL.\n\nVisit that URL in your browser. (As far as I can find, there\u2019s no way to spoof the server into thinking that your Python script is a real person and doing this part automatically. If you know how, please leave a comment, and I\u2019ll update the tutorial!)\n\nIf, like me, you used a fake URL for your callback, you\u2019ll get a 404 Page Not Found response. *BUT* take a look at the address bar. The URL has changed from what you entered in, and it will end with \u2018code=XXXXXXXX\u2019, where the X\u2019s represent your secret code. Copy that code and go back to your Python window.\n\nNow run the following Python code (replacing the X\u2019s with the code you just received, and replacing my callback URL with yours):\n\n~~~python\nauth = client.exchange_authorization_code(\u201cXXXXXXXX\u201d, \u201chttps://pushpullfork.com/callback\")\nclient.access_token = auth[\u201caccess_token\u201d]\nuser = client.get_current_user()\nprint(user)\n~~~\n\nIf it works, you\u2019ll see a small Python dictionary that contains basic data for your Medium user. You can access its internal elements like usual. To get the username, simply enter:\n\n~~~python\nuser['username']\n~~~\n\nLet\u2019s try one more request. Let\u2019s get all the publications that I (or you) have published in on Medium.\n\nOh, wait! [That\u2019s supported by the Medium API](https://github.com/Medium/medium-api-docs). But [it doesn\u2019t look like it\u2019s supported by the Python SDK](https://github.com/Medium/medium-sdk-python/blob/master/medium/__init__.py)! What to do?\n\nWell, we\u2019ll have to take a step back and hard-code the request ourselves. (And maybe contribute the code to the SDK!)\n\nSo in the coming post(s), I\u2019ll dig into making authenticated requests in Python that aren\u2019t supported by the SDK, and/or using JavaScript to make those requests. I haven\u2019t done either before! (Though I\u2019ve done the latter with other APIs.) So I\u2019ll be blogging as I learn.\u00a0:)\n\n*Header image by* [*Kjetil Korslien*](https://www.flickr.com/photos/kjetikor/8484119632/) *(CC BY-NC).*\n",
                        "html": "",
                        "image": "/content/images/apiHeader3.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "A journey through API programming \u2015 Part 3: Retrieving data",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-08-30 15:01:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-08-30 15:01:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-08-30 15:01:00 -0400",
                        "published_by": 1,
                        "og_title": "A journey through API programming \u2015 Part 3: Retrieving data",
                        "twitter_title": "A journey through API programming \u2015 Part 3: Retrieving data",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/apiHeader3.jpg",
                        "twitter_image": "/content/images/apiHeader3.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Promoting musical fluency -or- Why I de-emphasize sight-singing and dictation in class",
                        "slug": "promoting-musical-fluency-or-why-i-de-emphasize-sight-singing-and-dictation-in-class",
                        "markdown": "\nSight-singing and dictation are staples of university-level musical training. While it is a worthy ambition to \"see what you hear and hear what you see,\" as many state on their aural skills syllabus, I believe that there is a better goal to pursue: _musical fluency_. And while the ability to perform from sight and to notate musical passages after a limited exposure is a natural (and readily assessable) by-product of musical fluency, it is not the same thing as musical fluency. Further, based on some experiments with class-time this year, I hypothesize that repeated practice of sight-singing and dictation in class is neither the best way to develop musical fluency nor even the best way to improve students' ability to sight-sing or dictate.\n\n\n\n\n\n## What is musical fluency?\n\n\n\n\n\nI define musical fluency as the successful depositing of musical information (a \"vocabulary,\" perhaps) into readily-accessible long-term memory, and the successful cognitive assimilation of musical concepts and structures. I take this largely from linguistic fluency. One is fluent in a language when one has deposited a large enough vocabulary in one's memory, can access that vocabulary in real time, and has assimilated a sufficient subset of that language's grammar that one can function comfortably in a culture dominated by that language. Notice that while fluency will lead to and may be indicated by one's performance with the language in a culture bound to it, fluency is not the same thing as the ability to perform a list of tasks associated with that performance. Musical fluency is similar. While it will lead to an ability to perform certain tasks, and may be indicated by those abilities, it is not the same thing.\n\n\n\n\n\nThis distinction is important for a musicianship class. It is easy (and common) in American education to gear class activities toward the final assessment. If the assessment involves sight-singing, then class will involve a heavy amount of sight-singing. That makes sense if the goal is an ability to sing from sight _and_ if repeated sight-singing is the best way to improve one's ability to sing from sight. However, if musical fluency is the goal, we instructors need to ask 1) _are our musicianship assessments in sight-singing and dictation the best ways to assess musical fluency?_ and 2) _are sight-singing and dictation practice the best way to help students achieve musical fluency?_\n\n\n\n\n\nRegarding the first question, I think that sight-singing and dictation are helpful indicators of aspects of musical fluency, but not sufficient to stand on their own as the sole indicators we use. Instead, we should spend some time as a community of aural skills instructors reconsidering other methods of assessment (perhaps enhanced by developments in technology) that may better indicate fluency, or that may be placed alongside sight-singing and dictation to provide a more complete picture of a student's fluency.\n\n\n\n\n\n(It should be noted that, just as linguistic fluency involves more than reading aloud and dictating sentences spoken in a language, musical fluency involves more than performance and listening. It necessarily involves \"theory\"\u2014i.e., analysis, composition, improvisation, verbal description, and the construction of reasoned arguments. With that in mind, I'm a strong proponent of \"Musicianship\" courses that combine \"theory\" with \"ear training.\" However, I believe that the points I raise in this post can apply even if these topics are treated in separate courses.)\n\n\n\n\n\nRegarding the second question, I think that devoting the bulk of aural instruction to sight-singing and dictation is not the best use of class time in order to develop our students' musical fluency. Other activities (which I will discuss below and/or in future posts) have helped my students grow in their musical fluency, and I think that further exploration of these and other activities will reveal better ways to spend class time in order to promote fluency\u2014and even to increase student ability to sight-sing and dictate.\n\n\n\n\n\n## Assimilation and what it looks like\n\n\n\n\n\nLet me take a step back and describe in better detail what I think musical fluency looks like by way of an example: compound meter. What does it take for a musician to assimilate the concept of compound meter?\n\n\n\n\n\nThose of us who teach music theory and/or aural skills know the many strategies employed by music students to skirt the concept of compound meter while performing or dictating it accurately. (Most involve shifting attention to beat divisions instead of beats.) Performing a piece in compound duple meter while attending to the divisions (six per bar) does not constitute assimilation of compound meter, but rather clever employment of a workaround. Compound meter has been assimilated when a student understands that a beat can be divided by two (simple meter) or three (compound meter), and can apply that difference\u2014specifically the three-division variety\u2014in a diversity of musical activities. We only know that a student has successfully assimilated this concept when the student can explain the difference between compound and simple meter, recognize the difference by ear or in a score, can perform a piece in compound meter while attending to the triply divided beat (perhaps indicated by conducting that pulse while singing), etc.\n\n\n\n\n\nStudents employing a workaround will _not_ be assisted in assimilating this concept by repeating the tasks in which they are already employing the workaround (sight-reading and dictation). Other tasks must be devised. In my experience, playing a recording while giving students the counting pulse and asking them to sing both duple and triple divisions of that pulse and then determining the meter of the piece is helpful. Forcing students to conduct or tap beats and _not_ divisions during practice performing is another helpful task. Ultimately, though, sight-singing and dictation neither help students assimilate this concept, nor are they indicators of successful assimilation. (And in my experience, quizzes asking \"How many beats are in a bar of 6/8?\" are not sufficient either, given how many students answer such questions correctly, yet still can only perform or dictate when attending to the division.)\n\n\n\n\n\n## Improving sight-singing without sight-singing\n\n\n\n\n\nThis semester, I tested the hypothesis that sight-singing could be improved without in-class practice in sight-singing. Instead, we focused on developing students' familiarity with and conceptual understanding of some of the elements they would find in melodies I would ask them to sing. Specifically, we focused on rhythmic performance (with a combination of repetition of stock rhythmic schemata, and some sight-reading of rhythmic lines without pitch) and species counterpoint. The species counterpoint unit emphasized pitch over rhythm (gradually introducing simple rhythmic complications between species), and included both composition and performance. (Students worked in pairs to compose counterpoints in each species in major and minor, above and below the cantus firmus, and then were assessed on both their composition and their performance of those exercises. Only correct exercises were accepted, and their grade was the number of species they were able to compose and perform with a partner without mistakes.)\n\n\n\n\n\nThis unit involved a lot of performing, but no pitch sight-reading. All pitches sung were composed by the students or, in early stages, supplemented by the keyboard as they analyzed model exercises (we have class in the piano lab). Prior to this unit, students had assessed (mostly successfully) singing stepwise melodies in major and minor with beats and divisions, and tonic-triad leaps at least in major (some successfully assessed tonic-triad leaps in both modes). This unit was designed to expose them to diatonic leaps not bound to the tonic triad.\n\n\n\n\n\nAfter this unit of rhythmic study and species counterpoint, we returned to melodic sight-singing. Students were given melodies with the same rhythmic figures studied in isolation, and with a wide variety of types of leaps (Karpinski's text and anthology through Chapter 31). Students were quite successful in class, and when I remarked about this, one student said, \"This is easy. It's just what we did in species counterpoint.\" Which was, of course, the idea. Despite spending far less time on sight-singing in class than in last spring's iteration of the same course\u2014probably less than ten meetings all semester\u2014their final sight-singing assessments of the semester were right in line with the previous year's students. (I used the same melodies and a similar grading scale.) The difference, of course, was more class time to focus on voice-leading and other topics, which has paid dividends in those areas.\n\n\n\n\n\nThe reason I believe that this process worked is that the task of sight-singing involves three primary components: fluent knowledge of the musical structures contained in the melodies sung, an ability to perform at a relatively high level in a high-stakes, one-on-one assessment environment, and an ability to strategize appropriately to ensure success in that high-stakes assessment. When a student fails a sight-singing assessment, the failure can be caused by any one of these things. All are important, but good teaching requires that we identify which is in need of improvement, and fair assessment requires that we identify which _have_ been mastered by the student. That's difficult to do if the only assessment is a brief task that combines all three.\n\n\n\n\n\nIt is also worth pointing out that strategizing can be learned in far less time than we spend practicing sight-singing. If indeed, as my experience suggests, concepts can be learned and performance consistency achieved in other ways as well, a lot of class time can be freed for other purposes, or potentially used to achieve a _higher_ standard of performance from sight involving a broader spectrum of structures and concepts.\n\n\n\n\n\nI hope to share more examples of this kind of success in future posts. However, this example illustrates that my hypothesis is viable and worth more rigorous exploration: sight-singing is a _by-product_ of musical fluency, and tasks other than sight-singing can develop that fluency\u2014and thus the ability to sing from sight\u2014with at least as much efficacy, and possibly in less class time.\n\n\n\n\n\n## What are the real elements of musical fluency?\n\n\n\n\n\nI don't have a set list outlining the elements of musical fluency. For one, it would not be the same for every program, career aspiration, student, or instructor. And in any case, I'm still in the process of reworking my current lists of course objectives into a description of conceptual fluency as I would articulate it. However, I think the key in making such a change is to differentiate _tasks_ from _concepts_. We often construct tasks that include concepts, and then organize class around the tasks\u2014both class activities and assessments. I think that teasing out the concepts from the tasks, orienting class activities around assimilating those concepts, and assessing the _concepts_ as evidenced by the tasks rather than assessing the tasks themselves, will make great strides towards promoting and accurately assessing musical fluency.\n\n\n\n\n\nI'm eager to hear if others have tried similar class arrangements and how successful they were (or were not). I'm also eager to hear what others think about the appropriate goals of these kinds of courses: specific skills, or conceptual assimilation evidenced in application. Please comment below if you have thoughts or experiences to share.\n",
                        "html": "",
                        "image": "",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Promoting musical fluency -or- Why I de-emphasize sight-singing and dictation in class",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2013-04-01 18:50:15",
                        "created_by": 1,
                        "updated_at": "2013-04-01 18:50:15",
                        "updated_by": 1,
                        "published_at": "2013-04-01 18:50:15",
                        "published_by": 1,
                        "og_title": "Promoting musical fluency -or- Why I de-emphasize sight-singing and dictation in class",
                        "twitter_title": "Promoting musical fluency -or- Why I de-emphasize sight-singing and dictation in class",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "",
                        "twitter_image": ""
                    },
                    {
                        "id": 0,
                        "title": "The economics of the classroom -or- Why grades encourage bad habits",
                        "slug": "the-economics-of-the-classroom-or-why-grades-encourage-bad-habits",
                        "markdown": "\nI recently watched the TED talk by Clay Shirky, \"How Cognitive Surplus will Change the World.\" It's well worth the 20 minutes it takes to watch/listen for many reasons (you can download it to a mobile device for convenient viewing or listening). But what struck me most was his discussion of an economic study about the use of fines to regulate behavior.\n\n<p><iframe src=\"http://embed.ted.com/talks/clay_shirky_how_cognitive_surplus_will_change_the_world.html\" width=\"560\" height=\"315\" frameborder=\"0\" scrolling=\"no\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></p>\n\nThe study\u2014\"A fine is a price,\" by Gneezy and Rustichini (_Journal of Legal Studies_ 2000)\u2014which also makes an appearance in the book _Freakonomics_, recounts an experiment at 10 nearly identical Israeli day care centers. The day cares all closed at 4pm, but found a number of parents picking up their children late, causing employees to stay beyond work hours without extra compensation. In the study, after four weeks of observation, half of the day cares instituted a small fine (approx. $3) for each child picked up more than 10 minutes late. After four weeks, the fine was removed so that none of the day cares charged extra for late pickups.\n\n\n\n\n\nDuring the four weeks of observation, there was no significant difference in the number of late pickups between the target group of day cares and the control group. As soon as the fines were instituted in the target group, however, the number of late pickups promptly _doubled_ in the day cares that instituted the fines. Once the fines were removed, the late pickups continued, with the target group having twice as many late pickups as the control group, even though there was no longer any difference in policy.\n\n\n\n\n\nJust to be clear, that was not a typo. The fines drastically _increased_ the frequency of late pickups\u2014the very behavior the fines were supposed to reduce.\n\n\n\n\n\nThe authors of the study suggest several interpretations of this. The main possibility they suggest is that there is an implicit social/moral contract where parents recognize that a late pickup is imposing on the workers and presuming upon their kindness. They feel bad about imposing, and try to keep it to a minimum, with everyone realizing that these things just happen sometimes. Once a fine is imposed, however, there is no longer an imposition; parents view the fine a price for the extra time, and if they are willing to pay it, they take advantage of the extra time. The fine makes the late pickup into a \"commodity,\" and for some reason, it retains that status after the fines are removed. The guilt is gone, so to speak, never to return.\n\n\n\n\n\nWhat does this have to do with grades and the classroom?\n\n\n\n\n\nI've blogged and tweeted in the past about how removing late penalties for assignments or ceasing to factor attendance into final grades does not increase student delinquency for my classes, on the whole. However, that does not mean that delinquency does not exist. On Friday, a large number of my students were absent, with only a small number of them notifying me why they were missing. (This actually happens a lot on Fridays.) In Musicianship IV, only 4 of my 12 students showed up! (Though, three were already excused for an inter-collegiate band festival.) I was livid, as were some of the students who showed up. I sent personal emails to each student in that class who did not come and did not talk to me beforehand about why, some of which were very stern if the student is a repeat offender. (I've had all of them for three or four semesters in a row, now, so I know their habits well.)\n\n\n\n\n\nThe responses I got from some of those emails, combined with others things I've heard students say on campus, reflect ideas consistent with those suggested by Gneezy and Rustichini:\n\n\n\n\n\n\n\n  * Excused absences are like sick days at work; they need to be used up because they don't roll over to the next semester.\n\n\n  * If an instructor counts students as absent who are more than _x_ minutes late to class (for me, the cutoff is 10 minutes), there's no point in going to class once you're already 11 minutes late.\n\n\n  * If there is a grace period (excused absences, or the 10-minute late/absent cutoff), then compliance should be defined as being within the grace period. In other words, if a student is 11 minutes late, that's just one minute away from compliance, not 11.\n\n\n\n\n\nThese ideas come from a fundamental misunderstanding, which grades and graded attendance contribute to. Some students believe they have met their obligations as long as they stay shy of where penalties kick in. The same is true for grades: even if I as the professor have put in the efforts required of me to make an A possible for every student, if they are happy with a C or a B, they don't need to match my efforts and seek to learn/master everything in the course\u2014just enough to get the grade they are content with. Grades and attendance penalties have made student coursework into a commodity.\n\n\n\n\n\nThis is bad.\n\n\n\n\n\nWhen I was an undergraduate, I believe I missed two or three classes or rehearsals without being excused ahead of time by my professor. And when I did, I was on the phone or in their office _immediately_, hat in hand, and in one case hoping desperately that I would not get kicked out of the orchestra concert. (I was in the conservatory practicing before rehearsal, my watch stopped, and I was 15 minutes late when I realized it; the conductor was merciful and let me remain in.) I have _never_ been late for or missing from a professional rehearsal, concert, or class I taught. And this isn't just me. At Lawrence, there was (and still is) a culture that expects everyone to be at everything, and prepared. Not everyone is, but everyone knows it's the expectation, and on average, compliance is high.\n\n\n\n\n\nI want to foster this culture at CSU: one in which students don't view their responsibility as doing just enough not to be penalized, but doing everything they are asked; one in which students don't work just hard enough not to get in trouble with their professor, but one in which students work _at least_ as hard as their professor at fostering their own musical and academic development. There is talk among some faculty here about tightening the institution-wide compulsory attendance policies. Rather than stiffening penalties or making cutoffs more strict, I want to work with my students to make _explicit_ the implicit social contract of our class: what they expect from me, what they expect from each other, and what I expect from them. I obviously have some things in mind, but I want them to come up with them themselves. So I won't list them here just yet, in case any of my students read this post. (And I hope, and expect, at least a few will.) I will post an update later, after my students and I have had a pointed chat about these matters.\n\n\n\n\n\nIn the mean time, how do other professors, teachers, and students view this moral/social contract? Do grades and graded attendance motivate compliance for you? Or do they just lay out a cost for non-compliance that some students are willing to pay?\n",
                        "html": "",
                        "image": "",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "The economics of the classroom -or- Why grades encourage bad habits",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2013-01-26 15:28:42",
                        "created_by": 1,
                        "updated_at": "2013-01-26 15:28:42",
                        "updated_by": 1,
                        "published_at": "2013-01-26 15:28:42",
                        "published_by": 1,
                        "og_title": "The economics of the classroom -or- Why grades encourage bad habits",
                        "twitter_title": "The economics of the classroom -or- Why grades encourage bad habits",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "",
                        "twitter_image": ""
                    },
                    {
                        "id": 0,
                        "title": "The flipped classroom: six myths",
                        "slug": "the-flipped-classroom-six-myths",
                        "markdown": "\nWhat is the flipped classroom?\n\nAccording to many in the educational technology business, it\u2019s using online video to deliver lectures to students and personalize the learning process. However, if you read the work of education researchers, the flipped class model is more about promoting active learning in class, in pursuit of higher-level, critical thinking skills. So which is it?\n\nI\u2019d rather not get into the business of erecting fences and proclaiming who is in and who is out. However, it is important to correct misinformation and expose marketing claims masquerading as pedagogical philosophy, so that thoughtful teachers can make wise choices as we plan for the upcoming academic year.\n\nFollowing are six common misconceptions about the flipped classroom, and my attempts to correct them based on peer-reviewed research and several years\u2019 experience flipping my own classes.\n\n# Myth \\#1: The flipped classroom means video lectures.\n\nOne of the common approaches to the flipped classroom involves giving students videos to watch at home so that class time can be focused on more active learning. This is certainly the model being put forward by [those whose business is educational technology](http://assets.techsmith.com/docs/pdf-landingpages/flippedclassroom-explore.pdf). However, the reason to flip a class is not the technology used to distribute content. The reason to flip is to bring the more difficult, more important elements of learning into class time: application, collaboration, critique. Video lectures that students watch at home can help free up that class time. But [so can a decent textbook](http://www.seas.harvard.edu/news/2013/03/flipped-classroom-will-redefine-role-educators).\n\n# Myth \\#2: The flipped classroom requires internet access at home.\n\nMost teachers who make video lectures for their students post them to a streaming video site. However, [not all students have access to broadband internet at home](http://www.theatlantic.com/education/archive/2014/12/what-happens-when-kids-dont-have-internet-at-home/383680/), or to a device that can handle streaming video. When our teaching requires at-home access to expensive technology, we run the risk of [further disadvantaging students of lower socio-economic status](http://www.hybridpedagogy.com/journal/homework-is-a-social-justice-issue/). But that doesn\u2019t mean that we cannot harness the advantages of a flipped class model. As with Myth \\#1, in many cases a book or a paper handout will suffice for communicating the necessary information to students. But even where video is advantageous (I teach music, so multimedia demonstrations are essential), teachers can burn DVDs or put files on flash drives and SD cards, even iPods, for students to check out. And schools can provide resources in the library or computer lab to ensure that students have the access they need to class materials. The internet is easy, but it\u2019s not the only channel for providing students access to quality educational materials.\n\n# Myth \\#3: The flipped classroom is about personalized learning.\n\nMany educational technology companies point to [*personalization*](http://www.educationworld.com/a_curr/vodcast-sites-enable-flipped-classroom.shtml) and \u201cadaptive\u201d (i.e., algorithm-driven) learning as the goal of the flipped classroom. And yes, this is possible. In what some call \u201cFlip 101\u201d or \u201cThe Flipped Classroom 1.0,\u201d students have control over the pacing of videos, can pause and rewatch them as needed, and then come to class where they do traditional homework on their own while the teacher circulates throughout the room. Each student may be working on something different at a given time, and each student may receive different guidance from the teacher as they interact one-on-one. For many classes, this already is an advantage over traditional lecture-homework models, since the students are doing the hardest work when the teacher is there to help.\n\nHowever, many who try \u201cFlip 1.0\u201d soon find themselves thinking, \u201cWe could do so much more with class time!\u201d The initial flip, rather than being an end in itself, becomes an opening of a door towards other pedagogical strategies. These may include [peer instruction](http://blog.peerinstruction.net/2012/03/15/peer-instruction-101-what-is-peer-instruction/), [problem-based learning](http://www.udel.edu/inst/), [constructionist learning](http://web.media.mit.edu/~calla/web_comunidad/Reading-En/situating_constructionism.pdf), [a focus on collaboration](http://www.elearnspace.org/Articles/connectivism.htm) \u2014 all of which help develop higher-level reasoning and foster deeper, longer-lasting learning. In such a class, there can still be an element of personalization and differentiation in class activities, but the focus of many experienced \u201cflippers\u201d is on deep learning and application. Often collaborative and social strategies lead to deeper learning than completely \u201cpersonalized\u201d strategies.\n\n# Myth \\#4: The flipped class makes learning more efficient\n\n\u201cPersonalized,\u201d \u201cadaptive\u201d learning sounds like a boon for educational efficiency. When students master a concept, they move onto the next one \u2014 no waiting for others. When a student struggles with a concept, they stick with it until they get it \u2014 lest they move on before they\u2019re ready and subsequent activities become a waste of time. Add a central source of content (Khan Academy, TED-Ed, etc.), and learning becomes a well-oiled, and well-managed, machine. But deep learning is rarely so neat \u2014 and the \u201creal world\u201d never is.\n\nTeaching shouldn\u2019t just make the unfamiliar familiar, it should make *unfamiliarity* familiar. School should prepare students to answer questions that haven\u2019t been answered before, to ask questions that haven\u2019t been asked before, to make sense of the mess of the universe. To do this, school needs to introduce them to the mess and help them learn to navigate it. The flipped classroom frees up class time from the tyranny of the lecture so that teachers and students can learn how to face the mess of the world. How does a scientist formulate a research question and hypothesis? What made Beethoven\u2019s symphonies so striking to their first audiences? How were some of the most hated people in history able to gain so many loyal followers? Done well, the flipped classroom brings these kinds of questions front-and-center in the class. Rather than taking down information (lecture) or doing drilling practice (Flip 101) in class, students work together on these tough questions and the methods they might employ to answer them.\n\n# Myth \\#5: The flipped classroom is a change in technology, not a change in pedagogy.\n\nThe flipped class *can* be a mere change in technology \u2014 a new way to get information into students\u2019 brains (or at least their notebooks). But, as I\u2019ve laid out above, the flipped class can be, and should be, far more than that. Questions of technology are secondary, even ancillary, to the change in pedagogy that comes with the flipped class. Content and content delivery are no longer the focus of the class. Rather, students and *their* work become the focus of the class. That is the flip that matters most. And it can happen without changing *any* of the technology.\n\n# Myth \\#6: The flipped classroom is one thing.\n\nThere are many ways to flip a class. But perhaps central to the flipped class is the freedom that it brings to both teachers and students. So if you\u2019re thinking about flipping your class, or you\u2019re intrigued by the idea, don\u2019t sweat the tech. Think about the goals you have for your students \u2014 and the goals your students have for themselves. Then let the textbook (or video) do its work, and devote class time to helping students work through the beautiful mess that sits between them and those goals.\n\n*If you want to learn more about flipped pedagogy, join us for a [three-week, intensive, online course](http://www.digitalpedagogylab.com/blog/course/the-flipped-classroom/) on the flipped classroom at Digital Pedagogy Lab, beginning July 19.*\n",
                        "html": "",
                        "image": "/content/images/personalized.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "The flipped classroom: six myths",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-07-09 16:22:54 -0600",
                        "created_by": 1,
                        "updated_at": "2015-07-09 16:22:54 -0600",
                        "updated_by": 1,
                        "published_at": "2015-07-09 16:22:54 -0600",
                        "published_by": 1,
                        "og_title": "The flipped classroom: six myths",
                        "twitter_title": "The flipped classroom: six myths",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/personalized.jpg",
                        "twitter_image": "/content/images/personalized.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Fake news, adtech, and the spread of misinformation",
                        "slug": "fake-news-adtech-misinformation",
                        "markdown": "\n> The more we study extremist communities online, the more convinced we are that standard advertising practices play a significant role in the spread of misinformation. Further, while many are rightly encouraging us to think critically about the information we consume and the sources we trust, it\u2019s also becoming clear to us that mainstream news media is playing the same advertising game, and by the same rules, as the spammers and the spreaders of misinformation. And as long as the pay-per-click advertising model rules the roost, and news sites (mainstream and extremist) continue to be the largest users of data-collecting adtech, online, data-driven propaganda will be a real possibility, not to mention a lucrative business opportunity.\n\n<small><i>This is the third post in a series on misinformation, propaganda, and digital media by Bill Fitzgerald and myself. We are currently researching the places that political extremists get their (mis)information, and the role that adtech and social media play in the spread of that misinformation. To learn more, see our earlier posts: <a href=\"http://pushpullfork.com/2017/02/mining-twitter-data-tidy-text-tags/\" target=\"blank_\">Mining Twitter data with R, TidyText, and TAGS</a> and <a href=\"https://funnymonkey.com/2017/adtech-and-misinformation-the-middlemen-who-sell-to-all-sides\" target=\"blank_\">Adtech and Misinformation: the Middlemen Who Sell to All Sides</a>.</i></small>\n\nWhere do people get their \"news\"? Do Americans on the political right get their information from different sources than their left-leaning counterparts? And what about so-called \"fake news\"? How prevalent is it? And is it a problem for everyone, or more of a problem for a certain political group?\n\nThese are the questions underlying the research that Bill Fitzgerald and I have been conducting these past few weeks. It started with a simple question from Bill on Twitter: how difficult would it be to assemble a list of domains commonly tweeted by far-right accounts? My answer was *not very*. And then I began collecting tweets.\n\nTo be specific, I collected 591,680 tweets from early March. I used the TAGS Twitter archive tool to collect tweets indiscriminately from four right-wing hashtags \u2015 **#maga, #americafirst, #pizzagate,** and **#whitegenocide** \u2015 and four left-wing hashtags \u2015 **#resist, #trumprussia, #impeachtrump,** and **#blacklivesmatter.** (I tried to mix hashtags that would get both mainstream and extreme perspectives from both sides.) The resulting dataset had 258,173 tweets from right-wing hashtags and 333,507 tweets from left-wing hashtags, collected beginning on March 3, 2017, and ending when each respective TAGS archive reached the data limit for its Google spreadsheet (in some cases three days, in others almost two weeks). It's an imperfect dataset, to be sure. But it's a good place to start.\n\nWhat did we find?\n\nLet's start with the money graph: the websites that most uniquely define right-wing and left-wing information sources in this Twitter archive. To determine this, I calculated an *odds ratio*, a statistical measurement that tells us if a website is more likely to pop up in right-wing tweets, left-wing tweets, or equally likely to pop up in either. This graphic shows the domains with the largest left-leaning and right-leaning odds ratios (on a logarithmic scale to make the visualization easier to read). Click the image to enlarge.\n\n<a href=\"/content/images/domains_log_odds.png\" target=\"blank_\"><img src=\"/content/images/domains_log_odds.png\" alt=\"Log odds ratios for the most characteristic right-wing and left-wing domains linked on Twitter in early March\" /></a>\n\nOn this graphic, we can see that left-wing tweets are biased in favor of Twitter, Instagram, and mainstream news sources. Though there are some left-leaning partisan sites on the list, the list is dominated by mainstream sources like the New York Times, Politico, Sky News, CNN, Washington Post, and The Guardian. On the right, however, there are **no mainstream news sources.** Aside from YouTube (mostly independent \"news\" sources), most links come from far-right sites like Breitbart, fake-news sites like TruthFeed, and ad-tech/malware sites like theatlantic.ga and USAToday24h.ml.\n\nThere are several important takeaways from this graphic. First **mainstream news media tend to appear more commonly in left-wing hashtags.** Second, though there are left-wing fake news sites further down the list, **the most characteristic right-wing sites include a large number of fake news sites.** However, I don't think this means that right-wing Twitter users are more likely to be sharers of misinformation per se. There's something else going on here. Specifically, **the presence of malware sites suggests a significant bot presence, particularly in right-wing hashtags.**\n\nThis is worth unpacking more.\n\nOne of the top fake/malware sites in this tweet corpus is theatlantic.ga. The domain appeared 2651 times in the corpus, more than TruthFeed (2384 times), Breitbart (2036 times), or the New York Times (1816 times). Only Twitter (89,218 times), YouTube (12,268 times), and a couple link shorteners appear more often. What is theatlantic.ga? If you visit the domain directly, it redirects you to adf.ly, a site whose motto is \"Get paid to share your links on the Internet!\" (Aside: there's a reason I'm not linking directly to these domains.) adf.ly (whose logo is a bee?!) offers services that help people sell advertising on their sites, \"Pay for real visitors on your website,\" and track aspects of the identity and behavior of those visitors. They also offer an API (application programming interface) that makes it easy to automate the process of generating and sharing links. The way it seems to work is similar to other link shorteners (like bit.ly), but instead of just making a long URL short and tracking clicks, it embeds ads along the way. Users who click on an adf.ly link see a brief ad on their way to the page they intended to go to, without embedding any ads on the actual page.\n\nBusinesses like adf.ly mean that\n\n1) **it's easy to advertise on your site with minimal technical knowledge,**  \n2) **it's easy to advertise on *other people's sites* by sharing links to them via social media,** and thus  \n3) **it's possible to make money off of ads on other people's sites without even having your own website.**\n\nAll you need is a Twitter account (or bot, or army of bots), and if you have an audience for those Twitter bots, you have people who will click on the links you provide, which will in turn generate \"advertising\" revenue for you, the bot owner. (Not the site owner.)\n\nSo what kind of pages are adf.ly sites like theatlantic.ga sending Twitter users to? The most frequently shared theatlantic.ga link is theatlantic.ga/7At (13 times). (DO NOT VISIT THAT LINK. IF YOU DO, BE SURE NOT TO CLICK ON ANYTHING!) Here's what that page looks like.\n\n<img src=\"/content/images/screenshot_theatlantic_ga1.png\" alt=\"Screenshot of sample page on theatlantic.ga\" />\n\nIt's a page that makes only a minimal attempt to be right-wing (the Trump campaign logo in the upper left), but instead masquerades as a browser security warning in order to get users to download malware. (I'm assuming it's malware. I didn't actually click on it!) When you try to close the page, you get a popup message discouraging you from leaving the page.\n\n<img src=\"/content/images/screenshot_theatlantic_ga2.png\" alt=\"Screenshot of popup on theatlantic.ga\" />\n\nWe've seen all this before. But in this Twitter dataset, we see it **a lot**, and we see it **more on right-wing hashtags than on left-wing hashtags.**\n\nSo what! People are spamming hashtags. We've seen that before, too. We can just ignore it. And it's probably not going to have an effect on people, right? I mean, this isn't misinformation, it's just spam. ... Right?\n\nNot quite.\n\nLet's look at some of the Twitter users who share these links. Here are the ten most prolific accounts tweeting links to theatlantic.ga.\n\n| user | number of tweets |\n| --: | :-- |\n| Imwithtrump4 | 282 |\n| Hopetotrump | 247 |\n| AllWithTrump5 | 241 |\n| FollowersofTru2 | 230 |\n| americaxtrump | 179 |\n| ALWAYSTRUE19 | 137 |\n| Changewithtrump | 104 |\n| Tru_republicans | 103 |\n| Republicanfore2 | 88 |\n| FloridaxTrump | 54 |\n\nI visited the first account, and lo and behold, it had been suspended! (Remember, those 282 tweets containing theatlantic.ga are only a couple weeks old.) The second account, Hopetotrump, in many ways looks like a typical right-wing Twitter user. Patriotic banner image, content consistent with right-wing ideas and ideals, and what looks like links to many right-wing and mainstream news media. But on closer inspection, we see that the links they are tweeting are all ```theatlantic.ga/something```. And that ```/something``` tends to be in sequential order. (Right now, the latest links are ```/Bz9```, ```/Bz6```, ```/Byq```, ```/Byp```, etc.) Each tweet contains a headline, an image, and a conservative hashtag. The hashtag puts the tweet in front of people who otherwise would not follow the account (this account as only 188 followers, but some of these accounts have tens of thousands), and the headline/image combination are designed to entice right-wing readers to click.\n\nThis account tweets very often (and doesn't take a break to sleep), so someone must be working hard to generate all that content, right? Take one of their tweets from this morning:\n\n<img src=\"/content/images/screenshot_fake_tweet.png\" alt=\"Screenshot of fake tweet linking to theatlantic.ga\" />\n\nI searched twitter for this text and found this:\n\n<img src=\"/content/images/screenshot_orig_tweet.png\" alt=\"Screenshot of original tweet linking to The Federalist Papers conservative website\" />\n\nIn fact, these feeds have a lot of tweets in common. It seems that accounts like Hopetotrump are simply mining conservative Twitter accounts for text and images, and substituting their own malware links and random conservative hashtags, with the goal of grabbing the attention of conservative readers and getting them to click on a link that will make them money. In fact, when you look in more detail at the data coming from this dataset, you can see that many of these malware sites own multiple bots that tweet exactly the same content.\n\nSo the recipe seems to be this:\n\n1) Mine existing conservative Twitter accounts for content.  \n2) Substitute a malware link for the original link, and append a commonly read conservative hashtag.  \n3) Tweet that new content out via multiple Twitter bots.  \n4) Collect data and money from adf.ly and/or malware.\n\nThe content is generated by others, there are minimal webhosting costs involved, and even the \"click-bait\" properties of the \"ads\" are engineered by others. Scripts abound on the internet for mining Twitter content and remixing it to make your own content. (I even have my students do this, albeit in more artful, ethical ways.) So this is a pretty simple way to make at least a few bucks on the internet without much technical knowledge.\n\nBut this still seems like classic spamming and phishing. What does this have to do with misinformation?\n\nBoth <a href=\"https://hapgood.us/2016/11/14/the-they-had-their-minds-made-up-anyway-excuse/\" target=\"blank_\">Mike Caulfield</a> and <a href=\"http://www.digitalpedagogylab.com/hybridped/truthy-lies-surreal-truths/\" target=\"blank_\">I have written</a> about the effects of *casual scrolling* on social media. When we see the same claim repeated often enough, especially when our guard is down, we slowly become more predisposed to believe it. Further, social media platforms go to significant lengths to ensure that content looks uniform, meaning that it doesn't matter how ugly TruthFeed.com looks. We see their tweets alongside, and with a similar look and feel to, tweets from legitimate sources. We also know that the more polarizing claims tend to get the most clicks, and thus make the best advertising fodder.\n\n<a href=\"/content/images/screenshot_truthfeed.png\" traget=\"blank_\"><img src=\"/content/images/screenshot_truthfeed.png\" alt=\"Unattractive design on truthfeed.com, which would typically be a sign of low quality content.\" /></a>\n\n\nThe sum of all this is that **the most polarizing claims are the most repeated by the bots, and as readers see them in aggregate over time, they become more plausible in our unconscious mind.** We may not click on the links, we may not even engage or believe the idea when the bot shares it. But once one of our friends, relatives, or colleagues shares that same claim \u2015 perhaps from the original site the spammers stole content from \u2015 **we will be that much more likely to believe the claim.** That gives the more polarizing, partisan sites more psychological power, and that lays the foundation for more purposeful \u2015 and successful \u2015 propaganda campaigns.\n\nThe more Bill and I study extremist communities online, the more convinced we are that **standard advertising practices play a significant role in the spread of misinformation.** Further, while many are rightly encouraging us to think critically about the information we consume and the sources we trust, it's also becoming clear to us that **mainstream news media is playing the same advertising game, and by the same rules, as the spammers and the spreaders of misinformation.** And as long as the pay-per-click advertising model rules the roost, and news sites (mainstream and extremist) continue to be the largest users of data-collecting adtech, online, data-driven propaganda will be a real possibility, not to mention a lucrative business opportunity.\n\nAnd that has to change.\n\nMore on that in future posts...\n\n<i>Header image by <a href=\"https://www.pexels.com/photo/administration-articles-bank-black-and-white-261949/\" target=\"blank_\">=Pixabay</a> (CC0).</i>\n",
                        "html": "",
                        "image": "/content/images/newspapers.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Fake news, adtech, and the spread of misinformation",
                        "meta_description": "The more we study extremist communities online, the more convinced we are that standard advertising practices play a significant role in the spread of misinformation.",
                        "author_id": 1,
                        "created_at": "2017-03-22 11:03:00 -0500",
                        "created_by": 1,
                        "updated_at": "2017-03-22 11:03:00 -0500",
                        "updated_by": 1,
                        "published_at": "2017-03-22 11:03:00 -0500",
                        "published_by": 1,
                        "og_title": "Fake news, adtech, and the spread of misinformation",
                        "twitter_title": "Fake news, adtech, and the spread of misinformation",
                        "og_description": "The more we study extremist communities online, the more convinced we are that standard advertising practices play a significant role in the spread of misinformation.",
                        "twitter_description": "The more we study extremist communities online, the more convinced we are that standard advertising practices play a significant role in the spread of misinformation.",
                        "og_image": "/content/images/newspapers.jpg",
                        "twitter_image": "/content/images/newspapers.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Data mining the New York Philharmonic performance history",
                        "slug": "data-mining-the-new-york-philharmonic-performance-history",
                        "markdown": "\nThe New York Philharmonic has a public dataset containing <a href=\"https://github.com/nyphilarchive/PerformanceHistory/\" target=\"blank_\">metadata for their entire performance history</a>. I recently discovered this, and of course downloaded it and started to geek out over it. (On what was supposed to be a day off, of course!) I only explored the data for a few hours, but was able to find some really interesting things. I'm sharing them here, along with the code I used to do them (in <a href=\"https://www.r-project.org/\" target=\"blank_\">R</a>, using <a href=\"http://tidyverse.org/\" target=\"blank_\">TidyVerse</a> tools), so you can reproduce them, or dive further into other questions. (If you just want to see the results, feel free to skip over the code and just check out the visualizations and discussion below.)\n\n<i>All scripts, extracted data, and visualizations in this blog post can also be found in the <a href=\"https://github.com/kshaffer/nyphil\" target=\"blank_\">GitHub repository</a> for this project.</i>\n\n## Downloading the data\n\nFirst, here are the R libraries that I use in the code that follows. If you're going to run the code, you'll need these libraries.\n\n~~~r\nlibrary(jsonlite)  \nlibrary(tidyverse)  \nlibrary(tidytext)  \nlibrary(stringr)  \nlibrary(scales)  \nlibrary(tidyjson)  \nlibrary(purrr)  \nlibrary(lubridate)  \nlibrary(broom)\n~~~\n\nTo load the NYPhil performance data into R, you can download it from GitHub and load it locally, or just load it directly into R from GitHub. (I chose the latter.)\n\n~~~r\nnyp <- fromJSON('https://raw.githubusercontent.com/nyphilarchive/PerformanceHistory/master/Programs/json/complete.json')\n~~~\n\nNow their entire performance history is in a data frame called ```nyp```!\n\n## Tidying the data\n\nThe performance history is organized in a hierarchical format \u2015 more-or-less lists of lists of lists. (See the <a href=\"https://github.com/nyphilarchive/PerformanceHistory/\" target=\"blank_\">README file</a> on GitHub for an explanation.) It's an intuitive way to organize the data, but it makes it difficult to do exploratory data analysis. So I spent more time than I care to admit unpacking the hierarchical structure into a flat, two-dimensional \"tidy\" structure, where each row is an *observation* (in this case, a piece of music that appears on a particular program) and each column is a *variable* or *measurement* (in this case, things like title, composer, date of program, performance season, conductor, soloist(s), performance venue, etc.).\n\nGetting from the hierarchical structure to a tidy data frame was something of a challenge. There are a number of different kinds of lists embedded in the JSON structure, not all of which I wanted to worry about. So I poked around for a while and then created some functions to extract the info I wanted and assign a single row to each piece on a particular program, which would include all of the pertinent details. Here are the custom functions for expanding the list of metadata for a musical work, and then reproducing the general program information for each work on that program. (Note that I left the soloist field included, but still as a list. I'm not planning on using it, but I left it in for future possibilities.)\n\n~~~r\nwork_to_data_frame <- function(work) {  \n  workID <- work['ID']  \n  composer <- work['composerName']  \n  title <- work['workTitle']  \n  movement <- work['movement']  \n  conductor <- work['conductorName']  \n  soloist <- work['soloists']  \n  return(c(workID = workID,  \n           composer = composer,  \n           title = title,  \n           movement = movement,  \n           conductor = conductor,  \n           soloist = soloist))  \n}  \n\nexpand_works <- function(record) {  \n  if (is_empty(record)) {  \n    works_db <- as.data.frame(cbind(workID = NA,  \n                                    composer = NA,  \n                                    title = NA,  \n                                    movement = NA,  \n                                    conductor = NA,  \n                                    soloist = NA))  \n    } else {  \n      total <- length(record)  \n      works_db <- t(sapply(record[1:total], work_to_data_frame))  \n      colnames(works_db) <- c('workID',  \n                              'composer',  \n                              'title',  \n                              'movement',  \n                              'conductor',  \n                              'soloist')  \n    }  \n  return(works_db)  \n}  \n\nexpand_program <- function(record_number) {  \n  record <- nyp$programs[[record_number]]  \n  total <- length(record)  \n  program <- as.data.frame(cbind(id = record$id,  \n                                 programID = record$programID,  \n                                 orchestra = record$orchestra,  \n                                 season = record$season,  \n                                 eventType = record$concerts[[1]]$eventType,  \n                                 location = record$concerts[[1]]$Location,  \n                                 venue = record$concerts[[1]]$Venue,  \n                                 date = record$concerts[[1]]$Date,  \n                                 time = record$concerts[[1]]$Time))  \n  works <- expand_works(record$works)  \n  return(cbind(program, works))  \n}\n~~~\n\nThen I used a loop to iterate these functions over the entire dataset (13771 records through the end of 2016 when I downloaded it, but this is a dynamic dataset that expands as new programs are performed), then save it to CSV and make it into a tibble (a TidyVerse-friendly data frame).\n\n~~~r\ndb <- data.frame()  \nfor (i in 1:13771) {  \n  db <- rbind(db, cbind(i, expand_program(i)))  \n}  \n\ntidy_nyp <- db %>%\n  as_tibble() %>%\n  mutate(workID = as.character(workID),\n         composer = as.character(composer),\n         title = as.character(title),\n         movement = as.character(movement),\n         conductor = as.character(conductor),\n         soloist = as.character(soloist))\n\ntidy_nyp %>%\n  write.csv('ny_phil_programs.csv')\n~~~\n\nThis takes a *looooooong* time to process on a dual-core PC, which is why I was sure to save the results immediately for reloading in the future. Normally I would write a function that could be vectorized (processed on each value in parallel), which takes advantage of R's (well, really C's) high-efficiency matrix multiplication capabilities. However, because the input (one record per concert program) and output (one record per piece per program) were necessarily different lengths, I couldn't make that work. *If you know how to do that, please drop me an email or tweet and I'll be eternally grateful!*\n\nAfter a cup of coffee, or maybe two!, I have a handy tibble of almost 82,000 performance records from the entire history of the NY Philharmonic!\n\n## Most common composers and works\n\nWith this tidy tibble, we can really easily find and visualize basic descriptive statistics about the dataset. For example, what composers have the most works in the corpus? Here are all the composers with 400 or more works performed, in order of frequency.  \n\n<img src=\"/content/images/nyphil_composers.png\" />\n\nThis is produced by running the following code.\n\n~~~r\ntidy_nyp %>%  \n  filter(!composer %in% c('NULL', 'Traditional,', 'Anthem,')) %>%  \n  count(composer, sort=TRUE) %>%  \n  filter(n > 400) %>%  \n  mutate(composer = reorder(composer, n)) %>%  \n  ggplot(aes(composer, n, fill = composer)) +  \n  geom_bar(stat = 'identity') +  \n  xlab('Composer') +  \n  ylab('Number of works performed') +  \n  theme(legend.position=\"none\") +  \n  coord_flip()\n~~~\n\nI was surprised to see Wagner on top, even ahead of Beethoven. Tchaikovsky was also a big surprise to me. He's popular, but I've ushered or attended over 200 performances of the Chicago Symphony Orchestra, and Beethoven and Mozart are definitely performed more recently than Wagner and Tchaikovsky by the CSO today. So is this a NYP/CSO difference? Many of my music theory & history friends on Twitter were also surprised to see this ordering, so maybe not. In that case, have things changed over time?\n\nBefore looking at trends over time, let's see if looking at specific works can shed any light. Here are the most performed works (and the code to produce the visualization), correcting for multiple movements listed from the same piece on the same program.\n\n<img src=\"/content/images/nyphil_pieces.png\" />\n\n~~~r\ntidy_nyp %>%  \n  filter(!title %in% c('NULL')) %>%  \n  mutate(composer_work = paste(composer, '-', title)) %>%  \n  group_by(composer_work, programID) %>%  \n  summarize(times_on_program = n()) %>%  \n  count(composer_work, sort=TRUE) %>%  \n  filter(n > 220) %>%  \n  mutate(composer_work = reorder(composer_work, n)) %>%  \n  ggplot(aes(composer_work, n, fill = composer_work)) +  \n  geom_bar(stat = 'identity') +  \n  xlab('Composer and work') +  \n  ylab('Number of times performed') +  \n  theme(legend.position=\"none\") +  \n  coord_flip()\n~~~\n\nThere are a lot of Wagner operas at the top! (Though it's worth noting that only a few instances of each are full performances. Instead, most are just the overture or prelude, a common way of opening out a symphony concert.) While many of Wagner's most performed works are very short (10-minute overtures compared to 30-to-60-minute Beethoven and Tchaikovsky symphonies), and thus Beethoven probably occupies more *time* on the program than Wagner, the high number of Wagner, and even Tchaikovsky, pieces on NY Phil programs is still surprising to me.\n\n## Changes over time\n\nLet's see how things have changed over time. We can start simply by comparing their early history to their late history. Here are composer counts from 1842 to 1929 and 1930 to 2016 (roughly equal timespans, though not equal numbers of pieces).\n\nPre-1930:\n\n<img src=\"/content/images/nyphil_composers_pre1930.png\" />\n\nAnd post-1929:\n\n<img src=\"/content/images/nyphil_composers_post1929.png\" />\n\nTo do this, I simply added another filter to tidy_nyp:\n\n~~~r\nfilter(as.integer(substr(as.character(date),1,4)) < 1930) %>%\n~~~\n\nHere we see Beethoven, Tchaikovsky, and Mozart all ahead of Wagner in more recent history, with Wagner dominating (and Mozart missing from) the earlier history.\n\nBut we can model this with more nuance. Let's make a new tibble that contains just the information we need on composer frequency year-by-year.\n\n~~~r\ncomp_counts <- tidy_nyp %>%  \n  filter(!composer %in% c('NULL', 'Traditional,', 'Anthem,')) %>%  \n  mutate(year = as.integer(substr(as.character(date),1,4))) %>%  \n  group_by(year) %>%  \n  mutate(year_total = n()) %>%  \n  group_by(composer, year) %>%  \n  mutate(comp_total_by_year = n()) %>%  \n  ungroup() %>%  \n  group_by(composer, year, comp_total_by_year, year_total) %>%  \n  summarize() %>%  \n  mutate(share = comp_total_by_year/year_total) %>%  \n  group_by(year) %>%  \n  mutate(average_share = mean(share))\n~~~\n\nThis produces a tibble that contains a record for each composer-year combination, with fields for:  \n- composer name    \n- year    \n- number of pieces by that composer in that year    \n- total number of pieces for the year    \n- composer's share of pieces for the year    \n- average composer share for the year (total / number of composers)    \n\n\nWith this information, we can then plot the changing frequency of each composer. Here are the top four on a single plot.\n\n<img src=\"/content/images/nyphil_top4.png\" />\n\nWe can very clearly see the change in these composers' frequency of occurrence on the NY Phil's program over time, with Wagner's decline very pronounced, and Mozart's rise (in the twentieth century) clearly evident as well.\n\nHowever, comparing a composer's share of the programming year by year isn't always apples-to-apples. Early on in the Philharmonic's history, seasons contained far fewer pieces, and thus far fewer composers, than recent years. This has the potential to provide artificially high numbers for composers in sparser years, as seen in the following visualization (and accompanying code).\n\n<img src=\"/content/images/nyphil_composers_per_year.png\" />\n\n~~~r\ncomp_counts %>%  \n  group_by(year) %>%  \n  summarize(comp_per_year = n()) %>%  \n  ggplot(aes(year, comp_per_year)) +  \n  geom_line() +  \n  xlab('Year') +  \n  ylab('Composers appearing on a program')\n~~~\n\nTo account for this, we can normalize a composer's share of the repertoire in a given year by dividing it by the average repertoire share for composers in the year. So here is the changing normalized frequency for each of the top four composers on a year-by-year basis.\n\n<img src=\"/content/images/nyphil_top4_normalized.png\" />\n\nThe same trends can be seen here \u2015 Mozart's gentle rise and Wagner's drastic decline \u2015 perhaps even more starkly. In particular, Wagner's decline from a peak in 1921 to a trough in the 1960s stands out quite strikingly. The decline is the most precipitous in the late 1940s and early 1950s.\n\nAnd now an explanation begins to emerge.\n\nA number of musicians began to boycott or avoid performing the music of Richard Wagner in the late 1930s, <a href=\"https://www.theguardian.com/music/2002/sep/06/classicalmusicandopera.artsfeatures\" target=\"blank_\">as recounted by conductor Daniel Barenboim</a>. Wagner was known as \"Hitler's favorite composer,\" and his music was used prominently in the Reich. The Israel Philharmonic stopped performing his music in 1938, Arturo Toscanini (who occupies a not insignificant share of this dataset as a conductor) stopped performing at Wagner festivals in Bayreuth, etc. Looking at the NY Philharmonic data, it seems like this may be a broader trend.\n\nIn addition to Wagner's decline between WWI and the early Cold War, we can see another significant wartime change, this time an increase. From 1939 to 1946, Tchaikovsky's share of the NY Philharmonic's repertoire rose precipitously to his highest (normalized) share in the entire corpus. Could this be due to Russia's role in the Grand Alliance? I don't know. I *do* know that during World War II, then-living Russian composer Dmitri Shostakovich was widely performed in the US as part of a pro-Russia, anti-Nazi wartime propaganda effort (see below). Could Tchaikovsky have been part of that? I don't know the history of it. But I wouldn't be surprised. I also wouldn't be surprised if Tchaikovsky simply filled the role of popular, grand, Romantic composer ... who wasn't German. (Any Tchaikovsky scholars have a perspective to add?)\n\n<img src=\"/content/images/nyphil_russians_wartime.png\" />\n\n## Conclusion\n\nThis is just a start, but I think they're interesting findings. As a music student and scholar, I never studied performance trends like this. My studies were mostly focused on musical structures and the evolution of compositional styles. But it's cool to take a different kind of empirical look at musical evolution.\n\nIf this code helps you find other insights in the corpus, please drop me a line. I'm sure there's much more to be mined out of this fascinating corpus.\n\nAnd thanks to the archivists of the New York Philharmonic for putting this together! Hopefully more major orchestras will release their programming history publicly, so we can start mapping larger trends and make comparisons between them.\n\n<i>Banner image by <a href=\"https://www.flickr.com/photos/downthetrack/7871525476/\" target=\"blank_\">Tim Hynes</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/stoneStacks.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Data mining the New York Philharmonic performance history",
                        "meta_description": "How does war affect the music an orchestra plays?",
                        "author_id": 1,
                        "created_at": "2017-01-19 17:02:00 -0500",
                        "created_by": 1,
                        "updated_at": "2017-01-19 17:02:00 -0500",
                        "updated_by": 1,
                        "published_at": "2017-01-19 17:02:00 -0500",
                        "published_by": 1,
                        "og_title": "Data mining the New York Philharmonic performance history",
                        "twitter_title": "Data mining the New York Philharmonic performance history",
                        "og_description": "How does war affect the music an orchestra plays?",
                        "twitter_description": "How does war affect the music an orchestra plays?",
                        "og_image": "/content/images/stoneStacks.jpg",
                        "twitter_image": "/content/images/stoneStacks.jpg"
                    },
                    {
                        "id": 0,
                        "title": "What is computational musicology?",
                        "slug": "computational-musicology",
                        "markdown": "\nComputational musicology is a young, growing field \u2015 one in which I find myself increasingly involved. In addition to [The Lieder Project](http://liederproject.shaffermusic.com/), I will be teaching my course on computational music analysis again this summer, and two students and I will be giving a presentation on it for the College of Music colloquium series later this spring. Since it is a young field, and draws on so many disciplines, I often find myself asked just what computational musicology is. As far as I know, there isn't a public-facing introduction to the field. So with both my upcoming class and these informal interactions in mind, I thought I'd write one. So here goes...\n\n## What is computational musicology?\n\nIn a nutshell, *computational musicology* \u2015 more-or-less synonymous with *computational music theory* or *music informatics* \u2015 is the use of computational methods and statistics to analyze musical structures (notes, chords, rhythms, etc., and patterns thereof). This combination of computation, statistics, and a domain of knowledge makes computational musicology a form of *data science*. However, due to the music theoretical aspect, computational musicology sits firmly within the *digital humanities* and focuses on the same kinds of questions as traditional humanities research. (Though, because of the differing methods, the specific questions are often different.) Like other digital humanists, computational musicologists use (and make) digital tools to explore, ask, and answer questions about human artifacts \u2015 in this case, the structural elements of musical works and (meta)data about human interaction with music.\n\n## Types of computational musicology projects\n\n**Corpus studies.** Corpus studies are possibly the most common type of project in computational musicology. A corpus study uses software to analyze statistical patterns in a large collection \u2015 corpus \u2015 of musical works. It is, essentially, descriptive statistics for musical data. Like text-based corpus studies, musical corpus studies often use n-gram and cluster analysis methods. Unlike text-based corpus studies, musical corpus studies often involve Markov models \u2015 probability analyses for progressions in time, such as how likely is a C-major chord to progress to a D-minor chord in a piece in the key of A minor.\n\n**Modeling.** More recently, computational musicologists have been employing more advanced statistical methods to uncover underlying functions and patterns that contribute to the \"surface\" musical features of pieces in their corpora. This may involve Bayesian statistics, and more specifically, a hidden Markov model: for example, what are the underlying chordal *functions* in a chord progression, and do the traditional T/S/D functions explain the actual chord progressions observed in a corpus.\n\n**Music encoding.** In order to perform a computational analysis, musical data must be encoded digitally in a way that makes sense both for the music and for the analysis to be performed. This can be a complicated and subjective task for musical artifacts.\n\n**Music information retrieval.** Encoding musical data by hand is incredibly time-consuming. Music informational retrieval (MIR) is an attempt to automate that process, accurately extracting information from musical scores and, especially, audio. MIR draws on electrical engineering, machine learning, and digital signal processing (DSP).\n\n## Example projects and resources\n\nThat's just a brief overview. For more details, check out some of the following projects, resources, and articles.\n\nBurgoyne, John Ashley. 2011. [\"Stochastic Processes & Database-Driven Musicology.\"](http://oatd.org/oatd/record?record=oai%5C:digitool.library.mcgill.ca%5C:107704) Ph.D. diss., McGill University.\n\n[Music Genre and Spotify Metadata](http://scholarslab.org/uncategorized/music-genre-and-spotify-metadata/).\n\nDe Clercq, Trevor and David Temperley. 2011. [\"A corpus analysis of rock harmony.\"](http://dx.doi.org/10.1017/S026114301000067X) In *Popular Music* 30/1, pp. 47\u201370.\n\nYim, Gary. 2012. [\"Affordant Harmony in Popular Music: Do Physical Attributes of the Guitar Influence Chord Sequences?\"](http://icmpc\u2015escom2012.web.auth.gr/sites/default/files/papers/1156_Proc.pdf) In *Proceedings of the 12th International Conference on Music Perception and Cognition and the 8th Triennial Conference of the European Society for the Cognitive Sciences of Music*, July 23\u201328, 2012.\n\n[The Lieder Project.](http://liederproject.shaffermusic.com/) A collaborative, computational research project looking at the relationship between the sounds of poetry and the structure of the music it to which it is set. David Lonowski (CU\u2013Boulder), Jordan Pyle (CU\u2013Boulder), Stephen Rodgers (U Oregon), Kris Shaffer (CU\u2013Boulder), Leigh VanHandel (Michigan State U).\n\n[CorpusMusic group on GitHub](http://github.com/corpusmusic).  \n\n[Python Hidden Markov Model](http://www.cs.colostate.edu/~hamiltom/code.html) \u2015 Michael Hamilton, Colorado State University (the basis for unpublished HMM musical studies by Christopher William White and Ian Quinn, presented at the Society for Music Theory).  \n\n[The McGill Billboard Project](http://ddmal.music.mcgill.ca/billboard) \u2015 a large data set of harmonic annotations of songs from several decades of the Billboard Top 100 list.  \n\n[\"A Corpus Study of Rock Music\"](http://theory.esm.rochester.edu/rock_corpus/) \u2015 David Temperley and Trevor de Clercq's data and programs for analyzing the harmonic progressions in a corpus of 200 songs from Rolling Stone magazine's \"Greatest [Rock] Songs of All Time.\"  \n\n[The Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/) \u2015 \"a freely-available collection of audio features and metadata for a million contemporary popular music tracks\" (Columbia Univ.).  \n\n[Reference Annotations: The Beatles (and others)](http://isophonics.net/content/reference-annotations-beatles) \u2015 a substantial collection of annotations for The Beatles catalogue, including chords, beats, keys, and large-scale structure.  \n\n[*Empirical Musicology Review*](http://emusicology.org/).\n",
                        "html": "",
                        "image": "/content/images/bletchley.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "What is computational musicology?",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-01-16 17:34:00 -0700",
                        "created_by": 1,
                        "updated_at": "2016-01-16 17:34:00 -0700",
                        "updated_by": 1,
                        "published_at": "2016-01-16 17:34:00 -0700",
                        "published_by": 1,
                        "og_title": "What is computational musicology?",
                        "twitter_title": "What is computational musicology?",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/bletchley.jpg",
                        "twitter_image": "/content/images/bletchley.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Trump's immigration language",
                        "slug": "trump-on-immigration",
                        "markdown": "\nDonald Trump talks about immigration in some uniquely horrifying ways. We're all familiar with his \"not sending us their best people\" speech from early in his campaign. But how has his language developed over time? And when looking specifically at *official White House materials*, how extreme is his rhetoric?\n\nFollowing the same methods as <a href=\"/2017/03/trumping-science/\" target=\"blank_\">my earlier post analyzing the Trump administrations language about science and the environment</a>, I analyzed the full text of whitehouse.gov (downloaded on March 1) and compared its language about immigration with the Obama administration's site (as of the morning of January 20). Here's how they compare.\n\nThe data\nI already had both websites parsed by *bigram* (two-word phrases) with stop words (\"a\", \"an\", \"the\", \"of\", etc.) removed. To find language about immigration, I searched that parsed archive for bigrams containing the text \"migra\". This will extract any phrases containing the words \"immigration\", \"immigrant\", \"immigrants\", as well as \"migrant\" and \"migration\", verbs like \"immigrate\" and \"migrate\", and Spanish-language text like \"immigracion\".\n\nThis search for \"migra\" returns **624 bigrams on Trump's website** and **12,370 bigrams on Obama's website.** Obama's website is about three times the size of Trump's March 1 site, and yet mentions immigration about 20 times more often than Trump's site.\n\nHere are the ten most frequent immigration bigrams on Obama's site:\n\n| bigram | count |\n| --: | :-- |\n| immigration action | 5009 |\n| deal immigration | 4956 |\n| immigration reform | 222 |\n| immigration system | 192 |\n| security immigration | 191 |\n| immigration poverty | 90 |\n| broken immigration | 82 |\n| care immigration | 72 |\n| comprehensive immigration | 60 |\n| immigration refinancing | 50 |\n\nA full 80% of the bigrams on immigration on Obama's site are comprised of just two phrases: \"immigration action\" and \"deal immigration\". It turns out that these come from a common header on a number of pages on the website, containing links to a page about the \"Iran Deal\" and \"Immigration Action\". Skipping those headers brings the Obama-Trump proportion of immigration mentions far closer to the expected 3-to-1, and then the rest of this list confirms what many of us already knew: the Obama administration was concerned about current immigration policy in the US, and they desired to \"reform\" the current, \"broken\" immigration system.\n\nHere are the top ten bigrams on Trump's website relating to immigration:\n\n| bigram | count |\n| --: | :-- |\n| immigration laws | 76 |\n| illegal immigration | 37 |\n| immigration enforcement | 37 |\n| federal immigration | 35 |\n| illegal immigrants | 29 |\n| immigration law | 23 |\n| illegal immigrant | 19 |\n| immigration system | 18 |\n| immigration reform | 16 |\n| undocumented immigrants | 16 |\n\nWhile the idea of reforming a broken system can also be seen on Trump's site, there seems to be a greater focus on the (negatively judged) status of individual immigrants \u2015 illegal, undocumented \u2015 as well as the enforcement of immigration laws \u2015 think ICE raids, Customs and Border Patrol actions at international airports, travel bans, etc.\n\nComparing top-10 lists can provide interesting results, but there are statistical measures that can provide a more robust sense of what the most distinctive language of each administration is. Following the procedure described in the previous blog post in this series, here are the most distinctive bigrams containing \"migra\" in the Trump and Obama administrations' websites, calculated by a <a href=\"https://en.wikipedia.org/wiki/Odds_ratio\" target=\"blank_\">log odds ratio</a>, with phrases removed that were typically (or exclusively) the results of page headers (\"immigration iran\" was the result of a link for \"immigration\" preceding one for \"Iran deal\", similarly for \"immigration rural\", \"immigration climate\", \"care immigration\", and several others).\n\n<a href=\"/content/images/migra_Mar1.png\" target=\"blank_\"><img src=\"/content/images/migra_Mar1.png\" alt=\"Log odds ratio: most distinctive two-word phrases containing 'migra' in Trump and Obama administration websites\" /></a>\n\nComparing the most distinctive immigration-related phrases on these two websites reveals several important trends. First, **the Trump administration is more heavily focused on the enactment and enforcement of immigration law** \u2015 in other words, keeping (and kicking) immigrants out. Second, **the Trump administration is more likely to label immigrants as illegal.** On the other side, we see that **the Obama administration is more focused on speaking *to* immigrants,** with significant Spanish-language content on the site. Also, **the Obama administration is more focused on reforming a broken system through \"commonsense\" and \"comprehensive\" means,** presumably legislation.\n\nThis distills into two main realizations:\n\n- The Obama administration saw *immigration law* as the problem in need of fixing.  \n- The Trump administration sees *immigrants* as the problem that needs fixing.  \n\nThe result is a greater emphasis of the Obama administration on the need for changes in the law that help immigrants join American society in meaningful, \"commonsense\" ways, using legislation as the vehicle for this change, and an emphasis of the Trump administration on getting and keeping certain people out of the country, using various law enforcement agencies to accomplish this.\n\nAs with all \"distant readings\", this is a generalization, and there are a lot of important nuances about the two administrations and their specific actions that cannot be glazed over. However, in this time of non-stop, head-spinning news, I find it incredibly valuable to take a step back and get a broad view of what's going on.\n\nAs with science, I find this broad view of Trump's approach to immigration disturbing, even more so when compared with the Obama administration. While I agree with Obama that *immigration policy* is an issue in need of both scrutiny and reform, I find Trump's view reprehensible \u2015 the idea that *immigrants themselves* are the problem that needs to be solved. And as long as Trump keeps talking up immigrants as the problem in need of a solution, people will continue to get hurt, no matter how many laws are passed or court cases are tried.\n\n*All data and code for this analysis can be downloaded from my <a href=\"https://github.com/kshaffer/whitehouse\" target=\"blank_\">whitehouse GitHub repository</a>.*\n\n*Header image by <a href=\"https://unsplash.com/photos/jS09rLLu-kM\" target=\"blank_\">Nico Beard</a>.*\n",
                        "html": "",
                        "image": "/content/images/policeLine.jpeg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Trump's immigration language",
                        "meta_description": "Donald Trump talks about immigration in some uniquely horrifying ways.",
                        "author_id": 1,
                        "created_at": "2017-03-29 12:12:00 -0500",
                        "created_by": 1,
                        "updated_at": "2017-03-29 12:12:00 -0500",
                        "updated_by": 1,
                        "published_at": "2017-03-29 12:12:00 -0500",
                        "published_by": 1,
                        "og_title": "Trump's immigration language",
                        "twitter_title": "Trump's immigration language",
                        "og_description": "Donald Trump talks about immigration in some uniquely horrifying ways.",
                        "twitter_description": "Donald Trump talks about immigration in some uniquely horrifying ways.",
                        "og_image": "/content/images/policeLine.jpeg",
                        "twitter_image": "/content/images/policeLine.jpeg"
                    },
                    {
                        "id": 0,
                        "title": "Visualizing the network that connects mainstream and extremist news",
                        "slug": "visualizing-adtech-network",
                        "markdown": "\n\nOnline advertising plays a crucial role in the spread of misinformation. As <a href=\"https://funnymonkey.com/2017/adtech-and-misinformation-the-middlemen-who-sell-to-all-sides\" target=\"blank_\">Bill Fitzgerald wrote</a> last week,\n\n> Right now, the status quo in adtech is to sell to all sides, and profit from both the arms race and the battles. While our discourse and news ecosystem remains mired in misinformation, adtech pulls profit.Adtech profits when we read lies, and adtech allows liars to earn revenue.Adtech profits when we read hate speech, and adtech allows the people who spread hate to earn revenue.Adtech profits when places like the Huffington Post convince writers to publish for \"exposure,\" and adtech allows the Huffington Post to generate revenue for these exploitive practices.Adtech profits when people read traditional news outlets, and adtech allows these news outlets to generate revenue.\n\nIn <a href=\"http://pushpullfork.com/2017/03/fake-news-adtech-misinformation/\" target=\"blank_\">my follow-up post</a> on adtech and misinformation, I concluded\n\n> The more Bill and I study extremist communities online, the more convinced we are that **standard advertising practices play a significant role in the spread of misinformation.** Further, while many are rightly encouraging us to think critically about the information we consume and the sources we trust, it\u2019s also becoming clear to us that **mainstream news media is playing the same advertising game, and by the same rules, as the spammers and the spreaders of misinformation.** And as long as the pay-per-click advertising model rules the roost, and news sites (mainstream and extremist) continue to be the largest users of data-collecting adtech, online, data-driven propaganda will be a real possibility, not to mention a lucrative business opportunity.\n\nWe both shared a lot of data and examples of what we've been finding as we research the spread of misinformation and extremism online. But what does this misinformation network *look like?*\n\nHere's what it looks like...\n\n<a href=\"/content/images/ad_network.png\" target=\"blank_\"><img src=\"/content/images/ad_network.png\" alt=\"a network graph of 25 different news websites and the adtech services they call in the background when visiting pages\" /></a>\n\nThis network graph (click the image to enlarge) takes <a href=\"https://gist.github.com/billfitzgerald/5965a6009a9b939f4155cffea2fe8170\" target=\"blank_\">the data from Bill's analysis</a> and arranges it spatially. Each node (dot) in this network represents a domain on the web. The arrows tell you what happens to your data when you visit one of those domains. Darker arrows signify more data requests. (For the sake of clarity, we only included connections made at least ten times in the course of visiting three pages on a site.)\n\nFor example, when you visit the New York Times, it sends requests for data from its own network of servers, as well as Google (both Google Syndication and Doubleclick.net), 2mdn.net, and a variety of other ad servers. When you visit a page on RT, it makes requests from even more sites. And while most of these requests are requests *for* data, most of them also involve *sending* data. Data about your location, your browser, your window size, the network you're accessing their website from, the page that you're on, what you clicked on, if you're logged into Google or Facebook, if you have a cookie from one of their partners installed, etc. This is all in the service of providing you a more \"personalized\" experience (without, or so they claim, using your truly \"personal\" data). It's also in the service of providing themselves revenue in the form of advertising, and in some cases, data collection with the purpose of trading or selling that data.\n\nThis network graph also shows something beyond simply where your data goes \u2015 which, I'll admit, was frightening the first time I saw it. It shows how your data can travel between entities, if any of those entities are so inclined to share, trade, or sell it. That realization \u2015 that an entity *I did not explicitly decide to send my data to* would both receive my data *and* be in the position to share it with others \u2015 frightened me. Still frightens me. It's the reason I have ad blockers, <a href=\"https://www.eff.org/https-everywhere\" target=\"blank_\">HTTPS Everywhere</a>, <a href=\"https://www.eff.org/privacybadger\" target=\"blank_\">Privacy Badger</a>, <a href=\"https://duckduckgo.com/\" target=\"blank_\">DuckDuckGo</a>, <a href=\"https://www.torproject.org/\" target=\"blank_\">Tor</a>, and other privacy tools. Because, as Bill wrote and this graph shows, **ad brokers work indiscriminately with all kinds of content providers.** And it doesn't take much for data collected while I browsed my favorite news source to make it into the hands of Breitbart (associated with, though no longer managed by, White House Chief of Staff, Steve Bannon) or RT (formerly *Russia Today*, but still <a href=\"https://en.wikipedia.org/wiki/Russia_today\" target=\"blank_\">funded by Putin's government</a>).\n\nSomething else this graph hints at is the similar associations that different sites have with adtech vendors. The algorithm that places nodes in the graph promotes clarity by putting nodes with similar associations near each other, and thus nodes with more connections near the center. This is why the most-connected nodes (many of which are owned by Google) are in the center, and why many of the far-right sources are clumped together, as are the mainstream news networks. But that representation is imperfect, being somewhat incidental. So I made a second visualization that shows which websites have the most adtech connections in common.\n\n<a href=\"/content/images/ad_tech_similarity.png\" target=\"blank_\"><img src=\"/content/images/ad_tech_similarity.png\" alt=\"Heatmap showing the similarity of adtech sources for 25 different news websites.\" /></a>\n\nFor this visualization, I took the list of background requests made by each site, and compared them all to each other for similarity. For example, when visiting three pages on Breitbart, Bill's analysis found Breitbart making 87 background requests. RT, on the other hand, made 121. 51 of the requests made by each of those sites went to the same domains. That's **59% of Breitbart's requests going to servers called by RT, and 42% of RT's requests going to servers called by Breitbart.** To come up with a single number, I simply multiplied the two values together (0.59 * 0.42) to get a share product of roughly 0.247. (<a href=\"https://www.researchgate.net/publication/220723600_Probabilistic_Combination_of_Features_for_Music_Classification\" target=\"blank_\">The product method</a> ensures both simplicity and that very small numbers don't exert undue influence on the result.) I calculated a share product for every pair of sites in Bill's dataset, and produced the heat-map above, with darker blue squares representing higher degrees of overlap in the background requests made by these sites.\n\nSome of the results aren't particularly surprising. I would expect conservative sites like Patriot Post and Daily Stormer to have a strong degree of similarity. I'm also not surprised by the fact that a mainstream news source like The Atlantic has its most significant overlap with the New York Times and the Washington Post.\n\nBut look at The Guardian. It most strongly overlaps with Daily Stormer and YouTube, which is turning out in our research to lean very far right as an information source on social media. Perhaps more alarming, though, look at the rows represented by mainstream sites like the New York Times, The Atlantic, or the Washington Post. There's a lot of blue in each of those rows! That's because **mainstream news media are among the most prevalent users of adtech.** And thus, they are more likely to overlap significantly with the adtech choices of more fringe sites.\n\nNow mainstream media have significant motivation to safeguard the data of their readers, both to keep them as customers and to maintain their credibility as a source of good information. But what about the adtech they use? Would you feel particularly violated if adnxs.com or 2mdn.net were in the news tomorrow as the source of a major data breach? And yet, outside of Google, those are two of the most prevalent sources of adtech in the sites we examined. The integrity of these ad sources are crucial to our ability to own, control, and safeguard our personal information. If any one of them fails, we could be in a heap of trouble. And yet **just stopping by the Washington Post to read one or two articles could potentially expose information about us to dozens of different companies, most of which we know nothing about,** and most of which do business with sites run by extremists or \u2015 in the case of RT \u2015 a hostile foreign government.\n\nLike I said in my last post, this has to change. But first, we need to grok it. It can be hard thing to get our heads around, and a scary thing to ponder. But especially as the US government increases its surveillance activities and <a href=\"http://www.pcworld.com/article/3184410/security/senate-votes-to-kill-fccs-broadband-privacy-rules.html\" target=\"blank_\">makes corporate surveillance easier to do without our knowledge</a>, it's more important than ever that we get our heads around this, so we can resist.\n\nGood night. And good luck.\n\n<i>Header image by <a href=\"https://www.pexels.com/photo/timelapse-photography-of-vehicle-on-concrete-road-near-in-high-rise-building-during-nighttime-169677/\" target=\"blank_\">Peng Liu</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/nightstreet.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Visualizing the network that connects mainstream and extremist news",
                        "meta_description": "Just stopping by our favorite news site could expose information about us to dozens of different companies, most of which we know nothing about.",
                        "author_id": 1,
                        "created_at": "2017-03-24 16:01:00 -0400",
                        "created_by": 1,
                        "updated_at": "2017-03-24 16:01:00 -0400",
                        "updated_by": 1,
                        "published_at": "2017-03-24 16:01:00 -0400",
                        "published_by": 1,
                        "og_title": "Visualizing the network that connects mainstream and extremist news",
                        "twitter_title": "Visualizing the network that connects mainstream and extremist news",
                        "og_description": "Just stopping by our favorite news site could expose information about us to dozens of different companies, most of which we know nothing about.",
                        "twitter_description": "Just stopping by our favorite news site could expose information about us to dozens of different companies, most of which we know nothing about.",
                        "og_image": "/content/images/nightstreet.jpg",
                        "twitter_image": "/content/images/nightstreet.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Starting off with critical pedagogy",
                        "slug": "starting-off-with-critical-pedagogy",
                        "markdown": "\nA teacher's first encounter with critical pedagogy can be overwhelming. The ideology is radically (pun intended) different from \"traditional\" approaches to education, and it can be easy to lose the trees for the forest, so to speak. Critical pedagogues, myself included, like to write sweeping manifestos. And because the movement is heavily ideological and rooted in justice, it can be easy to think that the only way to become a critical pedagogue is to change *absolutely everything* about the way we relate to our students.\n\nBut a radical (again, pun intended) overhaul of our teaching is rarely practical, and only occasionally desirable. Not only is such an overhaul a lot of work, but changing too many things at once makes it difficult for us to manage the changes well. Of course, we will make mistakes any time we try something new. But more than that, critical pedagogy is a student-centered pedagogy, one that requires us to be attentive and responsive to our students. There is no one-size-fits-all technique, there are no \"best practices\"\u2014in fact, that's the point. Making that kind of transition quickly, especially if we are uncomfortable, risks alientating the students we are seeking to empower.\n\nWorking out a change from \"traditional\" (i.e., industrial) pedagogy to critical pedagogy often benefits from a slow transition, and in some cases is only possible on a small scale\u2014at least until skeptical colleagues (and students) see it in action. And every new batch of students will require some measure of \"retransition\" to allow them to get used to approaches they may not have seen before. So in this post, I offer a few things that we can do on the small scale to introduce critical pedagogy to ourselves, our students, and our colleagues. In some cases, these kinds of things will be a gateway to more radical change in the future. In others, they may be as far as we go in a particular setting. Either way, I hope they are helpful.\n\n**Students should always be making decisions.** This was on a list of tips that my son's U-7 soccer league sent to all the coaches. Drilling technique at practice will win more games in the short-term, but it will harm the kids' ability to play well in the future as the game grows in complexity. The same goes for students at all levels: mindful agency is the goal. Technique will come in time\u2014and there might even be some drilling in the future. But as much as possible, during our class meetings, our students should be exercising their agency in some way. This can be as simple as using clicker questions that challenge all students to work through the logic of a conceptual issue simultaneously, as opposed to one student at a time being asked to recall a specific fact. Or it could involve self-contained problem-based learning tasks. No matter the details, we can make substantial critical progress if, when we plan for class, we choose tasks that put students in a position to make real decisions.\n\n**Don't make students wait in line.** This is another soccer-coaching insight. If most of the kids on the field are waiting behind a line of cones for their chance to play while one or two teammates are active, we're failing the kids. In other words, every kid should have a ball at their feet for most of practice. When there are more than one or two players in the queue at a time, get another line going, and keep them moving. Often our class discussions look like one of these soccer drills: an excellent drill, but it only engages one or two students at a time. The rest are waiting in line, and possibly not even spectating. Critical discussions often happen better in smaller groups than full classes anyway, so why not split the class up, appoint a couple \"cone jockeys\" (what we call parents or older siblings at practice who don't actively coach, but simply keep things moving), and set them all to work.\n\n**Ask questions that don't have answers (yet).** If class activities are directed towards a single, correct answer, our student-centered pedagogy will be disingenuous. Chris Friend calls this [\"lecture in disguise.\"](http://www.hybridpedagogy.com/journal/learning-let-go-listening-students-discussion/) Teleology kills inquiry, and when the answer is known, our pedagogy is content-centered rather than student-centered. Asking questions that don't have answers can help us break our addiction to answers, and can help students do the same.\n\nI should point out, though, that sometimes there are answers that students need to know. But with a little extra thought, we can usually devise a class activity that either uses that information as the springboard for critical inquiry, or that frames pursuit of that answer in a way that has students propose, experiment with, and evaluate methods for finding the answer. For example, students can compare different methods of transcribing music by ear to evaluate which method works better for different music or individuals. Or students could compare \"traditional\" with \"new\" methods for doing arithmetic. In such situations, we shift the focus of the class from the answer that is known to questions about things that are not known or that are particular to the individual. They still learn the \"facts,\" but in the process learn more about how to ask and pursue answers to other questions they encounter in the future.\n\n**Shut up for a while.** Many of us find that in our attempts to get students talking to each other, [we often just get them taking turns talking to us](http://www.williamohara.net/blog/2014/9/18/first-nights-reflection-i). For example, in my current doctoral seminar, we begin class with basic informational issues before moving on to deeper conceptual discussion and questions of application. Because so much of the information is new to the students, I am often the only one who can answer some of the questions. Even after a break, the tone is set for students to ask questions that often ultimately get answered by me. So I decided that for some portion of class, usually when discussing application (where the students are all experts in different areas than I am), I will only communicate to students via Twitter. Verbal discussion is completely controlled by them. Almost immediately after my first attempt with this, Chris Friend published a similar approach in [an article for Hybrid Pedagogy](http://www.hybridpedagogy.com/journal/learning-let-go-listening-students-discussion/), where he listens to students and acts as a scribe for the class in Google Docs. In both cases, the onus is put on students to lead and carry on the discussion, and they are freed from note-taking so they can do just that. I am also freed from discussion-leading duties so that I can focus more on observing students and thinking carefully about their comments and level of (in)activity.\n\nThese are just a few small ideas, but in each case they can be incorporated relatively painlessly. With a little extra thought in planning, and no major curricular overhaul, things like these can open up the world of critical pedagogy in our teaching and in our students' work. These ideas may not represent a major social revolution, but they do offer simple ways to begin to flatten the classroom hierarchy and empower students to exercise their agency and critical thought.\n",
                        "html": "",
                        "image": "/content/images/cones.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Starting off with critical pedagogy",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2014-09-21 21:41:42 +0000",
                        "created_by": 1,
                        "updated_at": "2014-09-21 21:41:42 +0000",
                        "updated_by": 1,
                        "published_at": "2014-09-21 21:41:42 +0000",
                        "published_by": 1,
                        "og_title": "Starting off with critical pedagogy",
                        "twitter_title": "Starting off with critical pedagogy",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/cones.jpg",
                        "twitter_image": "/content/images/cones.jpg"
                    },
                    {
                        "id": 0,
                        "title": "I deleted 40,000 tweets last week. Here's why (and how).",
                        "slug": "i-deleted-tweets",
                        "markdown": "\n<i>This post is Part 2 in my series on digital minimalism. Check out my first post in this series, <a href=\"/2016/12/digital-minimalism-being-deliberate-about-digital-identity/\">Digital minimalism \u2015 being deliberate about digital identity</a>.</i>\n\nI tweet a lot. Since joining six years ago, I've posted approximately 50,000 tweets, retweets, and replies. Some of it is good stuff. I've used Twitter to share my thoughts-in-progress about a variety of things \u2015 teaching, music, coding, data \u2015 as well as to connect with friends, students, and other scholars, particularly those from fields other than my own. I joke that my blog got me my job in Colorado and Twitter got me my job at UMW, but those statements aren't far from the truth. My work as a public scholar online has been instrumental in gaining me a lot of opportunities to work, write, speak, meet collaborators, and even make friends.\n\nBut as I think about curating my public materials, <a href=\"/2016/12/digital-minimalism-being-deliberate-about-digital-identity/\">being deliberate about my digital identity</a>, and keeping myself safe from would-be trolls and harassers, Twitter stands out as a big problem. It's a very useful tool, but it's really easy to get lost in the weeds of old posts, retweets, replies, making it hard to find the good stuff. In fact, most people I know have resigned themselves to the idea that Twitter is ephemeral. The good content is going to quickly flow downstream. While it may be retweeted and brought back into view again, eventually it will go away, and our attention will move on to what's next.\n\nExcept it doesn't go away.\n\nIn 2013, Twitter introduced a major change to its service, allowing users to search for tweets from the entirety of Twitter's history. Those of us using the service at the time had grown accustomed to tweets disappearing from search results after five days, becoming inaccessible to all but the most tenacious of scrollers and swipers. But when Twitter made their entire history searchable, all of those old tweets we thought had dissipated into the ether were now instantly findable.\n\nI'm not particularly worried about overly embarrassing tweets resurfacing. However, I have changed a lot as a person since I started tweeting. As I remarked to a friend while discussing my purging of Twitter content,\n\n> My professional, spiritual, and political identities (and contexts) have all changed a lot since I started tweeting.\n\nThere are a lot of implications in that tweet, but it really boils down to the idea that I'm both the same person and a different person from the Kris who joined Twitter in 2010. If Twitter is truly *social* media, then I want it to be media that helps people get to know me *now*, not me in 2010.\n\nMy use of Twitter has also changed. Some of that stems from my own personal changes, but also from ways in which the Twitter service has changed, and from the ways that others use it. Because of the rise of online abuse and harassment, I post less personal and family information on Twitter than I used to. And partly because of the potential for online abuse, or at least <a href=\"https://en.wikipedia.org/wiki/Parasocial_interaction\">parasocial behavior</a>, and partly because of the change in search capabilities, the kinds of conversations I had openly on Twitter in 2011 or 2012 are often the kinds of conversations that I would have on Slack or in private messages in 2016 and 2017.\n\nUltimately, though, as I think about being deliberate about public digital identity, and as I teach that to my digital studies students, I realize the need to pull some weeds, so to speak. Yes, I want to be more deliberate about what I post, don't post, keep, delete, retweet, etc. But I also want to make it so if someone searched for something from me, they'd be more likely to find my good stuff. I don't want people to get bogged down in old retweets, conversations with friends, and comments about teaching classes I don't teach anymore, or political positions I don't (fully) embrace anymore. And as I increase in visibility and follower count, I don't want to bring potentially unwanted attention to others, particularly former students with whom I conversed back under the old search rules.\n\nSo I decided a couple weeks ago to embark on a big tweet-delete campaign. Specifically, I decided to delete the following:\n\n<ul>\n<li>All retweets of other users' posts (about 17k tweets)</li>\n<li>All replies more than six months old (about 18k tweets)</li>\n<li>Most old Twitter chats (#profchat, #flipclass, etc.)</li>\n<li>Tweets containing old conference hashtags</li>\n<li>Tweets containing old course hashtags</li>\n</ul>\n\nI still have about 10,000 tweets left! And I probably want to go back and cull more. But by deleting these five categories of things, I was able to pull back a lot. And it wasn't that difficult! So if you're thinking about purging some of your old Twitter content, keep reading for a walk-through of how I did it.\n\n<em><strong>Update:</strong> Since writing this post, I have gone and deleted almost all of my tweets. After taking some time off of Twitter completely, I realized that personally, I'm done with Twitter as a service and as a company. However, I also realized, especially at the end of Digital Pedagogy Lab Vancouver, that Twitter is the easiest way to find people I want to connect with in my field(s), and Twitter is what most people (at DPL and the like, anyway) are still using as their primary way of connecting and communicating. So I'm staying on Twitter, posting occasional links to things I write and create (so those who use social media to find those kinds of things can access my work), and connecting with people on the platform (primarily via DM). But I'm not tweeting anymore. And as I connect with people on Twitter, I'm going to bookmark their website/blog, and do my best to build channels of communication and collaboration that extend beyond the bounds of this problematic and proprietary platform \u2015 while also still respecting their preferences, of course. And that's the tough thing to balance. We all have different preferences, and while I can make some decisions for myself, network decisions need to be made by the network.</em>\n\n## Delete some tweets\n\nBecause I'm a nerd, and because I don't want to give full control over my Twitter account to a service I don't know or trust, I wrote code in Python using the Twitter API to do the work. Even if you're not much of a coder, if you have *some* experience writing code, I think you should be able to follow the code and make the tweaks that you need. So let's get started!\n\n<b>The first thing to do is <a href=\"https://support.twitter.com/articles/20170160?lang=en#\" style=\"font-weight: bold;\">download your Twitter archive</a></b> and extract the zip file. This not only gives you a copy of all your old tweets \u2015 for posterity sake, and which you can even post on your own independent domain if you want to! \u2015 but we'll use the archive in the deletion process.\n\nNext, <b>create a new app on <a href=\"https://apps.twitter.com/\">apps.twitter.com</a>.</b> Just click \"Create New App\"; then provide a name for your app (\"tweet deleter\" is fine), description (same), and website (your website, or even the URL for your Twitter account).\n\nOnce you've created your account, you need to get a few **authorization codes** to link your Python script with your Twitter app. Open the \"Keys and Access Tokens\" tab, where you'll find a *Consumer Key (API Key)* and a *Consumer Secret (API Secret)*. You'll also probably need to click on \"Generate My Access Token and Token Secret\" to generate the *Access Token* and *Access Secret*. Keep this tab open in your browser so you can copy these codes into your Python script, and be sure not to share these codes with others, or they'll be able to access your account.\n\nNow it's time to start that Python script. I use Python 3, and to interact simply with the Twitter API, I use the <a href=\"http://tweepy.readthedocs.io/en/v3.5.0/\">Tweepy</a> framework, which you can install via\n\n```pip install tweepy```\n\n(If you're new to Python, pip, and tweepy, check out the <a href=\"https://www.continuum.io/downloads\">Anaconda bundle</a> for scientific computing.)\n\nLet's start scripting! First import ```tweepy``` and ```csv```, and declare those API keys (I left mine out).\n\n~~~python\nimport tweepy\nimport csv\n\nconsumer_key = ''\nconsumer_secret = ''\naccess_key = ''\naccess_secret = ''\n~~~\n\nThen create a couple functions. This function will read in the ```tweets.csv``` file from your Twitter archive without a header row into a list of lists.\n\n~~~python\ndef read_csv(file):\n    \"\"\"\n    reads a CSV file into a list of lists\n    \"\"\"\n    with open(file, encoding = 'utf-8') as csvfile:\n        reader = csv.reader(csvfile, delimiter = ',')\n        rows = []\n        for line in reader:\n            row_data = []\n            for element in line:\n                row_data.append(element)\n            if row_data != []:\n                rows.append(row_data)\n    rows.pop(0)\n    return(rows)\n~~~\n\nAnd this function provides what you need for authenticating with Twitter.\n\n~~~python\ndef oauth_login(consumer_key, consumer_secret):\n    \"\"\"Authenticate with twitter using OAuth\"\"\"\n    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n    auth_url = auth.get_authorization_url()\n    verify_code = raw_input(\"Authenticate at %s and then enter you verification code here > \" % auth_url)\n    auth.get_access_token(verify_code)\n    return tweepy.API(auth)\n~~~\n\nAfter creating these functions, perform the authentication.\n\n~~~python\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_key, access_secret)\napi = tweepy.API(auth)\nprint(\"Authenticated as: %s\" % api.me().screen_name)\n~~~\n\nAnd read in the downloaded Twitter archive.\n\n~~~python\ntweets = read_csv('/path/to/file/tweets.csv')\n~~~\n\nNow it's time to decide which tweets to delete and collect them. I've set my script up so that Python will search the downloaded archive to assemble this list, which is much faster than using the API. It also doesn't have any limits on how many tweets you can query at once. And both of these things mean you can do more double-checking before actually deleting anything!\n\nHere's how to build a list of tweets to delete (called ```tweets_marked```) by searching your archive for a specific hashtag.\n\n~~~python\nhashtag = '#profchat'\ntweets_marked = []\nfor tweet in tweets:\n    if hashtag in tweet[5]:\n        tweets_marked.append(tweet)\n~~~\n\nThis will make a list of all tweets (represented by a list of values) whose ```text``` field in the Twitter archive contain the ```#profchat``` hashtag. It doesn't delete anything just yet, but it does allow you to explore those tweets and make sure you do indeed want to delete them.\n\nTo find out how many tweets were found by that search, use\n\n~~~python\nprint(len(tweets_marked), 'tweets marked for deletion.')\n~~~\n\nTo print all of the text of those tweets and make sure they're the right ones, use\n\n~~~python\nfor tweet in tweets_marked:\n    print(tweet[5])\n~~~\n\nAlternatively, to collect a list of all retweets, use\n\n~~~python\ntweets_marked = []\nfor tweet in tweets:\n    if tweet[5][0:3] == 'RT ':\n        tweets_marked.append(tweet)\n~~~\n\nOr a list of all direct replies:\n\n~~~python\ntweets_marked = []\nfor tweet in tweets:\n    if tweet[5][0] == '@':\n        tweets_marked.append(tweet)\n~~~\n\nOr all direct replies from a specified list of months:\n\n~~~python\ntweets_marked = []\nmonth_list = ['2016-01', '2016-02', '2016-03', '2016-04', '2016-05', '2016-06', '2016-07', '2016-08']\nfor tweet in tweets:\n    if tweet[5][0] == '@':\n        if tweet[3][0:7] in month_list:\n        tweets_marked.append(tweet)\n~~~\n\nCombine these methods as appropriate (search, date ranges, etc.) for what you want to delete. I found it helpful to do one round of deletion at a time \u2015 e.g., #profchat then #flipclass then retweets all as separate processes \u2015 rather than try to assemble a single master list of every tweet to delete. This makes the deletion process into multiple small steps, rather than one arduous one. It also makes it easier to double-check what you're doing piece-by-piece, leading to quite a bit less stress throughout the process.\n\nOnce you've assembled your list of tweets to delete, it's time to delete!\n\n~~~python\n# build list of marked status IDs\nto_delete_ids = []\ndelete_count = 0\nfor tweet in tweets_marked:\n    to_delete_ids.append(tweet[0])\n\n# delete marked tweets by status ID\nfor status_id in to_delete_ids:\n    try:\n        api.destroy_status(status_id)\n        print(status_id, 'deleted!')\n        delete_count += 1\n    except:\n        print(status_id, 'could not be deleted.')\nprint(delete_count, 'tweets deleted.')\n~~~\n\nThis code will send an API call to Twitter for each of the tweets in ```tweets_marked```, instructing Twitter to delete the tweet. **THIS IS PERMANENT, SO BE SURE YOU REALLY WANT TO DO THIS!** It will print the ID number of the tweet and whether or not it was able to delete the tweet. Note that if you have tweets in your archive that come up in multiple searches, they will come up in your archive search each time, but will only actually be deleted the first time. The later searches will get more 'could not be deleted' messages as a result, but that will not cause any other problems.\n\nTwitter's API will not allow this process to go particularly fast if run in a single-threaded environment (as this code uses). I've seen a solution for multi-threading, but it didn't work on my setup (it was written for Python 2). It's not super-slow, but it won't be instantaneous. I didn't time it, but you can definitely delete a few thousand tweets in less than an hour.\n\nThat's it!\n\nLike I said above, I probably want to go back and delete more content, but this process got me through several very large categories of tweets and deleted 80% of my tweets in just a few hours. You could also use it to \"go nuclear\" and delete all your tweets ever. Or all your tweets before/after a certain date and time. You can also use Tweepy to delete favorites, private messages, mass block and unblock, and edit your list of whom you follow (with very sensitively named methods like ```destroy_friendship()```). See the <a href=\"http://tweepy.readthedocs.io/en/v3.5.0/api.html\" target=\"blank_\">Tweepy documentation</a> for a complete list of functions and methods.\n\nI still tweet a lot, and retweet a lot, and converse a lot online, but having a tool like this means that I can regularly go through and purge some of those things that were good at the time, but not worth hanging onto in perpetuity. And it means that searches on my profile are at least a little more likely to return things worth finding.\n\nAs I tell my students, when you write, *deleting words is just as much progress as writing words*. Maybe more. The same goes for what we write on the web. Curation can be hard work, but it means that what remains is more valuable, easier to find, and has room to grow. Time will tell if that's true for Twitter, too.\n\n<i>This post is Part 2 in my series on digital minimalism. Check out my first post in this series, <a href=\"/2016/12/digital-minimalism-being-deliberate-about-digital-identity/\">Digital minimalism \u2015 being deliberate about digital identity</a>.</i>\n\n<i>Some of the code in this post was based on a Gist in GitHub by <a href=\"https://gist.github.com/davej/113241\">Dave Jeffrey</a>. Header image by <a href=\"https://www.flickr.com/photos/mathiasappel/16255827151/\" target=\"blank_\">Mathias Appel</a> (CC0).</i>\n",
                        "html": "",
                        "image": "/content/images/bird.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "I deleted 40,000 tweets last week. Here's why (and how).",
                        "meta_description": "Use Python and the Twitter API do curate your public digital identity.",
                        "author_id": 1,
                        "created_at": "2016-12-30 20:12:00 -0500",
                        "created_by": 1,
                        "updated_at": "2016-12-30 20:12:00 -0500",
                        "updated_by": 1,
                        "published_at": "2016-12-30 20:12:00 -0500",
                        "published_by": 1,
                        "og_title": "I deleted 40,000 tweets last week. Here's why (and how).",
                        "twitter_title": "I deleted 40,000 tweets last week. Here's why (and how).",
                        "og_description": "Use Python and the Twitter API do curate your public digital identity.",
                        "twitter_description": "Use Python and the Twitter API do curate your public digital identity.",
                        "og_image": "/content/images/bird.jpg",
                        "twitter_image": "/content/images/bird.jpg"
                    },
                    {
                        "id": 0,
                        "title": "How Imogen Heap tells a linear story in a cyclical song",
                        "slug": "linear-narrative-cyclical-song",
                        "markdown": "\nA song tells a story, right? But most traditional song forms are cyclical \u2014 some major component of the music and the text repeat multiple times during the song. (Think of the 'chorus' to your favorite song.) How do you tell a story that moves forward in time with urgency, while still using traditional, cyclical song forms? In her song [\"The Moment I Said It\"](https://open.spotify.com/track/6ytW6iyGw0BerTe4vo03ee), Imogen Heap shows that it's not only possible to tell a linear story in a cylical song, but that playing with our expectations in a cyclical form can have a powerful emotional effect.\n\n\"The Moment I Said It\" tells the story of an argument between a couple in love. It's not clear to me whether the narrator's lover is drunk or simply not thinking straight as a result of anger. Whatever the case, the two fight it out, her lover leaves, he crashes the car, and she loses him. It's not explicit in the text whether he dies or simply abandons her, but lines like \"Just put back the car keys or somebody's going to get hurt\" and \"You're scaring me to death\" make it sound like after the \"smash\" she loses him permanently.\n\nThere isn't much in this narrative that sounds like it would fit in a cyclical, repetitive song. However, Imogen Heap uses the cyclical structure of [a typical verse-chorus song](http://openmusictheory.com/popRockForm.html) to build intensity to the climax of the lyric narrative \u2014 \"smash\" \u2014 and to generate an uncertaintly that helps us to feel the narrative more poignantly.\n\n[![](/images/theMomentISaidIt.png)](/images/theMomentISaidIt.png)\n\nThe above image is a formal diagram of the song, for those familiar with the music theory behind song structure. In a nutshell, this song begins with a verse, follows with a chorus, then another verse and chorus, then a bridge (new material that creates contrast with the repeated parts), a final chorus, and then a long, slow \"outro\". As you listen, you can hear the verses begin with the lyrics \"The moment I said it...\" and \"Just put back the car keys...\" The choruses (except the last one) begin with \"It's not even light out...\"\n\nIn order to accomplish the linear narrative in this cyclical form, Imogen Heap breaks with some of the traditional markers of these formal elements (called functions). For example, the title lyrics to a song typically repeat several times and come during the choruses. In this song, the title lyrics only come once at the very beginning. Similarly, choruses tend to have all or most of the lyrics repeat verbatim each time the chorus music appears. In this song, only some of the lyrics are the same in the first two choruses, and the third chorus has entirely new lyrics.\n\nHarmony also tends to give us a clue regarding which formal function is in operation at any given moment: verses tend to start \"on-tonic\" \u2014 that is, on the main, stable chord of the key \u2014 and end \"harmonically open\" \u2014 on some other chord that sounds unstable and in need of some resolution. Choruses, on the other hand, tend to end \"harmonically closed\" \u2014 on the stable tonic chord. This song breaks almost all of those conventions. Both the verses and the choruses begin on-tonic and end harmonically open, and they end on the same chord (flat\u2013VII).\n\nIf lyrics and harmony don't provide the clues we're used to hearing, then how do we know where we are in the form? To an extent, this disorientation is an important part of the narrative. While the narrator sings about \"lead in your eyelids\" and the fact that her \"darling\" is \"not thinking straight\", Heap's music disorients us as we (consciously or unconsciously) try to find our way through the song.\n\nThat said, there are still some clues that help us figure out how the form is put together, and how Imogen Heap uses that structure to communicate her meaning. First, we can hear a clear difference in energy between the three main \"modules\" in the song: the verse has a very low energy level, the chorus starts at a higher state of energy and increases, and the bridge is the emotional climax of the song, with the highest energy level. This energy is projected by things like adding/subtracting instruments, increasing/decreasing volume, speeding up or slowing down the melody and chord progression, etc.\n\nBut that doesn't settle the whole question. The second module (what I call the chorus) builds in energy and only has some lyrics that are invariant between occurrences. Might it be a prechorus? Might the chorus be the climax \u2014 \"Don't...oh, smash...\" \u2014 which is withheld from the first cycle in order to add to its impact when it does occur? This is how I initially heard this song. However, there are a couple contextual clues that led me away from that hearing. The first is the way these modules are arranged throughout the song. Reading \"smash\" as the chorus would mean the song progresses verse\u2013prechorus\u2013verse\u2013prechorus\u2013chorus\u2013prechorus\u2013outro. Verses begin songs, and choruses can function as climaxes, but songs tend not to end with a prechorus followed by an outro. By definition, a prechorus is followed by a chorus. And while the chorus might be withheld early in the song to intensify the anticipation for its arrival later, it's rare to *end* with an unfulfilled prechorus \u2014 I can't think of another example. Second, interpreting the \"smash\" section as a bridge not only gets rid of this prechorus/chorus problem, but it also fits a standard role for bridges: highlight a psychological or narrative change, which leads to a difference in the final chorus, or at least a different meaning for the same words. Interpreting this song as verse\u2013chorus\u2013verse\u2013chorus\u2013bridge\u2013chorus\u2013outro not only fits a more standard pattern, but it gives each module something close to its typical role in a cyclical verse-chorus song. The verse gives critical narrative details, the chorus adds emotional commentary on that narrative, and the bridge carries the pivotal moment in the plot.\n\nThat said, what's important is not so much what we call these things as how they function in the narrative of the song. And the uncertaintly surrounding how each of these musical passages functions in light of patterns we're familiar with is a big part of how she conveys the uncertainty and the urgency of her narrative. This song tells a haunting story, and by playing with our expectations surrounding these common, cyclical patterns, Imogen Heap is able to communicate the emotion of this narrative in ways that would be more difficult if she used a non-cyclical song structure.\n\n\n*This blog post is a demonstration of short-form writing about music for a public audience, which I wrote for my music theory students at CU.*\n",
                        "html": "",
                        "image": "/content/images/imogen.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "How Imogen Heap tells a linear story in a cyclical song",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-10-07 08:32:02 -0600",
                        "created_by": 1,
                        "updated_at": "2015-10-07 08:32:02 -0600",
                        "updated_by": 1,
                        "published_at": "2015-10-07 08:32:02 -0600",
                        "published_by": 1,
                        "og_title": "How Imogen Heap tells a linear story in a cyclical song",
                        "twitter_title": "How Imogen Heap tells a linear story in a cyclical song",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/imogen.jpg",
                        "twitter_image": "/content/images/imogen.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Mining Twitter data with R, TidyText, and TAGS",
                        "slug": "mining-twitter-data-tidy-text-tags",
                        "markdown": "\nOne of the best places to get your feet wet with text mining is Twitter data. Though not as open as it used to be for developers, the Twitter API makes it incredibly easy to download large swaths of text from its public users, accompanied by substantial metadata. A treasure trove for data miners that is relatively easy to parse.\n\nIt's also a great source of data for those studying the distribution of (mis)information via digital media. This is something I've been working on a lot lately, both in independent projects and in preparation for my courses on Digital Storytelling, Digital Studies, and The Internet. It's amazing how much data you can get, and how detailed a picture it can paint about how citizens, voters, and activists find and disseminate information. Most recently, <a href=\"https://twitter.com/funnymonkey\">Bill Fitzgerald</a> and I have begun discussing a project analyzing the distribution of (mis)information in extremist, so-called \"alt-right\" circles on Twitter, and comparing the language and information sources of left- and right-leaning communities on Twiter.\n\nIt turns out this is a really straightforward thing to do, thanks to Martin Hawksey's <a href=\"https://tags.hawksey.info/\">TAGS (Twitter Archiving Google Sheet)</a> and Julia Silge's and David Robinson's <a href=\"http://tidytextmining.com/\">TidyText</a> package for R. In what follows, I'll walk through the process of setting up a TAGS archive, linking it to R, and mining it with TidyText (and other tools from the TidyVerse).\n\n## Setting up a TAGS archive\n\nThere are some excellent tools for interacting with the Twitter API directly, but if what you want is a regularly updating archive of tweets that you can repeatedly mine and analyze, TAGS is definitely the way to go. All you need is a Google account, a Twitter account, and a copy of Martin Hawksey's Google Sheet, and you're good to go. You don't even need your own API key!\n\nTo set it up, <a href=\"https://tags.hawksey.info/get-tags/\">make a copy of Martin's TAGS sheet</a> in your Google account. Then follow the instructions on the setup page to get up and running. After entering your search terms, I recommend setting it up to update every hour *and* making a one-off collection to start (\"Run now!\"). For more information on setting it up, check out Martin's video:\n\n<iframe src=\"https://www.youtube.com/embed/Vm0kjAvH5HM?ecver=2\" width=\"640\" height=\"360\" frameborder=\"0\" style=\"position:absolute;width:100%;height:100%;left:0\" allowfullscreen></iframe>\n\n## Connecting your Google sheet with R\n\nIf you're just planning on doing a one-time analysis of the tweets you archived, you can simply export your Google sheet as a CSV file (specifically, the Archive page), and read it into R with ```read.csv``` or ```read_csv```. However, if you want to keep the archive updating over time and check on it regularly with R (or maybe even build a Shiny App that automatically updates analyses and visualizations for you!), you'll need to publish your sheet to the web. Go to ```File >> Publish to the web...``` and publish the Archive page to the web as a CSV file. Be sure to check the box to automatically republish when changes are made. That way, when your Google sheet downloads new Twitter content each hour, it will also update the public CSV file.\n\n<a href=\"/content/images/tags_publish.png\" target=\"blank_\"><img src=\"/content/images/tags_publish.png\" alt=\"publishing a Google sheet to the web as a CSV file\"/></a>\n\nWhen you click ```Publish```, it will give you a URL. Simply copy that URL and paste it into R:\n\n~~~r\ntweets <- read_csv('https://docs.google.com/spreadsheets/d/your-archive-page-id-here',\n                   col_types = 'ccccccccccccciiccc')\n~~~\n\nThe ```col_types``` will ensure that the long, numeric ID numbers import as characters, rather than convert to (rounded) scientific notation.\n\nNow you have your data, updated every hour, accessible to your R script!\n\n## Mining the tweets with TidyText (and dplyr and tidyr)\n\nOne of my favorite tools for text mining in R is <a href=\"http://tidytextmining.com/\" target=\"blank_\">TidyText</a>. It was developed by a friend from grad school, Julia Silge, in collaboration with her (now) Stack Overflow colleague, David Robinson. It's a great extension to the TidyVerse data wrangling suite. (Also, you should pre-order their new book, *<a href=\"https://www.amazon.com/Text-Mining-R-tidy-approach/dp/1491981652/ref=sr_1_fkmr0_1?ie=UTF8&amp;qid=1487958523&amp;sr=8-1-fkmr0&amp;keywords=text+mining+the+tidy+way\">Text Mining with R: A Tidy Approach</a>*.)\n\nLet's walk through some of the things you can do with your Twitter archive using TidyText (and the TidyVerse in general). As an example, I'll reference my growing collection of tweets with the hashtag ```#americafirst```. (Note that though the tweets are technically public, I want to protect user privacy, so I won't be linking to my Google sheet here or providing anything other than aggregate results. If you want, you can reproduce the study with the code, though it will return a later batch of tweets.)\n\nFirst, load the necessary libraries, import the data from your Google sheet, and append an R-friendly date column.\n\n~~~r\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(httr)\n\ngoogle_sheet_csv <- '' #insert URL of published CSV file from TAGS archive Google sheet\ntweets <- read_csv(google_sheet_csv,\n                   col_types = 'ccccccccccccciiccc') %>%\n  mutate(date = mdy(paste(substring(created_at, 5, 10), substring(created_at, 27 ,30))))\n\nsource_text <- '#americafirst'\nminimum_occurrences <- 5 # minimum number of occurrences to include in output\n~~~\n\nThis will give you a tibble (a tidy data frame) where each row is a tweet, and each column contains (meta)data for that tweet. I've also declared two variables that will help out in the parsing later.\n\nTo find the most frequent words, hashtags, or Twitter handles in the archive, we can pretty much lift the code out of <a href=\"http://tidytextmining.com/twitter.html\" target=\"blank_\">Julia and David's ebook</a>:\n\n~~~r\nreg_words <- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\ntidy_tweets <- tweets %>%\n  filter(!str_detect(text, \"^RT\")) %>%\n  mutate(text = str_replace_all(text, \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https\", \"\")) %>%\n  unnest_tokens(word, text, token = \"regex\", pattern = reg_words) %>%\n  filter(!word %in% stop_words$word,\n         str_detect(word, \"[a-z]\"))\n~~~\n\nThis snippet of code filters out any tweets whose text begins with ```RT``` (retweets; delete or comment out that line to keep retweets in), removes URLs and certain characters that signal something other than natural language text from the tweets, and *tokenizes* the tweets into words. That means splitting the text by spaces, removing punctuation, converting all letters to lower-case, etc., and then applying all of the metadata for the tweet to each individual word. A tweet with 10 words is a single record in the ```tweets``` data frame, but after tokenizing will result in 10 records in the new ```tidy_tweets``` data frame, each with a different word, but identical metadata. After tokenizing, we filter out any records where the word is contained in a list of stop words (a, an, the, of, etc.) or where the \"word\" contains no letters (i.e., raw numbers).\n\nWhat's unique about the way TidyText is tokenizing the Twitter data here is that it uses a regular expression to parse data in a Twitter-specific way. This regular expression includes all alphanumeric characters and the hash ```#``` and at-reply ```@``` symbols. (Julia and David drop those in their ebook analyses, but I want to filter out all Twitter handles for privacy, as well as apply special analysis to hashtags, so I'm leaving them in.) It doesn't always do this. For natural language, you can usually just tokenize by a pre-defined \"word\" concept, or n-gram. But this approach is necessary when parsing text that contains a lot of URLs and special symbols, as tweets do.\n\nTo find the most common words in this tweet corpus, we can use the following code to count occurrences of each unique word, filter out the most common (n > 150, in this case), and order them most to least frequent.\n\n~~~r\ntidy_tweets %>%\n  count(word, sort=TRUE) %>%\n  filter(n > 150) %>%\n  mutate(word = reorder(word, n))\n~~~\n\nThis produces:\n~~~r\n# A tibble: 31 \u00d7 2\n               word     n\n             <fctr> <int>\n1     #americafirst  5593\n2             #maga  2530\n3            #trump   821\n4            @potus   742\n5  @realdonaldtrump   741\n6       #trumptrain   666\n7             trump   503\n8               #2a   457\n9    #draintheswamp   432\n10    #buildthewall   408\n# ... with 21 more rows\n~~~\n\nAll hashtags and Twitter handles! Let's see what's most common if we omit hashtags and handles:\n\n~~~r\ntidy_tweets %>%\n  count(word, sort=TRUE) %>%\n  filter(substr(word, 1, 1) != '#', # omit hashtags\n         substr(word, 1, 1) != '@') %>% # omit Twitter handles\n  mutate(word = reorder(word, n))\n\n# A tibble: 6,533 \u00d7 2\n        word     n\n      <fctr> <int>\n1      trump   503\n2      video   339\n3  president   316\n4    america   197\n5     people   188\n6    illegal   165\n7   american   140\n8       time   121\n9  americans   114\n10   country   109\n# ... with 6,523 more rows\n~~~\n\nTo make this count into a new data frame (to view the whole thing or save to a file), just assign it to a variable.\n\n~~~r\nword_count <- tidy_tweets %>%\n  count(word, sort=TRUE) %>%\n  filter(substr(word, 1, 1) != '#', # omit hashtags\n         substr(word, 1, 1) != '@') %>% # omit Twitter handles\n  mutate(word = reorder(word, n))\n~~~\n\nOr visualize it with ```ggplot2``` (part of the ```TidyVerse``` meta-package).\n\n~~~r\ntidy_tweets %>%\n  count(word, sort=TRUE) %>%\n  filter(substr(word, 1, 1) != '#', # omit hashtags\n         substr(word, 1, 1) != '@', # omit Twitter handles\n         n > 80) %>% # only most common words\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(word, n, fill = word)) +\n  geom_bar(stat = 'identity') +\n  xlab(NULL) +\n  ylab(paste('Word count (since ',\n             min_date,\n             ')', sep = '')) +\n  ggtitle(paste('Most common words in tweets containing', source_text)) +\n  theme(legend.position=\"none\") +\n  coord_flip()\n~~~\n\n<a href=\"/content/images/af_words.png\" target=\"blank_\"><img src=\"/content/images/af_words.png\" alt=\"plot of most common words in #americafirst Twitter corpus\"/></a>\n\nOften times, looking for the most common bigrams (two-word phrases) is more instructive than individual words. Using TidyText to do this with Twitter data is a bit more complicated, as you need to parse by regex, rather than use the built-in n-gram option. Julia and David don't give an example in their ebook, but it's not too hard. We simply tokenize by regex like before, use dplyr's ```lead()``` function to append the following word to each record, and then ```unite()``` the two into a single bigram (assuming they both belong to the same tweet). Here's how to do that, as well as to remove bigrams containing hashtags, Twitter handles, raw numbers, stop words.\n\n~~~r\ntidy_bigrams <- tweets %>%\n  filter(!str_detect(text, \"^RT\")) %>%\n  mutate(text = str_replace_all(text, \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https\", \"\")) %>%\n  unnest_tokens(word, text, token = \"regex\", pattern = reg_words) %>%\n  mutate(next_word = lead(word)) %>%\n  filter(!word %in% stop_words$word, # remove stop words\n         !next_word %in% stop_words$word, # remove stop words\n         substr(word, 1, 1) != '@', # remove user handles to protect privacy\n         substr(next_word, 1, 1) != '@', # remove user handles to protect privacy\n         substr(word, 1, 1) != '#', # remove hashtags\n         substr(next_word, 1, 1) != '#',\n         str_detect(word, \"[a-z]\"), # remove words containing ony numbers or symbols\n         str_detect(next_word, \"[a-z]\")) %>% # remove words containing ony numbers or symbols\n  filter(id_str == lead(id_str)) %>% # needed to ensure bigrams to cross from one tweet into the next\n  unite(bigram, word, next_word, sep = ' ') %>%\n  select(bigram, from_user, date, id_str, user_followers_count, user_friends_count, user_location)\n~~~\n\nNow we can count and sort the bigrams.\n\n~~~r\ntidy_bigrams %>%\n  count(bigram, sort=TRUE) %>%\n  mutate(bigram = reorder(bigram, n))\n~~~\n\nThis gives us:\n\n~~~r\n# A tibble: 4,857 \u00d7 2\n                  bigram     n\n                  <fctr> <int>\n1         illegal aliens    77\n2        calls president    59\n3        president trump    57\n4              fake news    51\n5          maxine waters    38\n6       sanctuary cities    38\n7             anti trump    37\n8        project veritas    37\n9  enforcing immigration    31\n10     homeland security    31\n# ... with 4,847 more rows\n~~~\n\nAnd we can plot it like before.\n\n~~~r\ntidy_bigrams %>%\n  count(bigram, sort=TRUE) %>%\n  filter(n >= 30) %>%\n  mutate(bigram = reorder(bigram, n)) %>%\n  ggplot(aes(bigram, n, fill = bigram)) +\n  geom_bar(stat = 'identity') +\n  xlab(NULL) +\n  ylab(paste('bigram count (since ',\n             min_date,\n             ')', sep = '')) +\n  ggtitle(paste('Most common bigrams in tweets containing', source_text)) +\n  theme(legend.position=\"none\") +\n  coord_flip()\n~~~\n\n<a href=\"/content/images/af_bigrams.png\" target=\"blank_\"><img src=\"/content/images/af_bigrams.png\" alt=\"plot of most common bigrams in #americafirst Twitter corpus\"/></a>\n\nWe can do a lot with this word and bigram data, as Julia and David's ebook demonstrate. We can produce a network analysis of words (essentially a 2D visualization of a Markov model; we could also do this with user data), we can compare word or bigram frequency with another Twitter corpus, and we could search for the most common hashtags and handles in the corpus to find other terms to add to the search that generates the TAGS archive.\n\nOne thing Bill and I want to look at, though, is where some of the people using these hashtags are getting the information that they are sharing. In other words, we want to access *only* the URLs we filtered out at the beginning, and use them to find out which articles and which domains appear the most often. So let's go back to the beginning and do that.\n\nWe don't need to reimport the data, as the ```tweets``` tibble still contains the raw imported tweets with all the metadata. We just have to re-parse the tweets.\n\n~~~r\nreg <- \"([^A-Za-z_\\\\d#@:/']|'(?![A-Za-z_\\\\d#@:/]))\"\nurls_temp <- tweets %>%\n  unnest_tokens(word, text, token = \"regex\", pattern = reg, to_lower = FALSE) %>%\n  mutate(word = str_replace_all(word, \"https|//t|http|&amp;|&lt;|&gt;\", \"\"),\n         word = str_replace_all(word, \"co/\", \"https://t.co/\")) %>%\n  select(word) %>%\n  filter(grepl('https://t.co/', word, fixed = TRUE)) %>%\n  count(word, sort=TRUE) %>%\n  mutate(word = reorder(word, n))\n~~~\n\nNote the differences in this parse code: the regular expression now includes the forward slash. Without it, we'd lose the URL strings. Also note the ```to_lower = FALSE``` option in ```unnest_tokens```. All URLs returned by the Twitter API use the t.co URL shortener, which uses case-sensitive strings for their URLs. Without ```to_lower = FALSE```, TidyText would make everything lower-case, and our URLs would be useless. The first mutate command should look familiar, except that it no longer removes whole URLs that contain ```https://t.co```. Instead, it removes only the ```https```. What we're left with, at this point, is a bunch of \"words\" of the form ```co/juMbl3OfCh@ract3r$```. We want to re-construct these into full URLs, which the next line does for us. Then we remove all metadata, and then all \"words\" that don't contain ```https://t.co/```. This will leave us *only* with t.co shortened URLs, count them, and sort them by frequency, saved in a new data frame ```urls_temp```.\n\nBut a bunch of t.co URLs isn't very helpful. We want the targets they point to! Thankfully, Hadley Wickham's ```httr``` package can do that for us! First, let's limit ourselves to the most common URLs. (Remember that ```minimum_occurrences = 5``` we set at the beginning? That will save us a lot of processing time, while only skipping over URLs that contribute little to the corpus data.)\n\n~~~r\nurls_common <- urls_temp %>%\n  filter(n >= minimum_occurrences) %>%\n  mutate(source_url = as.character(word)) %>%\n  select(source_url, count = n)\n~~~\n\nNow we use ```httr``` to obtain the target URLs for those t.co URLs in the corpus that occur at least five times, and return an HTTP status code (so we can filter out any broken links). This will take some time, especially for a large corpus. If you regularly return to a large, growing corpus, I recommend saving the results to a CSV file, and then importing that CSV file and using ```anti_join()``` so you can limit your GET requests only to URLs you haven't already tracked down.\n\n~~~r\nurl <- t(sapply(urls_common$source_url, GET)) %>%\n  as_tibble() %>%\n  select(url, status_code)\n~~~\n\nThen we can bind those results to the original URLs and their frequency count, and filter out any 404 statuses (page not found), and any URLs that point back to Twitter (like quoted tweets). Of course, you can leave those in if it makes sense for your project.\n\n~~~r\nurl_list <- cbind(urls_common, unnest(url)) %>%\n  as_tibble() %>%\n  select(url, count, status_code) %>%\n  filter(status_code != 404,\n         url != 'https://t.co/',\n         !grepl('https://twitter.com/', url))\n~~~\n\nNow we have a list of URLs, sorted by how frequently they occur in our corpus! A great way to identify the (mis)information sources in an online community.\n\nIf it's not articles but *sites* we're after, we can just parse the URL character string to get to the root domain of each URL and count those instead.\n\n~~~r\n# extract domains from URLs\nextract_domain <- function(url) {\n  return(gsub('www.', '', unlist(strsplit(unlist(strsplit(as.character(url), '//'))[2], '/'))[1]))\n}\n\n# count the frequency of a domain's occurrence in the most frequent URL list\ndomain_list <- url_list %>%\n  mutate(domain = mapply(extract_domain, url)) %>%\n  group_by(domain) %>%\n  summarize(domain_count = sum(count)) %>%\n  arrange(desc(domain_count))\n~~~\n\nThat first function is really messy! It's simply splitting the URL by ```//``` and taking everything to the right of that, then splitting it by ```/``` and taking everything to the left of the first slash, then dropping ```www.```, if present.\n\nThis gives us a very interesting mix of domains found in the #americafirst tweets.\n\n~~~r\n# A tibble: 33 \u00d7 2\n                     domain domain_count\n                      <chr>        <int>\n1             breitbart.com          446\n2            patriotpost.us          139\n3          pamelageller.com          137\n4    NewsUnitedStates.co.vu          105\n5               youtube.com          101\n6          americaspaper.ml           69\n7            USATrump.co.vu           34\n8  overpassesforamerica.com           30\n9               foxnews.com           26\n10           whitehouse.gov           25\n# ... with 23 more rows\n~~~\n\nWhich we can also plot.\n\n~~~r\ndomain_list %>%\n  mutate(domain = reorder(domain, domain_count)) %>%\n  ggplot(aes(domain, domain_count, fill = domain)) +\n  geom_bar(stat = 'identity') +\n  xlab(NULL) +\n  ylab(paste('domain count (since ',\n             min_date,\n             ')', sep = '')) +\n  ggtitle(paste('Most common domains in tweets containing', source_text)) +\n  theme(legend.position=\"none\") +\n  coord_flip()\n~~~\n\n<a href=\"/content/images/af_domains.png\" target=\"blank_\"><img src=\"/content/images/af_domains.png\" alt=\"plot of most common domains linked in #americafirst Twitter corpus\"/></a>\n\nAnd if you follow further redirects for root domains, guess what you find! Those sites supposedly from Mali and Vanuatu? They redirect to adf.ly, whose motto is \"Get paid to share your links on the Internet!\" Add them together, and they're the second-highest frequency of the domains in this corpus. I smell something fishy. But that's for another day...\n\n## On GitHub\n\nI've put all of this code (and a little more) in <a href=\"https://github.com/kshaffer/tweetmineR/\" target=\"blank_\">a GitHub repository</a>, which I will continue to expand as time permits and my projects progress. Feel free to download it, test it out, and even send a pull request if you add some cool functionality to it!\n\nHappy Twitter mining!\n\n<i>Header image by <a href=\"https://www.pexels.com/photo/time-lapse-photography-of-water-splashing-on-brown-rock-during-dytime-33104/\" target=\"blank_\">Pixabay</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/waterfall.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Mining Twitter data with R, TidyText, and TAGS",
                        "meta_description": "Use TAGS archive, Google Drive, and TidyVerse tools in R to easily mine Twitter data.",
                        "author_id": 1,
                        "created_at": "2017-02-24 14:41:00 -0500",
                        "created_by": 1,
                        "updated_at": "2017-02-24 14:41:00 -0500",
                        "updated_by": 1,
                        "published_at": "2017-02-24 14:41:00 -0500",
                        "published_by": 1,
                        "og_title": "Mining Twitter data with R, TidyText, and TAGS",
                        "twitter_title": "Mining Twitter data with R, TidyText, and TAGS",
                        "og_description": "Use TAGS archive, Google Drive, and TidyVerse tools in R to easily mine Twitter data.",
                        "twitter_description": "Use TAGS archive, Google Drive, and TidyVerse tools in R to easily mine Twitter data.",
                        "og_image": "/content/images/waterfall.jpg",
                        "twitter_image": "/content/images/waterfall.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Twitter propaganda during 'Unite the Right'",
                        "slug": "twitter-propaganda-during-unite-the-right",
                        "markdown": "\nAs more and more people take to Twitter to keep up on fast-developing events, it's more important than ever important to understand how their platform facilitates the spread of misinformation, disinformation, and hate, as well as up-to-the-minute accounts of fact. While news and live video content was indeed shared widely and rapidly on Twitter during this weekend's #unitetheright rally, high-volume accounts highjacked the trending hashtags to push a pro-white-nationalist agenda and share disparaging disinformation about antifa, Black Lives Matter, other activists, and the mainstream media, with the intent (and effect) of flooding out the good information with propaganda.\n\n## Background\n\nHigh-volume Twitter accounts \u2015 such as bots \u2015 play <a href=\"https://medium.com/data-for-democracy/spot-a-bot-identifying-automation-and-disinformation-on-social-media-2966ad93a203\" target=\"blank_\">a significant role in the spread of disinformation online</a>. We have seen the same pattern in a number of disinformation campaigns: *catalyst* accounts seed a kernel of disinformation, and a large network of *signal boosters* amplify the signal, in the hopes that it will reach a wide audience that spills over into mainstream consciousness.\n\nThis was certainly the case in the <a href=\"https://medium.com/data-for-democracy/democracy-hacked-a46c04d9e6d1\" target=\"blank_\">lead-up to the French presidential election</a> this spring, and <a href=\"https://medium.com/data-for-democracy\" target=\"blank_\">my Data for Democracy colleagues and I</a> have found Twitter bots, sockpuppets, and other high-volume accounts played a significant role in a number of (mostly far-right) disinformation campaigns in the past couple years.\n\nBut this weekend's #unitetheright protests and terrorist attack presented a new problem. **Misinformation thrives during terror attacks and other breaking stories.** Many witnesses are sharing their perspective from their own up-close, but limited, vantage point; others are trying to spin (and omit) details into a narrative that favors their preconceived ideology; and no one \u2015 not even the most seasoned journalists \u2015 has the time to fact-check every claim at the pace of media consumption.\n\nGiven how easy it is for incorrect and incomplete information to spread, breaking stories like Friday's rally and Saturday's terror attack seem like the ideal environment in which to seed disinformation, misinformation, and heavily biased narratives. But it's not simple to manage such a campaign when the story develops rapidly, and the usual catalysts (and even many of the bot/sockpuppet operators) are *physically present at the event*. While bots and sockpuppets are at work during events like #unitetheright, their role is different than in traditional disinformation campaigns, and it's not always obvious from looking at an account which side they are on.\n\n## The tweets\n\nI collected just under 700,000 tweets from 6:24 UTC (2:24am Charlottesville local time) on Saturday, August 12, to 22:29 UTC (6:29pm Charlottesville local time) on Sunday, August 13. As far as I could tell, this collection included the vast majority of (if not all of) the public tweets from that time period containing the hashtag #unitetheright.\nThe bulk of these tweets were posted in the immediate aftermath of the car crash that killed one civil rights activist and injured at least 19 other individuals.\n\n<i>(For all images, click to view full-size.)</i>\n\n<a href=\"/content/images/frequency_all.png\" target=\"blank_\"><img src=\"/content/images/frequency_all.png\" alt=\"#unitetheright tweets per hour\" /></a>\n\nHowever, as is typically the case in a dataset like this, the majority of the tweets are generated by a much smaller minority of high-volume accounts. The top 10% of accounts by tweet volume account for approximately 50% of the tweets, and the top 5% of accounts generated approximately 37% of the tweets. To figure out how a \"typical\" user tweeted, I divided the corpus in two according to a number of different thresholds and compared. Here are the lowest-volume accounts (10 tweets or less using the #unitetheright hashtag \u2015 the 78% of the users in the dataset who produced roughly 31% of the tweets):\n\n<a href=\"/content/images/frequency_10_tweets_less.png\" target=\"blank_\"><img src=\"/content/images/frequency_10_tweets_less.png\" alt=\"#unitetheright tweets per hour, accounts with 10 or less tweets\" /></a>\n\nThe spike immediately following the car crash and the subsequent drop off in tweet volume are far sharper for these accounts. And as we raise the threshold, the drop off becomes less precipitous. Here are the accounts with 20 #unitetheright tweets or less (92% of users, 54% of tweets):\n\n<a href=\"/content/images/frequency_20_tweets_less.png\" target=\"blank_\"><img src=\"/content/images/frequency_20_tweets_less.png\" alt=\"#unitetheright tweets per hour, accounts with 20 or less tweets\" /></a>\n\nAnd 50 tweets or less (98% of users, 76% of tweets):\n\n<a href=\"/content/images/frequency_50_tweets_less.png\" target=\"blank_\"><img src=\"/content/images/frequency_50_tweets_less.png\" alt=\"#unitetheright tweets per hour, accounts with 50 or less tweets\" /></a>\n\nAnd 100 tweets or less (99% of users, 88% of tweets):\n\n<a href=\"/content/images/frequency_100_tweets_less.png\" target=\"blank_\"><img src=\"/content/images/frequency_100_tweets_less.png\" alt=\"#unitetheright tweets per hour, accounts with 100 or less tweets\" /></a>\n\nCompare these to the top tweeters. Here are the accounts with 100 #unitetheright tweets or more (0.6% of users, 12% of tweets):\n\n<a href=\"/content/images/frequency_100_tweets_more.png\" target=\"blank_\"><img src=\"/content/images/frequency_100_tweets_more.png\" alt=\"#unitetheright tweets per hour, accounts with 100 or more tweets\" /></a>\n\nAnd accounts with 2oo tweets or more (up to 681 tweets for the highest-volume account: 0.1% of users, 5% of tweets):\n\n<a href=\"/content/images/frequency_200_tweets_more.png\" target=\"blank_\"><img src=\"/content/images/frequency_200_tweets_more.png\" alt=\"#unitetheright tweets per hour, accounts with 200 or more tweets\" /></a>\n\nIt's clear from this sequence of plots that the highest-volume accounts tweeted on a very different schedule. Most users dropped off in their tweeting with the #unitetheright hashtag after the car crash (though many of them kept tweeting *about* Charlottesville, but without that hashtag). But most high-volume accounts *kept tweeting with the hashtag for several hours after the crash*. I don't know exactly why, and from spot-checking the tweets, it's hard to find a single narrative emerging. But I have seen situations before where after a hashtag begins to trend, the disinformation bot-nets and sock-nets latch onto it in order to keep it trending long enough for their narrative to spill over into more mainstream channels. It's hard to know for sure, but that seems to be at least part of what is going on here.\n\n## The information landscape\n\nIn an age of information glut, there's no shortage of sources for news and opinion content, and it's not surprising to see that people on the political right and left get much of their information <a href=\"http://pushpullfork.com/2017/03/fake-news-adtech-misinformation/\" target=\"blank_\">from different places</a>. In disinformation campaigns, we've also seen differences in the kinds of information sources being pushed by the high-volume accounts when compared with \"regular\" users, and this has <a href=\"http://pushpullfork.com/2017/05/macronleaks-timeline/\" target=\"blank_\">an impact on the sources being cited in the mainstream</a>. #unitetheright is no exception.\nI extracted all of the web pages linked from #unitetheright tweets and compared high-volume accounts to low-volume accounts. (It's difficult to parse left-leaning and right-leaning accounts without a pre-made list, especially when they are collected because of a single shared hashtag.) To start, here are the websites (domains and subdomains) shared most frequently by #unitetheright tweets overall:\n\n<a href=\"/content/images/domains.png\" target=\"blank_\"><img src=\"/content/images/domains.png\" alt=\"Top domains shared on #unitetheright\" /></a>\n\nThe emphasis on video content is striking, if not surprising. The most tweeted domain is Periscope (livestreamed video), followed by YouTube. @RVAwonk's account of the weekend's events accounted for almost all of the Medium links (third place), followed by a number of other partisan or extremist sites and social media platforms, with a few URL shorteners thrown in. Interestingly, mainstream news media is nowhere to be seen on this list at all. (Huffington Post is the closest to mainstream news on the list.) The lack of expert, edited journalism on this list should be a caution to all of us when we participate in the mass quick-fire retweeting of updates during a breaking event.\nBut not all of these sites were equally distributed among #unitetheright tweeters. Using a simple statistical measure called odds ratio, we can find the *most distinctive* domains shared by high-volume and low-volume accounts. (I used a threshold of 10 or more tweets for high-volume accounts, separating the tweets into the 25% most active accounts and their 72% of the tweets, and the 75% least active accounts and their 28% of the total tweet count.)\n\n<a href=\"/content/images/domains_logodds_with_retweets.png\" target=\"blank_\"><img src=\"/content/images/domains_logodds_with_retweets.png\" alt=\"Most distinctive domains shared on #unitetheright by low- and high-volume accounts\" /></a>\n\nHere we do see a few mainstream news media sites (among the low-volume accounts), but also many more extremist sites among those most characteristic of the high-volume accounts: not only Breitbart and InfoWars, but also 4chan and Order15. So like we've seen in studies of disinformation campaigns, **the highest-volume accounts are disproportionately pushing sites known for spreading disinformation, misinformation, and propaganda.**\nHowever, some of the \"most distinctive\" sites are not particularly widely shared overall. So here is a two-dimensional map of the 50 most shared sites, according to how distinctive they are of high- or low-volume accounts. (Domains to the bottom right are more distinctive of low-volume, \"regular\" users, while domains to the upper left are more distinctive of high-volume accounts. Domains near the dashed line are shared with similar frequency by low- and high-volume accounts.\n\n<a href=\"/content/images/domains_2d_with_retweets.png\" target=\"blank_\"><img src=\"/content/images/domains_2d_with_retweets.png\" alt=\"Most distinctive domains shared on #unitetheright by low- and high-volume accounts\" /></a>\n\nIt's interesting to see here how the typical perception of these sites' politics maps almost perfectly onto the high-/low-volume account distinction. Of the mainstream news sites, the New York Times is most distinctive of low-volume (left-leaning?), Fox News of high-volume (right-leaning?), and Washington Post near the center. Left-leaning Think Progress also sits on the low-volume side, while far-right extremist sites like Order 15, White Genocide Project, and Daily Stormer can be found on the high-volume side. There are anomalies, of course \u2015 never mind the fact that partisans tweet links to their opponents while disparaging or critiquing them. However, in this context, it makes perfect sense that **the high-volume accounts are generally pushing the narrative of the alt-right, white nationalist agenda of the #unitetheright organizers.**\n\n## The content of the tweets\n\nApplying the same analysis to the text of the #unitetheright tweets, we can see the same kinds of patterns emerge. Here are the bigrams (two-word phrases) most distinctive of high- and low-volume accounts, as well as a two-dimensional map of the 50 most common bigrams.\n\n<a href=\"/content/images/bigrams_logodds_10.png\" target=\"blank_\"><img src=\"/content/images/bigrams_logodds_10.png\" alt=\"Most distinctive bigrams shared on #unitetheright by low- and high-volume accounts\" /></a>\n\n<a href=\"/content/images/bigrams_2d_no_rt.png\" target=\"blank_\"><img src=\"/content/images/bigrams_2d_no_rt.png\" alt=\"Most distinctive bigrams shared on #unitetheright by low- and high-volume accounts\" /></a>\n\nThe phrases most distinctive of high-volume accounts are those talking about the identity of the attacker, and those pushing a narrative that sees the media, the antifa (anti-fascist) movement, and Black Lives Matter as the enemies. The phrases appearing in similar proportion across all accounts are those that describe the details of the incident (\"car attack\", \"counter protest\", \"Richard Spencer\", etc.). And the phrases most distinctive of low-volume accounts are those that describe (usually in less than flattering terms) the far-right marchers: \"white supremacist\", \"white nationalist\", \"neo nazis\", and (poking fun at the very non-white-American choice of light source on Friday night) \"tiki torches\".\nIt's heartening to see that the bulk of Twitter users are in opposition to the white nationalist extremists who marched on Saturday, one of whom killed an activist. But you wouldn't know that from looking at the tweets overall. Three-fourths of the *users* are in the low-volume, largely anti-white-nationalist category here, but roughly three-fourths of the *tweets* are in the high-volume, largely pro-white-nationalist category. And while Twitter does make meager attempts to keep botnets from gaming their \"trending\" algorithm, it was impossible to follow the weekend's events without being flooded with pro-white-nationalist content, on both the hashtags and Twitter Moments.\nAs my colleagues and I have written before, these disinformation campaigns are attempts to control the mainstream narrative. In this case, the high-volume, pro-white-nationalist accounts made the movement seem larger and louder than they really are, and that in turn emboldens isolated extremists to speak out and to act out. Further, when the voices of a movement of *violence* is amplified *during and immediately following an act of terror*, that messaging in itself is terrorism. And Twitter's platform makes that terrorism both more possible and more effective than it might otherwise be.\n\n## Takeaways\n\nIt turns out that while things look very different than in the #macronleaks campaign, high-volume accounts played an important part in the spread of (mis/dis)information during and after the #unitetheright rally. The high-volume accounts using the #unitetheright hashtag tended to share far-right propaganda more than the low-volume accounts, and the high-volume accounts maintained peak activity far longer after the terrorist attack than average accounts. In general, these high-volume accounts did not serve as signal boosters for specific catalysts like we've seen in the past. Instead, they capitalized on the media attention brought by the terror attack and used it to push several standard far-right propaganda narratives. While few of these narratives made it into the mainstream news media, they were echoed by Donald Trump's initial response to the events in Charlottesville, and any Twitter user who clicked on the #unitetheright trending moment would have seen a significant number of tweets pushing that narrative mixed in with actual news and eyewitness reporting.\n\nIn her recent book <a href=\"https://www.twitterandteargas.org/\" target=\"blank_\"><i>Twitter and Tear Gas</i></a>, Zeynep Tufekci redefines censorship for the digital age. Censorship is no longer a silencing of a voice by force, but a flooding of the information landscape with so much disinformation that the truth is covered. It seems that the extremists of #unitetheright are not only trying to push a false narrative about the media and identity groups other than young, white, straight men, but they are also trying to flood our news feeds with so much filth that the truth becomes harder to find \u2015 and so stained by the filth around it that we don't believe it when we do find it.\n\nThis isn't just disinformation. It's also censorship. And it's terrorism.\nIt's time <a href=\"http://www.digitalpedagogylab.com/hybridped/truthy-lies-surreal-truths/\" target=\"blank_\">to fight back</a>.\n\n<i>Note: the Twitter scrape and analysis were conducted with an extension of my <a href=\"https://github.com/kshaffer/tweetmineR\" target=\"blank_\">tweetmineR</a> utility for Python and R.</i>\n\n<i>Featured image by <a href=\"https://www.flickr.com/photos/ginz/6816885427/\" target=\"blank_\">Gwendolen Tee</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/sheep.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Twitter propaganda during 'Unite the Right'",
                        "meta_description": "This isn't just disinformation. It's also censorship. And it's terrorism. It's time to fight back.",
                        "author_id": 1,
                        "created_at": "2017-08-15 18:56:55 -0000",
                        "created_by": 1,
                        "updated_at": "2017-08-15 18:56:55 -0000",
                        "updated_by": 1,
                        "published_at": "2017-08-15 18:56:55 -0000",
                        "published_by": 1,
                        "og_title": "Twitter propaganda during 'Unite the Right'",
                        "twitter_title": "Twitter propaganda during 'Unite the Right'",
                        "og_description": "This isn't just disinformation. It's also censorship. And it's terrorism. It's time to fight back.",
                        "twitter_description": "This isn't just disinformation. It's also censorship. And it's terrorism. It's time to fight back.",
                        "og_image": "/content/images/sheep.jpg",
                        "twitter_image": "/content/images/sheep.jpg"
                    },
                    {
                        "id": 0,
                        "title": "A new way to do peer review",
                        "slug": "a-new-way-to-do-peer-review",
                        "markdown": "\nI recently published [two](http://www.hybridpedagogy.com/Journal/files/Open_Source_Scholarship.html) [articles](http://www.hybridpedagogy.com/Journal/files/GitHub_for_Academics.html) with [Hybrid Pedagogy](http://hybridpedagogy.com), and I thoroughly enjoyed the peer-review process. I hope their model will be picked up by other academic journals, because it did exactly what I think peer-reviewed academic publishing should do: it ensured that my ideas were sound before publication, it made my writing better, and it helped my writing reach a larger audience than I could on my own.\n\nThe process began with my submission. Per HP's instructions, I wrote my essay, put it on [Google Drive](http://drive.google.com), and shared it with HP. Immediately, I received an email, stating:\n\n> Thank you for submitting your work to Hybrid Pedagogy for consideration.\n\n> . . .\n\n> Pete Rorabaugh, the managing editor, will notify you within 2-3 business days whether your article has been accepted. If your article is accepted for publication, our goal is to work with you to have your article peer reviewed, revised, and in the publication queue within 14 business days of submission.\n\n> Warm regards,  \n> Robin Wharton, Production Editor\n\n(I know it's not practical for many scholarly situations (longer-form articles, need for highly specialized reviewers, etc.), but I wish every journal responded in this way! More on that later.)\n\nTwo days later, I received an email from one of the editors stating that both of my submissions had been accepted. This email contained projected dates of publication (within two weeks of the submissions), and a provisional schedule for review.\n\nWhat followed over the next few days was some back-and-forth in the comments of the Google documents. As I received comments from the reviewers, I responded and/or edited the articles. This was followed by a Google Hangout (video chat) with the two reviewers. The video chat was the most enjoyable part of the process. It involved resolving some remaining issues in the Google document comments, fine tuning the text, and adding hyperlinks to the articles in order to facilitate deeper exploration of the issues from interested readers. We even tossed around some ideas for future articles (one of which being a potential collaboration between myself and one of my reviewers). The last stage involved a final review from Robin Wharton, the production editor, in the Google documents (no video chat), followed by my final revisions in light of those comments.\n\nFrom submission to publication, the entire process took less than two weeks! It was also entirely transparent\u2014I knew who the reviewers were, they knew me, and we even tweeted publicly about the project during the review process.\n\nI've been involved in the traditional peer-review process only twice before\u2014once as an author, and once as a reviewer. The main differences were time (the traditional process is much slower) and transparency (the traditional review process was either single- or double-blind). I wish that more academic peer review functioned like Hybrid Pedagogy's review process.\n\nAs I mentioned above, it's not always possible (or desirable) for an academic journal to move this fast. Long-form articles on specialized topics require more time\u2014to write, to find qualified reviewers, to review, and to revise. But do academic journals need to focus so nearly exclusively on long-form articles? We are no longer bound to paper, printing presses, shipping costs and schedules, etc. Perhaps more academic journals could focus on shorter-form articles (or a greater diversity of publication types), in order to speed the publication process\u2014with an added side-benefit that we could address timely issues within the peer-review framework. (Some new journals are doing this, of course. The [Journal of Interactive Technology and Pedagogy](http://jitp.commons.gc.cuny.edu) and the various publications using the [PressForward](http://pressforward.org) platform are good examples.)\n\nI also think that we would do well as scholars to have more journals that encouraged less specialized submissions. If an article is worth publishing, it should be able to be read critically by the bulk of the journal's readership. Shouldn't that mean that hyper-specialists are not required to review it? Conversely, if less than a dozen scholars are qualified to critically review an article in detail, what is the audience for such an article? Should it be published at all, regardless of its quality, with so small an audience? There are times when such specialization is appropriate. Likewise, there are specialized articles that are accessible to anyone within a discipline, but which can be reviewed much more easily or quickly by scholar with the same specific specialized focus. However, I think that we in the humanities would do well to make the highly specialized articles more of the exception than the rule.\n\nThe issue of hyper-specialization in research and publishing is a larger matter for another time. However, I would love to see more academic journals focus on shorter-form articles that are capable of review by *any member of their editorial board* at the very least. That would lead to a faster, more pleasurable experience for the authors, and likely a larger audience capable of reading the article critically and taking value from it.\n\nAll in all, this was a great experience. If you have ideas that are fit for Hybrid Pedagogy, I strongly encourage you to write them up and submit them. And if you are in position to influence the publication process of a journal (or to put together a new one), I recommend that you explore Hybrid Pedagogy's review process and consider implementing elements of it in other scholarly journals.\n",
                        "html": "",
                        "image": "",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "A new way to do peer review",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2013-05-29 16:50",
                        "created_by": 1,
                        "updated_at": "2013-05-29 16:50",
                        "updated_by": 1,
                        "published_at": "2013-05-29 16:50",
                        "published_by": 1,
                        "og_title": "A new way to do peer review",
                        "twitter_title": "A new way to do peer review",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "",
                        "twitter_image": ""
                    },
                    {
                        "id": 0,
                        "title": "(Mis)information and the Trump administration",
                        "slug": "misinformation-trump-administration",
                        "markdown": "\nAlmost immediately after the election, people started noticing pages disappearing from federal government websites. Some of this is par for the course in a new administration. Of course the Trump administration cleaned out the Obama-specific pages on <a href=\"https://whitehouse.gov\" target=\"blank_\">whitehouse.gov</a> and started with a clean slate. <a href=\"https://web.archive.org/web/20090122232821/http://www.whitehouse.gov/blog/change_has_come_to_whitehouse-gov/\" target=\"blank_\">So did Obama</a>.\n\nBut in many ways, this seemed to be different. At the same time that whitehouse.gov was changing, Washington was instituting social media gag orders and <a href=\"https://www.theguardian.com/us-news/2017/jan/24/epa-department-agriculture-social-media-gag-order-trump\" target=\"blank_\">forbidding employees from talking with the public or the press</a> about their agency's research. And then people started noticing pages disappearing from websites that don't usually change much when a new administration takes over: <a href=\"https://www.washingtonpost.com/news/animalia/wp/2017/02/03/the-usda-abruptly-removes-animal-welfare-information-from-its-website/?utm_term=.b41edb1c581b\" target=\"blank_\">usda.gov</a>, ed.gov, ... And one of the more conspicuous deletions \u2015 <a href=\"http://thememoryhole2.org/blog/ed-idea\" target=\"blank_\">idea.ed.gov</a>, a site dedicated to helping teachers, students, and parents navigate the Individuals with Disabilities Education Act (<a href=\"http://idea.ed.gov\" target=\"blank_\">now back up</a>) \u2015 just happened to come a day after Betsy DeVos was confirmed as Secretary of Education. The same Betsy DeVos who <a href=\"https://www.washingtonpost.com/news/answer-sheet/wp/2017/01/17/betsy-devos-confused-about-federal-law-protecting-students-with-disabilities/?utm_term=.84b1cd5e1044\" target=\"blank_\">didn't know that IDEA was federal law</a> and would prefer that states and districts exercise their own judgment when dealing with disability-related issues.\n\nAs this news started to break, I was already looking for a new research project. Preferably one that would allow me to use (and hone) my skills as a data scientist in service of the public good. So I decided to start <a href=\"http://pushpullfork.com/2017/02/data-mining-whitehouse-gov/\" target=\"blank_\">scraping, spidering, mining, and analyzing federal government websites</a>, looking for additions, changes, and deletions, and comparing the content of these websites with their Obama-era counterparts. I'm still developing the code for some of that project, and figuring out how I can store and manage all the data that I want to collect. But I've got a good start on whitehouse.gov and ed.gov, with more news on that coming soon.\n\nIn the mean time, I found a really great tool from the Internet Archive's <a href=\"https://archive.org/web/web.php\" target=\"blank_\">Wayback Machine</a>. The Wayback Machine allows anyone to view old versions of websites. They crawl and scrape large swaths of the web on a regular basis, and as long as their scraper collected the data, anyone can view it. While for sites like my own blog, a lot of changes go unnoticed by the Wayback Machine, federal government websites have been scraped regularly from the very beginning, often crawled multiple times per day looking for updates, so it's a fairly reliable source, and definitely the best we have.\n\nThe ability to go to <a href=\"https://archive.org\" target=\"blank_\">archive.org</a> and search millions of websites from the past 20 years is amazing. But for big-data research, there's an even better tool: <a href=\"https://archive.org/help/wayback_api.php\" target=\"blank_\">their API</a>. In particular, I've found their <a href=\"https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server\" target=\"blank_\">CDX Server API</a> to be an amazing tool when studying changes in a website over time. You feed it a query (just a simple URL string, no authentication needed), and it returns the *entire change history* for all of the snapshots in the Wayback Machine archive for a domain. Want to download a single text file containing all of the additions and changes on ed.gov discovered by the Wayback Machine in the past 20 years? No problem. Just open a browser, type in\n\n    http://web.archive.org/cdx/search/cdx?url=ed.gov&amp;matchType=domain&amp;output=json&amp;collapse=digest\n\nand wait for a minute. When the page finishes loading, click save. Now you've got it! The entire history of page additions and changes for the domain whitehouse.gov and all its subdomains found by the Wayback Machine, with any duplicate pages found in successive snapshots collapsed into single entries.\n\nThere's one catch: the Wayback Machine is focused on finding old versions of pages that have since been changed or deleted. It does *not* preserve an easy-to-query history of page *deletions*. And while it's possible to ascertain that information from a spider crawl of the entire archive, their server (purposefully?) makes that a slow process. Given the size of these archives, it would take days. So an analysis of page deletions will have to happen another way.\n\nWith that caveat in mind, I wrote <a href=\"https://github.com/kshaffer/websitewatcher/blob/master/cdx_downloader.R\" target=\"blank_\">an R script</a> that will query the change/addition history for six federal agency websites (whitehouse.gov, ed.gov, fcc.gov, epa.gov, usda.gov, and nps.gov) as well as several retail, journalistic, and non-profit organization websites to serve as a baseline comparison. What I found was mind-blowing. In fact, I'm still not sure I believe it. But here goes...\n\n## The 20-year history\n\nThe first thing I did with this data was plot out the changes over time. The following image shows the number of page changes/additions per month for each of the six federal websites I analyzed. Notice anything?\n\n<a href=\"/content/images/gov_by_month.png\" target=\"blank_\"><img src=\"/content/images/gov_by_month.png\" alt=\"changes and additions to .gov websites over time, by month\" /></a>\n\nThat spike at the end? That's January 2017. Wow.\n\nNow, a few things could be happening here. It could be indicative of the changes that usually accompany a new administration. But if we look to January 2001 and 2009, there is no comparable spike. There are local peaks at the beginning of Bush's and Obama's administrations, but nothing like Trump's.\n\nSo I thought there might be a change in how the Wayback Machine takes its snapshots. Maybe it's just taking that many more snapshots now, so it catches all the little changes in 2017 that would have been combined into fewer, but more substantive, changes found by the Wayback Machine in 2001 and 2009. When you visit <a href=\"https://web.archive.org/web/20170222055950/https://www.whitehouse.gov/\" target=\"blank_\">a page on the Wayback Machine</a>, there is a discernible increase in snapshots over time. But is it this big and this precipitous?\n\nThe simplest way to test that was to examine change histories for non-government websites. So I queried the change history of seven retail, non-profit, and journalistic websites that update at a variety of frequencies \u2015 <a href=\"https://washingtonpost.com\" target=\"blank_\">washingtonpost.com</a>, <a href=\"https://theguardian.com\" target=\"blank_\">theguardian.com</a>, <a href=\"https://npr.org\" target=\"blank_\">npr.org</a>, <a href=\"https://www.aspca.org/\" target=\"blank_\">aspca.org</a>, <a href=\"https://societymusictheory.org/\" target=\"blank_\">societymusictheory.org</a>, <a href=\"https://www.psychologytoday.com/\" target=\"blank_\">psychologytoday.com</a>, and <a href=\"https://target.com\" target=\"blank_\">target.com</a>. Here are their change histories:\n\n<a href=\"/content/images/non_gov_by_month.png\" target=\"blank_\"><img src=\"/content/images/non_gov_by_month.png\" alt=\"changes and additions to commercial and non-profit websites over time, by month\" /></a>\n\nThese sites actually reflect something more like the general trend of increasing snapshots over time than the .gov websites do (as well as an apparently major site overhaul on Target.com last summer!). But with the exception of *Washington Post* and *The Guardian* (which spend much of their time covering government activity), there is no major spike in early 2017.\n\nSo as far as I can tell, the increase in .gov site changes in early 2017 is not an artifact of changes in the way the Wayback Machine crawls the web. And it's huge. But maybe it's not just Trump? Maybe Obama's agencies were making a lot of changes at the end of the administration, too?\n\nHere's a day-by-day breakdown of changes from January 1 to February 15, 2017. It's pretty clear that the bulk of the changes come after the inauguration.\n\n<a href=\"/content/images/gov_by_day.png\" target=\"blank_\"><img src=\"/content/images/gov_by_day.png\" alt=\"changes and additions to .gov websites Jan 1-Feb 15, by day\" /></a>\n\nThe non-government websites do not show such a change, even the journalistic ones.\n\n<a href=\"/content/images/non_gov_by_day.png\" target=\"blank_\"><img src=\"/content/images/non_gov_by_day.png\" alt=\"changes and additions to commercial and non-profit websites Jan 1-Feb 15, by day\" /></a>\n\nSo the best explanation I can come up with so far is that the Trump administration is making major additions and changes to federal agency websites. There's still the possibility that these are many *small* changes, rather than large changes. To explore that requires a more detailed study of larger downloads of data from those websites. (And <a href=\"/2017/02/data-mining-whitehouse-gov/\" target=\"blank_\">I'm working on that</a>, too!) But **something has definitely changed about how the federal government communicates information to the public digitally.** And taken together with the gag orders, <a href=\"https://www.nytimes.com/2017/02/07/us/politics/the-white-house-list-of-terror-attacks-underreported-by-media.html\" target=\"blank_\">the misdirection from the press secretary</a>, and the page deletions already documented, this is major. Our federal government is changing the way it communicates with us, it is not being transparent about those changes, and it becomes more likely with each passing day that important information is being withheld.\n\nThis is a problem.\n\n## Content types\n\nIn addition to adding, deleting, and changing content, the Trump administration is making major changes to the types of content on .gov sites. The most common file types in the .gov dataset overtime are HTML (regular web pages), PDF documents, plain text, JPEG images, and Microsoft Word documents, with HTML documents comprising 60% of the pages in the .gov dataset. When it comes to engaging information on the web, HTML and plain text are the most accessible and the most versatile, with Word documents and especially PDFs being more difficult to work with. (You may think everyone has MS Word and a PDF reader, but the added layers of browser plugins, downloads, and the tax-form-like formatting of many of these PDFs make them significantly less user-friendly than a well designed web page. Think <a href=\"https://irs.gov\" target=\"blank_\">IRS.gov</a>.)\n\nNot surprisingly, the past few weeks have seen a significant change in file type. Here is a change history of these six .gov websites, separated by file type instead of by site.\n\n<a href=\"/content/images/gov_file_types.png\" target=\"blank_\"><img src=\"/content/images/gov_file_types.png\" alt=\"changes and additions to .gov websites over time, by file type\" /></a>\n\nWhile HTML files go up in January 2017, they are far outstripped by PDFs, and their rate of increase is outstripped by Word docs. In fact, January 2017 brought 134,733 of the 185,468 PDFs in the history of these websites (73%!) and 17,149 of the 26,772 Word docs in the sites' history (64%). On the other hand, January 2017 saw 38,881 new or changed HTML pages, just 5% of the 725,119 in the whole dataset.\n\nThis is further evidence that the Trump administration is obfuscating information. Delete important content, make drastic changes to existing content, replace user-friendly web sites with PDFs and Word documents, pull back on social media usage, limit federal employee access to the press, and leave the press's inquiries unanswered by the press secretary.\n\n## A harrowing picture\n\nIn the weeks to come, I'll be refining my software and statistical models to look in more depth at specific content changes on some of these sites. But even this \"distant read\" paints a harrowing picture of our current government's approach to information. It's not surprising given the media history of Trump and several of his top advisors. But it's incredibly dangerous.\n\nWhat can we do? To start, we can support the efforts of the Internet Archive, especially as they pursue <a href=\"http://blog.archive.org/2016/11/29/help-us-keep-the-archive-free-accessible-and-private/\" target=\"blank_\">plans for a mirror outside the United States</a>. We can keep a close eye on these government sites, looking at which domains, subdomains, and pages are changing the most frequently. We can draw attention to particularly problematic deletions, additions, and changes. We can mirror and share data elsewhere on the web. And we can demand transparency from others in government who are in position to counter this trend from the Executive Branch.\n\nI've decided to spend time and energy setting up tools to accomplish some of these things, particularly the watching and the analyzing. It's something I can do to help, and hopefully it will prove valuable. I'll keep posting regular updates, and please check out my <a href=\"https://github.com/kshaffer/whitehouse\" target=\"blank_\">whitehouse</a> and <a href=\"https://github.com/kshaffer/websitewatcher\" target=\"blank_\">websitewatcher</a> repositories on GitHub if you want to run the code for yourself or contribute to the project.\n\n<i>Header image by <a href=\"https://www.pexels.com/photo/man-reading-newspaper-6053/\" target=\"blank_\">Kaboompics</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/newspaper.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "(Mis)information and the Trump administration",
                        "meta_description": "The Trump administration has made major changes to federal agency websites. Just how major?",
                        "author_id": 1,
                        "created_at": "2017-02-22 11:26:00 -0500",
                        "created_by": 1,
                        "updated_at": "2017-02-22 11:26:00 -0500",
                        "updated_by": 1,
                        "published_at": "2017-02-22 11:26:00 -0500",
                        "published_by": 1,
                        "og_title": "(Mis)information and the Trump administration",
                        "twitter_title": "(Mis)information and the Trump administration",
                        "og_description": "The Trump administration has made major changes to federal agency websites. Just how major?",
                        "twitter_description": "The Trump administration has made major changes to federal agency websites. Just how major?",
                        "og_image": "/content/images/newspaper.jpg",
                        "twitter_image": "/content/images/newspaper.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Publishing with GitHub pages",
                        "slug": "publishing-with-github-pages",
                        "markdown": "\nThis week, I helped the folks at [Hybrid Pedagogy](http://www.hybridpedagogy.com) publish [an ebook on open online learning](http://learnerexperiences.hybridpedagogy.com). Like [*Engaging Students: Essays in Music Pedagogy*](http://flipcamp.org/engagingstudents), and all of my course websites, we published it using GitHub's free web hosting option. I've written about [why one might want to use GitHub](http://hybridpedagogy.com/Journal/files/GitHub_for_Academics.html) for a project like this before, so this post will not explain why. Instead, I'd like to explain *how* for all those interested in this process, particularly those who want to pursue open-access or open-source academic publishing.\n\n## Setting up a blank ebook ##\n\nSince the ebooks I've built have essentially been websites, any host, platform, or design is fine. However, GitHub allows any public project\u2014including websites\u2014to be \"forked,\" or copied for remixing and republishing. The easiest way to publish an ebook, then, is to fork an existing ebook and replace the content. I have a template anyone can copy ([source](http://github.com/kshaffer/GHbook) \\| [site](http://kris.shaffermusic.com/GHbook)), or you can fork *Engaging Students* ([source](http://github.com/flipcamp/engagingstudents) \\| [site](http://flipcamp.org/engagingstudents)) or the HP project, *Learner Experiences* ([source](http://github.com/hybrid-pedagogy/LearnerExperiencesInMOOCs) \\| [site](http://learnerexperiences.hybridpedagogy.com/)).\n\n## Google Drive ##\n\nThe content for both the FlipCamp and HP projects began with Google Docs. These are easy to produce, and they involve as much new technology as I am comfortable asking most people to wrestle with when submitting something to a publishing project. HP asks authors to compose articles in Google Docs and submit by sharing them with the editors; we used the same process for *Engaging Students*. *Learner Experiences* involved student essays which were uploaded to GDocs for the editing and publishing process.\n\n## Exporting Google Docs for GitHub ##\n\nOf course, GitHub will not simply publish Google Docs. GitHub can publish HTML pages, or it can use the open-source application Jekyll to convert any MarkDown or Textile files to HTML for web publication. And if you want to publish the ebook in ePub, Kindle, or PDF format, MarkDown is helpful there as well. So we want each article in a MarkDown file.\n\nGoogle Docs does not support MarkDown. However, Google Docs can be exported in a number of file formats, which can in turn be converted to something GitHub can deal with. Each brings its own problematic aspect to that process, but I've found one almost ideal solution.\n\nFirst, we exported all the Google Docs as HTML files.\n\nNext, we converted those files to MarkDown, using the following code for each file (you may need to download and install [Pandoc](http://johnmacfarlane.net/pandoc/)):\n\n    pandoc -f html -t markdown --no-wrap -o FILENAME.md FILENAME.html\n\nBecause of Google's weird stylesheet, some formatting will be lost and will have to be re-added manually: bold, italics, headings, etc. Likewise, if you plan on using LaTeX to make a PDF, you will have to replace some Unicode characters (smart quotes, em dashes, etc.) with regular characters. However, this is a pretty quick process, especially compared to what you have to reconstruct if you export the Google Docs as MS Word files.\n\nThe only thing these documents are Missing are the headers that GitHub is expecting in order to build the website. Simply use the headers in the markdown files for the ebook you forked, replacing author and title information. Watch out for colons, though! These headers (called YAML headers) don't play nice with colons in titles. Use HTML code instead. (See [example](https://raw.github.com/flipcamp/engagingstudents/gh-pages/shafferintro.md).)\n\n## Building the site ##\n\nOnce you have forked an existing ebook and have each article/chapter in MarkDown format with an appropriate header, building the site is a piece of cake. It requires a bit of technical knowledge (or learning), but if you're editing and publishing an ebook, it's probably worth it. (Again, I've never had submitters dig into this, only editors.)\n\nFirst, if you haven't already, install [git](http://git-scm.com). You may also want to install [Jekyll](http://jekyllrb.com) to preview the site you create locally as you're working on it, but you don't have to. GitHub has Jekyll running, and it will automatically generate your site when you upload source files.\n\nOn the GitHub website, open the page for the project source\u2014this is the github.com/user/project site, not the user.github.io/project (or fancy, paid URL) for the actual site. For example, [github.com/kshaffer/GHbook](http://github.com/kshaffer/GHbook). You'll see something (probably off to the right) like this:\n\n![]({{ root_url }}/media/gitClone.png)\n\nClick on the clipboard icon to copy the clone URL to your clipboard.\n\nOpen a terminal and navigate to a place where you would like to download the ebook locally. Then type\n\n    git clone whateverYouJustCopiedToYourClipboard\n\nFor example:\n\n    git clone git@github.com:kshaffer/GHbook.git\n\nYou now have the entire ebook website code on your computer. If you cloned my ebook you're good. If you cloned something else, make sure you're on the right \"branch\" before doing anything else. Use the \"cd\" command to move into the downloaded project folder, for example:\n\n    cd GHbook\n\nThen check to see if you are on the *gh-pages* branch of the project (the one that publishes the website):\n\n    git branch\n\nIf it says gh-pages anywhere with an asterisk, you're good. If it says gh-pages somewhere, but the asterisk is by something else (like *master*), type the following:\n\n    git checkout gh-pages\n\nIf gh-pages is not listed, create it by typing:\n\n    git branch gh-pages\n\tgit checkout gh-pages\n\nOnce you're into the gh-pages branch, open it in your file manager (Finder, Explorer, whatever). Now copy all of your MarkDown files (with proper headers) into this folder, and delete the existing markdown files, with the exeption of index.md. If you have supporting images from the Google Docs, copy them in as well. Your content is now part of the book!\n\nNow open the index.md file and edit the front page. For example, if you want to include a table of contents right on the landing page, enter it here. Each link should be proper MarkDown:\n\n    [text people see](filename.html)\n\nFor example:\n\n    [Chapter 1: Introduction to GitHub Pages.](chapter1.html)\n\nKeep in mind that though all the source files are markdown files (\\*.md), Jekyll will produce a new batch of HTML files, so the link should point to an HTML file (\\*.html) with the same root name.\n\nOnce your front page is set and your chapters imported, you just need to make a couple changes to the header and footer that appear on each page. These are found in the default.html file in the \\_layouts folder of my template. In that file, you'll want to change the title of the book (it appears twice), links to github repositories, licensing information, and Google Analytics code.\n\nThat's it!\n\nOf course, you could do more sophisticated editing, like overhauling the CSS files, incorporating Twitter Bootstrap, inserting Dicqus comments, etc. You're on your own for that! If you want a basic, reader-centric design and an easy route to publishing, this is all you need to do.\n\n## Editing and publishing ##\n\nTo see how things look, you can either run Jekyll locally (if installed), or push the content to GitHub and see how it looks. To run jekyll locally, simply navigate to some folder inside this project at the terminal and type\n\n    jekyll\n\nThen your site will be built in the \\_site folder. (Otherwise, simply ignore that folder.)\n\nIf you are satisfied with the way it looks (or want to push it to GitHub to find out how it looks), run the following at the terminal (inside the project directory):\n\n    git add .\n\tgit commit -am 'some message here so you can keep track of changes'\n\tgit push origin gh-pages\n\nThis pushes all your changes to GitHub. They should show up in your project source directory. Now you can check to see if the project built the website successfully. Go to the project source page and click on \"settings\" to find the URL. Go to that URL to see if your ebook successfully built. If not, wait 5\u201310 minutes and try again; sometimes the first build is slow. If it still doesn't load, make sure you have validated your email address with your GitHub account. If that doesn't work either, you have some sleuthing to do. However, if you have a relatively straightforward project and have followed the instructions above, there should at least be *something* that is just in need of some tweaking.\n\nAssuming there are some changes needed, simply make them on your computer, and then repeat the above procedure to push the changes to GitHub.\n\n    git add .\n    git commit -am 'some message here so you can keep track of changes'\n    git push origin gh-pages\n\nI've found that the changes are published almost immediately, but I have to clear my browser cache to see them if I've been to the site recently enough.\n\n## Editing as a team ##\n\nEvery GitHub project includes an \"issues\" page: github.com/user/project/issues. Here, a team of collaborators can create a to-do list of editing tasks, assign them to specific editors, and even collect them together into \"milestones\" with deadlines. For *Learner Experiences*, we used milestones like *all content initially imported*, *ready for final review*, and *ready for publication*. This is a great tool for team-based editing and site-building, but not so much for communicating specific edits to authors. (That's much better in GDocs.)\n\n## Other formats ##\n\nMarkDown is a great source format, because it is so flexible. In addition to GitHub web pages, it can be the source for a PDF book created with LaTeX, an ePub file for importing onto non-Kindle eReaders, or a mobi file for importing onto Kindle eReaders.\n\nTo create an ePub, create a new folder for your ePub files and follow the instructions [here](http://johnmacfarlane.net/pandoc/epub.html) to create metadata and title-page files (very simple). Then copy all of your content MarkDown files into that folder. In each file, remove the YAML header and reaplce with a title heading and an author byline. For example, replace\n\n    ",
                        "html": "",
                        "image": "",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Publishing with GitHub pages",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2013-09-25 14:56",
                        "created_by": 1,
                        "updated_at": "2013-09-25 14:56",
                        "updated_by": 1,
                        "published_at": "2013-09-25 14:56",
                        "published_by": 1,
                        "og_title": "Publishing with GitHub pages",
                        "twitter_title": "Publishing with GitHub pages",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "",
                        "twitter_image": ""
                    },
                    {
                        "id": 0,
                        "title": "Coding for Teachers \u2015 A DPL Workshop",
                        "slug": "coding-for-teachers-dpl-workshop",
                        "markdown": "\nFollowing is the outline and resources for my workshop \"Coding for Teachers\" at Digital Pedagogy Lab's 2016 Institute. If you've been thinking about possibly learning to code, but don't know if it's worth it, or don't know where to start, take a stab at these activities. While they're designed with an in-person group in mind, I think they'll work for (motivated) individuals as well. :)\n\n## Part 1: Playing with Python and Pypothesis\n\nThe following is a module created with [Trinket](https://trinket.io). It contains the code to my [Pypothesis](https://github.com/kshaffer/pypothesis) module for the [Python 3](https://www.python.org/) programming language. The module allows users to write simple queries that collect and display information from public annotations on [hypothes.is](https://hypothes.is). We'll use it to learn a little about Python, API calls, and using existing code to do relatively powerful things with just a little code (and coding knowledge).\n\n### Activity\n\nIgnore the first 100 lines of code for now. (If you installed this module on your computer, you wouldn't have to look at it. It would be embedded in the system.) There are two blocks below the line that says *\\# test*:\n\n~~~python\n# search for all annotations with the tag IndieEdTech and return them in json format.\ns = searchurl(tag = 'IndieEdTech')\nl = retrievelist(s)\n# print the title of each article annotated.\nfor entry in l:\n    e = Annotation(entry)\n    print(e.title)\n    print(e.user)\n    print('\\n')\n~~~\n\nand\n\n~~~python\n# Using the hyothes.is annotation share URL, retrieve and parse the JSON data for that annotation, then print it.\nt = Annotation(retrieve(apiurl('https://hyp.is/s43Svk2xEeaKmptcVb4Svg/kris.shaffermusic.com/2015/03/sustainable-pedagogy/')))\nprint(t.title)\nprint(t.uri)\nprint(t.highlight)\nprint(t.comment)\nprint(t.user)\nprint(t.created)\nprint(t.updated)\nprint(t.id)\nprint(t.hypothesisurl)\n~~~\n\nChoose one of these blocks, and delete the three quotation marks at the beginning and end of the block. (These quotation marks tell Python to leave the code in place, but not to run it. Removing it runs that code, after loading the module in the first 100 lines.) Then click \"Run\" (it may just look like a play button) at the top of the Trinket window.\n\nNow put the quotation marks back. Then try the same thing with the other block.\n\nWhat do these blocks of code do? What alterations can you make? What else would you search for?\n\n<iframe src=\"https://trinket.io/embed/python3/ac6183e555\" style=\"indent: -200px\" width=\"1000\" height=\"800\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" allowfullscreen></iframe>\n\n### Reflection\n\nAfter you've played around a bit, take a peek at the code in lines 1-100. Can you figure out what the difference is between the search code in the two blocks? Why does one require a URL? Why does the *searchURL()* function not contain a URL?\n\n## Part 2: Playing with text in public using JavaScript\n\nVisit [Monkeys Writing Shakespeare (or Austen...)](http://kris.shaffermusic.com/monkeyswritingshakespeare). This site takes the opening of Jane Austen's *Pride and Prejudice* and randomly replaces words from the text with other words with the same part of speech. We'll use it (and hack it) to learn a little bit about the [JavaScript](https://developer.mozilla.org/en-US/docs/Web/JavaScript) programming language, asynchronous programming, and developing playful web apps.\n\n### Activity\n\nVisit the web app's [project page on GitHub](https://github.com/kshaffer/monkeyswritingshakespeare). Click on the green button to download a zip file containing the code for this web app. Extract that zip file somewhere on your computer. Open the index.html file in a web browser (double-clicking on it should do), and then open both the app.js and index.html files in a plain-text editor (something like Notepad or TextEdit, *not* a word processor like MS Word or Pages \u2015 if you're in the market for a more powerful text editor, I highly recommend [TextMate](https://macromates.com/) (Mac only) or [Atom](https://atom.io) (cross-platform)). It will also be helpful to open up your browser's *developer tools* (I'll walk you through this if you can't find it).\n\nOnce loaded up, take a look at the code and the web page and see if you can get your bearings. We'll take a couple minutes to get oriented to the code's organization together.\n\nOnce we have our bearings, there are a few activities to try. Feel free to tackle any or all of these, on your own or with a partner, depending on how comfortable you feel with each of them.\n\n- Add words to one or more of the *arrays* at the top of the app.js file. The more words from the source text can be found here, the more active the script is. What happens when you add words not in the source text? when you mix up parts of speech? when you insert non-English or non-sensical words?  \n- Switch out the source text with something else (a Shakespearean sonnet, a political stump speech, your (least) favorite song's lyrics...). Did it work? Look at the way the Austen source text is entered. What special additions/changes do you need to make to get it to work?  \n- Find the timer. What does the number mean? Change the timing and see what happens.  \n- Why does the app.js code include \"document.getElementById('shakespeare')\" when the text is by Austen? What happens if you change *shakespeare* to *austen*?\n- More advanced: Create a new array of words (such as places or conjunctions) at the top of the app.js file. Then find the comment 'WORD REPLACEMENT LOOP' in the code. Add the code required to use your new array.\n\n### Reflection\n\nWhat ideas does this app give you for your own projects?\n\nWhat differences do you note between Python and JavaScript? If you were to pick one of the two languages to learn first, which would you pick? Why?\n\n## Follow-up resources\n\n- [Eloquent JavaScript](http://eloquentjavascript.net/index.html) - a free, interactive book aimed at teaching JavaScript.  \n- [JavaScript: The Definitive Guide](http://shop.oreilly.com/product/9780596101992.do) - a print book aimed at being a (relatively) exhaustive resource on JavaScript programming.  \n- [Learning Python](http://shop.oreilly.com/product/0636920028154.do) - a great book for getting started with Python. (Most academic libraries have the O'Reilly books as electronic resources, which are really easy to check out and use. But be sure you have the most recent edition, *especially* for web programming!)  \n- [Codecademy](https://codecademy.com/) - not the best pedagogy, especially if you try to use it exclusively, but a good resource for practicing the basics you learn about in the books above.  \n- [Stack Overflow](https://stackoverflow.com/) - *the* place to find answers to questions about coding online. I have yet to come up with a question that hasn't already been asked, and usually there is a good answer here. If you Google your question, most of the good answers high in the search results will be from here.  \n- [W3Schools](http://www.w3schools.com/) - more interactive tutorials (like Codecademy), but focused on web-based programming (HTML, CSS, JavaScript, PHP, etc.).\n",
                        "html": "",
                        "image": "/content/images/pw2.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Coding for Teachers \u2015 A DPL Workshop",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-08-08 20:57:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-08-08 20:57:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-08-08 20:57:00 -0400",
                        "published_by": 1,
                        "og_title": "Coding for Teachers \u2015 A DPL Workshop",
                        "twitter_title": "Coding for Teachers \u2015 A DPL Workshop",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/pw2.jpg",
                        "twitter_image": "/content/images/pw2.jpg"
                    },
                    {
                        "id": 0,
                        "title": "A journey through API programming \u2015 Part 4: Posting to Medium",
                        "slug": "journey-through-api-programming-4",
                        "markdown": "\n<p><i>This is part of a series of posts in which I blog through my process of learning API programming in general and <a href=\"https://medium.com/blog/the-medium-api-is-now-open-to-everyone-3f4642e5c850#.8ehvndx6y\">the Medium API</a> in particular. For the beginning of this series, see <a href=\"http://kris.shaffermusic.com/2016/08/journey-through-api-programming-1/\">Part 1</a>, <a href=\"http://kris.shaffermusic.com/2016/08/journey-through-api-programming-2/\">Part 2</a>, and <a href=\"http://kris.shaffermusic.com/2016/08/journey-through-api-programming-3/\">Part 3</a>, as well as my post \"<a href=\"http://kris.shaffermusic.com/2016/09/getting-data-out-of-medium/\">Getting data out of Medium</a>\".</i>\n</p>\n\n<p>In my last couple of posts, I walked through the process of retrieving data from Medium. Medium severely limits the data that is available via the API, and as my \"Getting data out of Medium\" post discusses, the process of exporting your posts from Medium and importing them into another platform is not as simple as I would like. Nevermind exporting other users' posts, or all the posts of a particular publication.&nbsp;</p>\n\n<p>However, getting data <i>into</i>&nbsp;Medium is very easy. Not only do they have a beautiful, easy-to-use editor for writing posts, but the Medium API makes it very simple to create new posts and drafts on their platform. In this post, I'll walk through that process.</p>\n\n\n<h2>Authenticating</h2>\n\n<p>We'll start by authenticating, using the Python SDK that Medium provides (and which I describe in <a href=\"http://kris.shaffermusic.com/2016/08/journey-through-api-programming-3/\">Part 3</a>). If you are already authenticated \u2015 from the activities of Part 3, for example \u2015 you can skip this part.&nbsp;</p>\n\n<p>Open a Python terminal, and enter the following (substituting your data for the X's \u2015 see Part 3 if you don't know what those details are):</p>\n\n~~~ python\nfrom medium import Client\nimport requests\nclient = Client(application_id=\"xxxxxxxxxxx\", application_secret=\"xxxxxxxxxxxxxxxxxxxxxxxxx\")\nauth_url = client.get_authorization_url(\u201csecretstate\u201d, \u201chttps://pushpullfork.com/callback\", [\u201cbasicProfile\u201d, \u201cpublishPost\u201d])\nprint(auth_url)\n~~~\n\n<p>This will return a long URL. Copy that URL, paste it into your browser window, and hit enter/return.&nbsp;The URL will change from what you entered in, and it will end with \u2018code=XXXXXXXX\u2019, where the X\u2019s represent your secret code. Copy that code and go back to your Python window. (Again, I don't know of a way to automate this part of the process. If you do, please let me know!)</p>\n\n<p>In your Python terminal, enter the following (replacing the X's with the code you just copied):</p>\n\n~~~ python\nauth = client.exchange_authorization_code(\u201cXXXXXXXX\u201d, \u201chttps://pushpullfork.com/callback\")\nclient.access_token = auth[\u201caccess_token\u201d]\nuser = client.get_current_user()\n~~~\n\n<p>Now you're authenticated!</p>\n\n\n<h2>Posting to Medium</h2>\n\n<p>Authenticating is the hard part, as I'm finding is often the case with APIs. Posting is a piece of cake by comparison. In fact, with the Python SDK, it's only a single line of code.</p>\n\n~~~ python\npost = client.create_post(user_id=user[\"id\"], title=\"Title\", content=\"<h2>Test title</h2><p>Trying to post with the Medium API.</p>\", content_format=\"html\", publish_status=\"draft\")\n~~~\n\n<p>This line of code will create a draft post that looks a bit like the following:</p>\n\n<blockquote><h2>Test title</h2>Trying to post with the Medium API.</blockquote>\n\n<p>I recommend using publish_status=\"draft\" so that you can look things over on Medium before publishing, especially if you are embedding media. However, if you are confident in your content and formatting, you can change to publish_status=\"public\".</p>\n\n<p>It's easy to add media. Simply include the proper HTML tags. Medium will automatically embed videos, but retrieve images from the linked source and serve them up from their own content delivery network. That means you can import images without worrying about them being deleted from the source. Medium will always have them. But you're at the mercy of whoever is hosting any videos you embed.</p>\n\n<p>You can also add tags in the API call. Here's a bit more substantive call that includes an image (from one of my blog posts) and sample tags:</p>\n\n~~~ python\npost = client.create_post(user_id=user[\"id\"], title=\"Title\", content=\"<h2>Test title</h2><p>Trying to post with the Medium API.</p><p>And testing out an image...<br/><img src=\\\"http://kris.shaffermusic.com/content/images/scaffold.jpg\\\" />\", tags=['tag1', 'tag2'], content_format=\"html\", publish_status=\"draft\")\n~~~\n\n<p>If you sent this call while authenticated to your own account, you'd see the post in your list of drafts. And if you opened the draft, you could inspect the photo element to see that it indeed has been retrieved from my server and is being served up by Medium's CDN. Click on \"Publish\" to see (and edit) the list of tags. tag1 and tag2 should already be in the list.\n</p>\n\n<p>Pretty simple! Once you're authenticated, the only hard part is coming up with the content for the post!</p>\n\n<h2>API vs. \"Import Story\"</h2>\n\n<p>Why use the API to post to Medium when you can just go to Medium and import the story? After all, all you need to do is paste in the URL of the original, and Medium does the rest. And in both cases, you'll probably need to do a little fine-tuning.</p>\n\n<p>I see the API's value here in a cross-posting scenario. Suppose you follow the POSSE (Publish on your Own Site, Syndicate Elsewhere) model. You like to create content on your own domain (WordPress, Jekyll, Known, Ghost...), but you want to syndicate to places like Medium and link to your social media accounts in order to boost readership. Medium's API makes it easy to automatically cross-post. Medium already has <a href=\"https://github.com/Medium/medium-wordpress-plugin\">an official WordPress plugin</a>&nbsp;to do just that. But the API can support development of plugins and tools for other platforms. I'll probably add cross-posting capabilities to <a href=\"https://peasy.pushpullfork.com\">Peasy</a>, for example. And I'm working on something that would make it easier to write in one place for both <a href=\"http://jekyllrb.com/\">Jekyll</a> (i.e., <a href=\"https://pages.github.com/\">GitHub Pages</a>) and Medium.</p>\n\n<p>I also imagine that the API would make it easy, or at least possible, to import a large batch of posts from another platform into Medium. Medium already supports <a href=\"https://help.medium.com/hc/en-us/articles/218572107-How-to-move-to-Medium\">importing from a WordPress export file</a>, but I haven't come across tools for other platforms. If you want to move more than a handful of posts from Tumblr or Jekyll to Medium, writing a script around the API is probably the way to go. (And don't forget to share that code so others can use it!)</p>\n\n<p>Of course, as I wrote yesterday, <a href=\"http://kris.shaffermusic.com/2016/09/getting-data-out-of-medium/\">getting your data out of Medium and into another platform is kind of a pain</a>. So personally, I see the value primarily in terms of cross-posting, or moving old posts into Medium so I can sync up a Medium publication with my own site, and then cross-post future additions.</p>\n\n<p>Whatever you want to do with the Medium API, it's really easy to get content into Medium. After authenticating, it's just one line of code!</p>\n\n<p>If you're using the Medium API, especially in conjunction with a Domain of One's Own, please get in touch. I'd love to hear about what you're doing!</p>\n\n*Featured image by [paul bica](https://www.flickr.com/photos/dexxus/5791228117/).*\n",
                        "html": "",
                        "image": "/content/images/apiHeader4.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "A journey through API programming \u2015 Part 4: Posting to Medium",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-09-27 14:41:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-09-27 14:41:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-09-27 14:41:00 -0400",
                        "published_by": 1,
                        "og_title": "A journey through API programming \u2015 Part 4: Posting to Medium",
                        "twitter_title": "A journey through API programming \u2015 Part 4: Posting to Medium",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/apiHeader4.jpg",
                        "twitter_image": "/content/images/apiHeader4.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Introducing Pypothesis \u2015 Part 2: a Python module for the hypothes.is API",
                        "slug": "introducing-pypothesis-2",
                        "markdown": "\nRecently I've created two tools to help people make fuller use of hypothes.is in their work as public scholars. This post is the second in a two-part series introducing and explaining those tools, which are based on the hypothes.is API. For Part 1, see [Introducing Pypothesis \u2015 Part 1: hypothes.is to MarkDown](http://kris.shaffermusic.com/2016/06/introducing-pypothesis-1/).\n\n\n## The hypothes.is API\n\nThe hypothes.is API is pretty powerful. [As I've written about previously](http://kris.shaffermusic.com/2016/04/hypothesis-public-research-notebook/), there are a lot of cool things the hypothes.is software can do that their browser plugin cannot (yet). To date, most discussion and marketing has focused around what the browser plugin can do when multiple scholars/students/collaborators are annotating a single web page. However, as a teacher I'm more interested in what it can do when students are sent out to find *a variety* of public resources to annotate and share with each other. As a researcher, I'm more interested in how I can use hypothes.is as a simple tool to collect things I read that I want to come back to or build on in my work. And as a programmer, I'm interested in making it easier for people to use a cool, open-source, non-profit tool like hypothes.is to do those things.\n\nSo what follows is my attempt to create a Python module for the hypothes.is API. Not only can I then use it to create scripts and apps that can accomplish some of those goals just mentioned, but others can use it to build their own extensions of hypothes.is more easily.\n\n\n## Pypothesis \u2015 a Python module for the hypothes.is API\n\nPypothesis (download from GitHub) is a Python module that provides programmers a simpler interface for the hypothes.is API. Rather than calling the API and parsing the resulting JSON data directly in an application, this module includes object classes and functions that make coding with the hypothes.is API simpler.\n\nSince I am particularly interested in helping (aspiring) public scholars use hypothes.is as an early-stage tool to find, share, and build off of existing work, this module is focused on the GET portion of the hypothes.is API (retrieving data, *not* writing, editing, or deleting existing data). It also does not deal with authentication since, again, I'm focused on *public* work, which does not need authentication to retrieve via the hypothes.is API. If others want to build on my work and augment my code, I'd be more than willing to consider pull requests (GitHub lingo for code other people have written that they'd like me to consider adding to my module). However, at least for now, I'm not planning on adding authentication, writing, editing, or deleting to this module.\n\nI hope this tool is of value to many. If you're interested in checking it out, making use of it, and/or building on it, the information and sample code offered below will help you get started.\n\nFollowing is a list of classes and functions in this module that Python programmers can use to incorporate hypothes.is functionality in their scripts.\n\n### Annotation()\n\nAn object class for a single hypothes.is annotation. Call Annotation(json_data_for_single_annotation) to create a new object. This object has the following attributes:\n\n- title (the title of the annotated article)  \n- uri (the uri of the annotated article)  \n- highlight (the article text highlighted in the annotation)  \n- comment (the annotation comment left by the annotator)  \n- user (the hypothes.is user ID of the annotator)  \n- created (the date and time the annotation was created)  \n- updated (the date and time the annotation was updated)  \n- id (the unique ID of the hypothes.is annotation; this ID is included in the URL for the annotation)  \n- hypothesisurl (the URL for the annotation)  \n\n### retrieve()\n\nA function that retrieves the JSON data for a single hypothes.is annotation, given the annotation's API URL. Call retrieve(api_url_for_a_single_annotation) to retrieve the JSON data, for passing into the Annotation() class.\n\n### retrievelist()\n\nA function that retrieves the JSON data for a list of hypothes.is annotations, given a well-formed search URL for the hypothes.is API. Call retrievelist(search_url_for_the_hypthes.is_api) to retrieve the JSON data for all annotations in the search results. Each annotation's JSON data is an item in a list. Use a for loop to pass each item returned into the Annotation() class.\n\n### apiurl()\n\nA function that converts a *share* URL (easy to find in the hypothes.is user interface) into an API-friendly URL (difficult to find), for passing to retrieve(). Call apiurl(share_url_for_an_individual_annotation) to return the API-friendly URL.\n\n### searchurl()\n\nA function that takes a hypothes.is user name and/or a tag (or list of tags) and generates a well-formed search URL for the hypothes.is API. The format is:\n\n~~~ python\nsearchurl(user = '', tag = '', tags = [])\n~~~\n\nThe searchurl() function requires at least one search term. It can be either a user name, a single tag, or a list of tags (as a well-formed Python list). Use *either* a single tag or a list of tags, not both. If you happen to send it both, it will take the list of tags and ignore the single tag.\n\nExample searches:\n\n~~~ python\n# all annotations from a single user\nsearchurl(user = 'kris.shaffer@hypothes.is')\n# or\nsearchurl('kris.shaffer@hypothes.is')\n\n# all annotations tagged IndieEdTech\nsearchurl(tag = 'IndieEdTech')\n\n# all annotations from a single user tagged IndieEdTech\nsearchurl(user = 'kris.shaffer@hypothes.is', tag = 'IndieEdTech')\n# or\nsearchurl('kris.shaffer@hypothes.is', 'IndieEdTech')\n\n# all annotations tagged IndieEdTech AND EdTech (for an OR search, simply perform two searches and combine the results)\nsearchurl(tags = ['IndieEdTech', 'EdTech'])\n~~~\n\n### Example code\n\n~~~ python\n# search for all annotations with the tag IndieEdTech and return them in json format.\ns = searchurl(tag = 'IndieEdTech')\nl = retrievelist(s)\n\n# print the title of each article annotated.\nfor entry in l:\n    e = Annotation(entry)\n    print(e.title)\n~~~\n\n~~~ python\n# Using the hyothes.is annotation share URL, retrieve and parse the JSON data for that annotation, then print it.\nt = Annotation(retrieve(apiurl('https://hyp.is/AVOP5R06H9ZO4OKSlTrY/hackeducation.com/2016/03/18/i-love-my-label')))\nprint(t.title)\nprint(t.uri)\nprint(t.highlight)\nprint(t.comment)\nprint(t.user)\nprint(t.created)\nprint(t.updated)\nprint(t.id)\nprint(t.hypothesisurl)\n~~~\n",
                        "html": "",
                        "image": "/content/images/stormdrain.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Introducing Pypothesis \u2015 Part 2: a Python module for the hypothes.is API",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-06-06 14:43:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-06-06 14:43:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-06-06 14:43:00 -0400",
                        "published_by": 1,
                        "og_title": "Introducing Pypothesis \u2015 Part 2: a Python module for the hypothes.is API",
                        "twitter_title": "Introducing Pypothesis \u2015 Part 2: a Python module for the hypothes.is API",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/stormdrain.jpg",
                        "twitter_image": "/content/images/stormdrain.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Introducing Pypothesis \u2015 Part 1: hypothes.is to MarkDown",
                        "slug": "introducing-pypothesis-1",
                        "markdown": "\nI've been working and writing a lot lately about using the web annotation tool [hypothes.is](https://hypothes.is) for public scholarship. It has a lot of cool uses \u2015 not only the collaborative annotation of individual web pages, but also the creation of a [public research notebook](http://kris.shaffermusic.com/2016/04/hypothesis-public-research-notebook/), and the possibility of linking hypothes.is with other apps [through the use of their open API](http://kris.shaffermusic.com/2016/05/getting-started-with-the-hypothesis-api/).\n\nBased on that work, I've created two tools to help people make fuller use of hypothes.is in their work as public scholars. This post is the first in a two-part series introducing and explaining those tools.\n\n\n## hypothes.is to MarkDown\n\nThe first tool is for those who like to blog. This tool is [a simple Python script](https://github.com/kshaffer/pypothesis/blob/master/hypothesisToMarkDown.py) that downloads a batch of hypothes.is annotations (from a single user, tagged with a single tag, or both) and formats them into nice, clean MarkDown text. It's based around the Jekyll platform (the basis for GitHub Pages), but if you use the MarkDown editor for WordPress, it will work for you too, with just one extra step than you might be used to \u2015 but it's a small one! (I'm hoping to build a more automated version for WordPress soon.) The script generates a full Jekyll-friendly MarkDown file, complete with header, so you can use the output as a page on your blog. If you use WordPress or another MarkDown-based blog platform, you can simply copy and paste all the text *except the header* and paste it into a new post/page on your site.\n\nTo use the script, download it from the link above, open it in a text editor (not a word processor like MS Word or Apple Pages), and go to the following section near the top of the file:\n\n~~~ python\n# adjust these variables for different searches\n# also be sure to adjust the page title in the YAML header section\nuser = 'kris.shaffer@hypothes.is'\ntags = 'IndieEdTech'\n# search string for fetching annotations from a specific user using a specific tag:\n#     searchstring = source + usercall + user + conn + tagcall + tags\n# search string for fetching all annotations from a specific user:\nsearchstring = source + usercall + user\n# search string for fetching all annotations from any user, but limited to a specific tag (a class hashtag, for example):\n#     searchstring = source + tagcall + tags\nfilename = 'jekyllOutput.md'\n~~~\n\nHere's where you define the search you want to perform. For example, if you want to take all of your annotations and add them to a single page on your blog, simply change 'kris.shaffer@hypothes.is' to your user name, and you're done. If you want to gather all of the public annotations *from anyone* marked with a particular tag, change 'IndieEdTech' to the tag in question. Then put a hash symbol (#) at the beginning of the following line:\n\n~~~ python\nsearchstring = source + usercall + user\n~~~\n\nand delete the hash symbol and spaces from the line:\n\n~~~ python\nsearchstring = source + tagcall + tags\n~~~\n\nIf you want to include only *your* public annotations *with a particular tag*, then Update\n\n~~~ python\nuser = 'kris.shaffer@hypothes.is'\ntags = 'IndieEdTech'\n~~~\n\nto the user and tag you want, then make sure the only 'searchstring' line without a hash in front of it is:\n\n~~~ python\nsearchstring = source + usercall + user + conn + tagcall + tags\n~~~\n\nYou also may want to change the output filename, especially if you use Jekyll and want to run the script regularly to create/update the same page.\n\nIf you use a programmer's text editor (TextMate, Atom, etc.), you can run the script right from your editor. Otherwise, open a terminal/command-line window, navigate to the folder the script is in, and type\n\n~~~ bash\npython hypothesisToMarkDown.py\n~~~\n\nIf you have a Jekyll blog and updated the file name appropriately, you're good to go! Now just push it to the server. If you run WordPress, open the output file, copy the new MarkDown text, and paste it into the appropriate page. (Again, be sure you have the MarkDown editor enabled.)\n\nThe one downside to this script is that you have to run it regularly \u2015 every time you know or suspect it needs to be updated. The page will not auto-update. (That's one thing I plan on looking into soon.) But for now, it does some cool stuff. :)\n\nPlease try it out and let me know how it works. I'm sure that as more people use it, they will notice more kinks that need to be worked out. Also, keep checking for updates, as I'll be fixing and enhancing code as things come up. Enjoy!\n\nCheck out [Part 2](http://kris.shaffermusic.com/2016/06/introducing-pypothesis-2/) for a description of the full Python module for programmers interested in making use of the hypothes.is API.\n",
                        "html": "",
                        "image": "/content/images/oldtype.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Introducing Pypothesis \u2015 Part 1: hypothes.is to MarkDown",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-06-06 13:58:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-06-06 13:58:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-06-06 13:58:00 -0400",
                        "published_by": 1,
                        "og_title": "Introducing Pypothesis \u2015 Part 1: hypothes.is to MarkDown",
                        "twitter_title": "Introducing Pypothesis \u2015 Part 1: hypothes.is to MarkDown",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/oldtype.jpg",
                        "twitter_image": "/content/images/oldtype.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Syllabus for 'The Flipped Classroom' at Hybrid Pedagogy Courses",
                        "slug": "syllabus-for-the-flipped-classroom-at-hybrid-pedagogy-courses",
                        "markdown": "\n*Starting on July 19, I will be leading a 3-week, intensive online course on the \"flipped\" or \"inverted\" classroom. This is the first professional development course offered by [Hybrid Pedagogy](http://www.hybridpedagogy.com) as part of its new [Digital Pedagogy Lab](http://www.digitalpedagogylab.com/). What follows is a draft syllabus for the course. I'm still finalizing the reading list, and will post that once it is set.*\n\n*If you're interested in the course, you can [register here](http://www.digitalpedagogylab.com/blog/course/the-flipped-classroom/). There is a $100 early-bird discount for those who register by July 5.*\n\n# Overview\n\nWhat does an active, engaged, student-oriented class look like?\n\nBy definition, every student-oriented class will look different. To the extent that they look the same, classes are not really centered around the students and their work. While we can define \u201cinverted\u201d or \u201cflipped\u201d pedagogy in some specific ways, the emphasis on student activity and agency means that inverted pedagogy must be characterized by a diversity of practices and results.\n\nIn this course, we will explore the diversity of practices that are typically associated with the inverted/flipped classroom, taking both a practical and a critical perspective. On the practical side, we will study each of these techniques (including the \u201cbasic flip,\u201d inquiry-driven learning, peer instruction, etc.), engaging both peer-reviewed research and instructor/student anecdotes, and create lesson plans for our own courses that are consistent with the technique. On the critical side, we will analyze the implicit values and goals of each technique, and the ends (if any) towards which each technique can and should be employed. The course will conclude with a final project, in which we\u2019ll produce a complete syllabus/plan for a course we\u2019ll teach in the near future, and a subset of lesson plans/materials, based on one or more of the practices engaged in the course.\n\nBecause inverted pedagogy privileges active, experiential learning, as well as collaborative work and peer feedback, we will take [an inverted approach to the course](http://kris.shaffermusic.com/2015/05/flipping-an-online-class/): student work will be the central focus of the course, and collaboration and peer instruction will be employed throughout the course. Thus, we\u2019ll gain both informational and experiential understanding of many of the practices we explore.\n\nA certificate of completion is available at the end of the course. This course counts towards *Hybrid Pedagogy*\u2019s forthcoming Certification Program in Critical Digital Pedagogy.\n\n# Required materials\n\nThere are no required purchases. All reading materials will be open-access or provided for study purposes within the private course Learning Management System (LMS). You will, however, need a few free tech tools/accounts:\n\n- **A public blog.** If you do not already have one, or want to use a separate site for this course, I highly recommend the [Known](http://withknown.com) platform, especially for beginners seeking a quick start.  \n- **A public Twitter account.** Pseudonyms are allowed, but please let your course colleagues and me know your pseudonym!  \n- **Other online resources TBA.** We are still exploring what online discussion and file/link-sharing tool(s) will be best for this course's format.\n\n# Schedule and coursework\n\nThe course will be divided into three one-week units.\n\n## Unit I: July 19\u201325\n\n### Topics\n\n- Personal introductions  \n- What is the \"flipped\"/\"inverted\" class model?  \n- Video microlectures/screencasting  \n- Active student work in class  \n- Just-in-time teaching (JiTT)  \n\n### Readings\n\n*Final reading list TBA.*\n\n### Schedule\n\nFollowing is a schedule overview. Full assignment prompts will be provided in the course LMS.\n\n**Before Monday morning:** Blog post: personal introduction.  \n**Before Monday morning:** Blog post: pre-reading reflection.  \n**Before Tuesday morning:** Blog post: reading response.  \n**Before Wednesay morning:** Post comments on at least three other particpants' personal introductions and reading responses.  \n**Before Thursday morning:** Blog post: lesson plan (pre-class HW, class activity/content, follow-up HW) based on one or more methods covered in the readings.  \n**Before Friday morning:** Post comments on at least three other participants' lesson plans.  \n**Before Friday morning:** Blog post: follow-up reflection/\"exit ticket\".\n\n*I also anticipate having two opportunities for synchronous discussion during Unit I. Schedule and format (Twitter/video-chat/etc.) will be determined by a poll of course participants.*\n\n\n## Unit II: July 26\u2013August 1\n\n### Topics\n\n- Peer Instruction & \"clickers\"  \n- Inquiry-driven learning  \n- Problem-based learning\n\n### Readings\n\n*Final reading list TBA.*\n\n### Schedule\n\nFollowing is a schedule overview. Full assignment prompts will be provided in the course LMS.\n\n**Before Monday morning:** Blog post: pre-reading reflection.  \n**Before Tuesday morning:** Blog post: reading response.  \n**Before Wednesay morning:** Post comments on at least three other particpants' personal introductions and reading responses.  \n**Before Thursday morning:** Blog post: lesson plan (pre-class HW, class activity/content, follow-up HW) based on one or more methods covered in the readings.  \n**Before Friday morning:** Post comments on at least three other participants' lesson plans.  \n**Before Friday morning:** Blog post: follow-up reflection/\"exit ticket\".\n\n*I also anticipate having two opportunities for synchronous discussion during Unit II. Schedule and format (Twitter/video-chat/etc.) will be determined by a poll of course participants.*\n\n\n## Unit III: August 2\u20138\n\n**Before Friday morning:** Final project.\n\nCreate a full course plan for a real course you intend to teach in the coming year, based on one or more of the inverted-pedagogy practices explord in this course. Post it publicly on your course blog.\n\nCreate a complete, detailed syllabus for the course, as well as at least two weeks of materials, including class activities, instructor-generated resources, homework assignments, projects, and/or tests, as appropriate. Some of these materials should be revisions of materials created in Units I and II, taking instructor and colleague feedback into account.\n\n# Assessment\n\nIn general, this is a course with no grades. However, I will provide comments and feedback on all work completed on schedule. Also, participants will be able to obtain a certificate of completion, if so desired.\n\nTo obtain a certificate of completion, participants complete 85% of the assigned work. To obtain a certification *with distinction*, participants complete 95% of the assigned work, *and* draw on optional readings or other non-required materials in at least 50% of the written blog posts (pre-reading reflections, reading responses, and follow-up reflections).\n\nTo certify completion, make a copy of [this Google Document]() (link forthcoming), which contains a list of all the course assignments. For each assignment completed, insert a link to your blog post (or the blog post on which you commented). At the end of the course, share the document with me (kris@hybridpedagogy.org).\n\n# Contacting the instructor\n\nThe best ways to reach me during the course are [email](mailto:kris@hybridpedagogy.org) and [Twitter](http://twitter.com/krisshaffer). Please allow up to 24 hours for a response, though during summer courses, I tend to respond much more quickly than that during morning and afternoon. Messages sent on Saturday or Sunday will likely not receive a reply until Monday.\n\n# About this syllabus\n\nThis syllabus is a *summary of course objectives and content*, not a contract. *All information in this syllabus is subject to change, with sufficient advanced notice provided by the instructor.* I will also consider proposals for changes or individually appropriate alternate forms of assessment from participants enrolled in the course.\n",
                        "html": "",
                        "image": "/content/images/jellyfish.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Syllabus for 'The Flipped Classroom' at Hybrid Pedagogy Courses",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-06-29 12:50:40 -0600",
                        "created_by": 1,
                        "updated_at": "2015-06-29 12:50:40 -0600",
                        "updated_by": 1,
                        "published_at": "2015-06-29 12:50:40 -0600",
                        "published_by": 1,
                        "og_title": "Syllabus for 'The Flipped Classroom' at Hybrid Pedagogy Courses",
                        "twitter_title": "Syllabus for 'The Flipped Classroom' at Hybrid Pedagogy Courses",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/jellyfish.jpg",
                        "twitter_image": "/content/images/jellyfish.jpg"
                    },
                    {
                        "id": 0,
                        "title": "From blended learning to hybrid pedagogy",
                        "slug": "from-blended-learning-to-hybrid-pedagogy",
                        "markdown": "\n*The following is a talk I shared at Moravian College to humanities faculty as they work through issues of blended learning in a liberal arts context. The original title of the talk was \"Digital affordances, digital limitations: blended learning in liberal arts education.\" I have since updated it for a presentation at Bates College.*\n\nWhen I was in elementary school, I had a wooden desk with cutout sections at the top. The long skinny sections, I assumed, were pencil holders, and I used them as such. But the square in the corner I didn't quite understand. My classmates and I used them for erasers, paperclips, and (if I remember correctly) spitballs and other contraband. But none of us really knew what they were for.\n\nIt was years later when I realized that not only were the desks leftovers from a previous generation, but so was the design. These square cutouts were for ink wells. In fact, it was only at this time that I learned why we called the study of handwriting *penmanship* when we were only allowed to use pencils. In those bygone days, not only did they use pens \u2015 fountain pens, dip pens, quill pens \u2015 but they studied the proper use of the pen, not just how to write legibly.\n\nRecently I've become something of a fountain pen geek. I say geek rather than connoisseur because I haven't spent \u2015 and in fact I don't have \u2015 enough money to consider myself even an armchair expert in fine writing implements. However, I have several inexpensive fountain pens, each with a different nib. And on my standing desk at work, alongside my Microsoft Surface tablet and my 4K display, I have three wells of ink.\n\n<img src=\"/content/images/dip-pen.jpg\" alt=\"writing with a fountain pen\" />\n\nOver the past two years, I've discovered the enjoyment, and at times the zen, of writing with fountain pens. It's taught me several things. It's taught me the value of good paper. Pencils and ball points can write on just about anything, but a fountain pen just doesn't work right on cheap, scratchy paper. It's taught me how to write more neatly. Not only do I try a little harder when I have a nice pen and nice paper, but *the pen works better* when I use proper form. As a left-hander, I've always struggled with neat handwriting (and with incompetent or not-so-understanding penmanship teachers), so it feels good to write something by hand and like the way it looks.\n\nWriting with fountain pens has also taught me something about technology. A good pen and a nice notebook are probably my favorite technologies, alongside a variety of musical instruments. But as I've used them more and more, and used them alongside pencils, ballpoints, dry erase markers, and various digital devices, I've grown keenly aware of the affordances and limitations of each. As a left-handed writer, I am acutely aware of the limitations. Let me illustrate.\n\nRemember this?\n\n<img src=\"/content/images/cursive.png\" alt=\"cursive writing chart\" />\n\nFor several years in elementary school, I was required to write every school assignment in cursive. I hated it. For roughly the same several years in elementary school, I was constantly hounded by my teachers for my poor handwriting. I was gifted, and stereotypes about gifted children include rushing through easy schoolwork, leaving a sloppy mess behind, so they can more quickly move onto something of interest. This was probably true of me, and it's certainly true of my oldest (right-handed) son. But this wasn't the whole story. My teachers, especially my third grade teacher, were very particular about the direction my pencil moved when making letters. Why must I make my O's counterclockwise, even when printing?! Why must the \"flag\" on my 5's be an additional stroke?\n\nI learned why only last year. I mentioned before that writing with a nice pen on nice paper makes me want to write especially neatly. As a left-handed writer, this is difficult, especially with fountain-pen ink. To make the ink move smoothly through the feed and down the nib, the ink must be wet. If it dries too quickly, it will clog the pen, and the pen will alternate scratches and blobs. But as a left-hander, my hand follows what I write, and if I'm not careful, it smudges the ink.\n\n<img src=\"/content/images/davinci.jpg\" alt=\"da Vinci's notebook\" />\n\nSo I took a page out of Leonardo da Vinci's book, and I tried mirror writing (where you write right-to-left with reversed letters). It only took me a little while to get the hang of it, and almost instantaneously I was able to write as quickly as left-to-write, and *much more neatly*. No smudges, prettier letters, more even spacing... Everything just worked. I eventually abandoned mirror writing, though, when I realized that I couldn't *read* it nearly as readily as left-to-right letters. But the lesson stuck with me. Being able to see the letters as I write and not worrying about smudging the ink after I write meant a more natural hand position, and better handwriting overall. *This is what it's like to be right-handed!*, I thought. And that's when I realized that the limitations of my own penmanship were directly related to the affordances of pen, paper, and ink for *right-handed* writers.\n\nI noticed something else, too. Remember those struggles with cursive? As soon as my teachers let me, I abandoned it for printing. For a while, I was only writing in all capital letters. But the fluidity of the fountain pen made me want to try cursive again. It was a mess. The lack of practice, plus all the old problems, made it pretty much a fool's errand. But then I tried it in mirror writing. The strokes were unfamiliar, so it wasn't easy, but after just a little practice, like printing, it was better in mirror writing than in left-to-right. Not only was it neater, though. I learned through experience why cursive developed the way it did.\n\nHave you ever wondered why the capital letters all seem to start with a little uptick? (Which, if I'm honest, looks goofy and always struck me as silly.) When I try to write cursive, left-handed, with an italic nib, those upticks are very awkward, and they look strange. I'll tell you that if I were using a dip pen, or especially a quill pen, I would make a complete mess of the page, as the pen would get stuck, then skip and splatter ink everywhere. This is why many left-handers \"hook\" their hand when they write, or turn the page sometimes a full 90 degrees. It's the only way to make it work. (I've verified this while attempting calligraphy with a quill. It only works 90 degrees off-axis.) But if I use mirror writing, the motions are the same as a right-handed writer. The uptick actually starts with the pen going down and then sideways. This is an ideal motion to get ink flowing at the start of a word: the downward motion gets the ink started, and the sideways motion gets it moving at a steady, but not too torrential, flow.\n\nIf we look back at the chart of cursive letters, we can see that this angle of motion begins many of the letters, both upper- and lower-case. This motion is, for a left-handed writer, awkward at best, prohibitive at worst (especially with a quill). But for a right-handed writer with a quill or dip pen, they are absolutely essential to legible writing.\n\n<img src=\"/content/images/german-cursive.jpg\" alt=\"cursive writing\" />\n\nAnd that's the key here. Why did I learn cursive as a young student? Why was it the required mode of written discourse for several (painful) years of my childhood? Because for past generations, *it was the mode of writing that best lined up with the affordances and limitations of the dominant writing technologies for the dominant class of writers.* Because of that, this medium became not just a form of discourse, but educational content in its own right. And when new writing technologies emerged that were more practical and less expensive, *the content of cursive penmanship was preserved until there was competition for that pedagogical space*. For at least three generations, content was taught not because of its own intrinsic value, but because of its technological expediency for past generations. New wine in old skins. Or, rather, old wine in new skins. Only now that digital technologies are competing for that space are we discussing the relative merits of teaching cursive to our children in American elementary schools.\n\nTo sum up, this story about my struggles with cursive illustrates two pedagogical problems that can easily emerge when choosing a technology to adopt:\n\n- Choosing a technology based on its affordances *for members of the dominant class* (or simply whatever class the developers belong to) *leaves people out*.  \n- If we don't constantly re-evaluate our educational purposes and our technological choices, we'll end up wine-wineskin mismatch \u2015 tools and technologies lined up with someone else's educational goals rather than our own.\n\nLet me illustrate with one more (shorter!) story, one that shows the opposite side of the coin: when old pedagogical practices aren't up to new challenges.\n\n<img src=\"/content/images/sm-propaganda.jpg\" alt=\"social media propaganda poster\" />\n\n<p style=\"text-align: center\" target=\"blank_\"><i>Social media propaganda posters, by <a href=\"https://www.etsy.com/shop/Justonescarf\" target=\"blank_\">Aaron Wood</a>.</i></p>\n\nMisinformation abounds. This has always been the case, but the problem has become acute in the age of digital communication. As <a href=\"https://hapgood.us/2016/11/13/fake-news-does-better-on-facebook-than-real-news/\" target=\"blank_\">Mike</a><a href=\"https://hapgood.us/2016/11/13/fake-news-does-better-on-facebook-than-real-news/\" target=\"blank_\"> Caulfield</a> and <a href=\"http://www.nytimes.com/2016/11/15/opinion/mark-zuckerberg-is-in-denial.html\" target=\"blank_\">Zeynep Tufekci</a> have been showing in the week following the 2016 US presidential election, Facebook is particularly susceptible to this problem. Of course, Facebook is not alone. The ease with which we can share \u201cnews\u201d on social media platforms makes it increasingly easy to contribute to the virality of falsehoods.\n\nI was tweeting about this phenomenon last week. Amid a stream of comments I posted to Twitter about deceptive media practices, I received this two part response from someone I don\u2019t know:\n\n> And that does not count the obvious foreign influence of people like Soros.<br> I\u2019ll throw RT and Putin in there just to keep the balance of the narrative, but the point stands.\n\nThis is subtle, and rather artful when I think about it, deception. Let\u2019s unpack it.\n\nThe first message is a jab at businessman George Soros, someone the far right often accuses of manipulating leftist activists to suit his own aims. Though it is a common accusation from white nationalists, I hadn\u2019t heard of it before. So I Googled \u201cSoros\u201d to see what the story was. Among results like Wikipedia and Soros\u2019s own web page are white nationalist, fake-news sites talking about reasons that Soros is \u201cdangerous,\u201d a co-conspirator with the Clintons, and the \u201chidden hand\u201d behind anti-Trump protests. If I wasn\u2019t suspicious of these claims to begin with, these Google results might make me sympathetic with the deception. I\u2019m already distrustful of billionaires trying to influence politics, education, and the media, and one can easily assume that top Google results mixed in with Wikipedia and the New York Review of Books would be fairly legitimate sources. The combination of innocuous and reputable sounding sources on the Google results page (gleaned just from their domain names) and a healthy dose of confirmation bias (he\u2019s a billionaire \u201cactivist,\u201d after all!) is dangerous for liberal readers trying to keep up with Twitter\u2019s information bombardment.\n\nThe second part of the message is more insidious, though. It makes reference to <a href=\"https://www.rt.com/\" target=\"blank_\">RT</a> (the state-run Russian news service), Putin, and the \u201cbalance of the narrative.\u201d In an attempt to be balanced, the author references <a href=\"http://www.nytimes.com/2016/08/29/world/europe/russia-sweden-disinformation.html?_r=0\" target=\"blank_\">two known sources of false information</a>, one of which is admired by Trump and has been <em>potentially</em> (but not definitively) linked to the leaking of information that may have cost Hillary Clinton the presidency. This combines two subtle and effective forms of deception: linking a lie to a truth to make it more credible (we know that RT and Putin are foreign actors that spread falsehood through the media to manipulate people, so why not Soros?), and linking a conspiracy <em>theory</em> (Soros manipulating the left) to an actual <em>conspiracy </em>(Russian involvement in the DNC server hack) to give the conspiracy theory more credulity. This combination of truth with <a href=\"https://www.washingtonpost.com/posteverything/wp/2016/11/18/my-fake-news-list-went-viral-but-made-up-stories-are-only-part-of-the-problem/?utm_term=.e17d707d9918\" target=\"blank_\">\u201ctruthy\u201d lies</a>, aimed at both the propagation of lies and the questioning of what we already know to be true is a psychological abuse tactic called <a href=\"https://everydayfeminism.com/2015/08/things-wish-known-gaslighting/\" target=\"blank_\"><em>gaslighting</em></a><em>.</em> (For more on the impact of repeating \u201ctruthy\u201d claims until they are taken for truth, see Audrey Watters\u2019s \u201c<a href=\"http://hackeducation.com/2016/12/01/top-ed-tech-trends-wishful-thinking\" target=\"blank_\">Education Technology and the Age of Wishful Thinking</a>.\u201d)\n\nLet me clarify. This isn\u2019t classic gaslighting, which tends to come as a sustained, subtle abuse of one person by another, usually someone in a close relationship with the abused. This is a new brand of digital gaslighting, where large groups of people (and their <a href=\"https://en.wikipedia.org/wiki/Sockpuppet_(Internet)\" target=\"blank_\">sock-puppet</a> Twitter accounts and bots) attack both individuals and groups, with the effect of the targeted group losing their <em>collective</em> grip on reality. (This was a common tactic during <a href=\"https://www.washingtonpost.com/news/the-intersect/wp/2014/10/14/the-only-guide-to-gamergate-you-will-ever-need-to-read/\" target=\"blank_\">the GamerGate </a><a href=\"https://www.washingtonpost.com/news/the-intersect/wp/2014/10/14/the-only-guide-to-gamergate-you-will-ever-need-to-read/\" target=\"blank_\">movement</a>.) Individuals may not question their own sanity, but they question the reality of their friends and allies. They question the truth of things for which there is good evidence, and they become susceptible to truthy lies. And when they uncritically retweet those truthy lies, those truthy untruths circulating alongside sometimes surreal truths fuel the uncertainty people have started to feel about their own movement. Worse yet, they give the attackers evidence to point to about the lies told by the movement, discrediting them in the face of moderates and the undecided. (Several in my social media circles suggested that this was the motivation behind the sharing of <a href=\"https://www.buzzfeed.com/dinograndoni/nope-this-video-is-old?utm_term=.ii7VE1QaP#.oqdjQ4vJG\" target=\"blank_\">a video of Venezuelan protests</a>, captioned as anti-Trump protests in Los Angeles.)\n\nFacing tactics like these, digital literacy and critical thinking about digital media require far more than knowledge of the fallacies of informal logic. <em>Ad hominem</em> attacks, <em>reductio ad absurdum</em>, the intentional fallacy \u2014 these pale in comparison to coordinated digital deception, powered by sock-puppet Twitter accounts, SEO expertise, and a Facebook algorithm that privileges fake news. We need a new, critical digital literacy: a deep understanding of the technological, sociological, and psychological implications of connective digital media and how people use it, with a view towards mindful, ethical media creation and consumption. This is more than traditional information literacy applied to digital media, more than technical knowledge of digital media production and network protocols. Connective digital media enables new modes of media creation and human activity, and critical digital literacy requires grokking those new modes, in addition to grasping the implications of porting \u201ctraditional\u201d media practices to the digital. And seeing both <a href=\"https://www.academia.edu/2493339/Social_Media_and_the_Decision_to_Participate_in_Political_Protest_Observations_From_Tahrir_Square\" target=\"blank_\">the good</a> and <a href=\"http://www.adl.org/assets/pdf/press-center/CR_4862_Journalism-Task-Force_v2.pdf\" target=\"blank_\">the bad</a> (and <a href=\"http://www.tabletmag.com/scroll/219117/we-built-a-bot-that-trolls-twitters-worst-anti-semitic-trolls\" target=\"blank_\">the good responses to the bad</a>) ways in which these new modes are enacted, it is clear that we must engage them, especially those of us who educate. But how do we?\n\n(<em>The previous few paragraphs are excerpted from my article <a href=\"http://www.digitalpedagogylab.com/hybridped/truthy-lies-surreal-truths/\" target=\"blank_\">\"Truthy Lies and Surreal Truths: A Plea for Critical Literacies\"</a> in</em> Hybrid Pedagogy.)\n\nLet's take a step back, first, and consider the landscape I've laid out. After discussing the pedagogy of penmanship, I noted two problems we need to keep in mind:\n\n- Choosing a technology based on its affordances *for members of the dominant class* (or simply whatever class the developers belong to) *leaves people out*.  \n- If we don't constantly re-evaluate our educational purposes and our technological choices, we'll end up wine-wineskin mismatch \u2015 tools and technologies lined up with someone else's educational goals rather than our own.\n\nCrowd-powered deceptive digital media adds more potential pitfalls and a need for new literacies like crap detection, attention management, and collaborative networked knowledge.\n\nIn the case of cursive, the new technologies of pencils and ball-point pens caused minimal friction. Very few new skills were required to adopt the new tools, and the old content and methods caused no major conflict with the new tools, so there was no pressure to push cursive out of the curriculum. However, the rise of digital technology, especially the internet, means that not only are computing skills necessary additions to the curriculum, but new modes of critical thinking and creative work \u2015 long hallmarks of liberal-arts education \u2015 are necessary. But surely these new focuses come at a cost? When we add something to our curriculum, to our mission, certainly something has to go? And we certainly don't want to abandon the things that we have done so well for so long?\n\nThis is often the frame around discussions of digital technology and digital pedagogy in higher education, especially in the liberal arts. But I wonder if we can add some nuance to this discussion. If we can move past the idea of <a href=\"https://www.slideshare.net/jessestommel/open-door-classroom\" target=\"blank_\">courses as containers for content</a> (Slide 12), and zero-sum games of curricular redesign. Can we create a hybrid, or blended, learning environment where we maximize the retention of what we've done well for ages while simultaneously taking advantage of new approaches and addressing new needs?\n\nI think some theoretical framework could help us as we address this question. In particular, I'd like to tease out the difference between *blended learning*, a term you are likely familiar with and which I've already hinted at today, and *hybrid pedagogy*, likely a less familiar concept, but an important one to help us approach the idea of digital pedagogy in a liberal arts context.\n\nJesse Stommel (my colleague at Mary Washington and co-founder of the journal, *Hybrid Pedagogy*) writes:\n\n> At its most basic level, the term \u201chybrid,\u201d ... refers to learning that happens both in a classroom (or other physical space) and online. In this respect, hybrid does overlap with another concept that is often used synonymously: blended. I would like to make some careful distinctions between these two terms. <a href=\"http://en.wikipedia.org/wiki/Blended_learning\" href=\"http://en.wikipedia.org/wiki/Blended_learning\" target=\"blank_\" target=\"blank_\">Blended learning</a> describes a process or practice; hybrid pedagogy is a methodological approach that helps define a series of varied processes and practices. (Blended learning is tactical, whereas hybrid pedagogy is strategic.) When people talk about \u201cblended learning,\u201d they are usually referring to the <em>place</em> where learning happens, a combination of the classroom and online. The word \u201chybrid\u201d has deeper resonances, suggesting not just that the <em>place</em> of learning is changed but that a hybrid pedagogy fundamentally rethinks our <em>conception of place</em>. (Thanks to <a href=\"https://twitter.com/vsuter\" href=\"https://twitter.com/vsuter\" target=\"blank_\" target=\"blank_\">@vsuter</a> for helping me work through my thinking on this.) So, hybrid pedagogy does not just describe an easy mixing of on-ground and online learning, but is about bringing the sorts of learning that happen in a physical place and the sorts of learning that happen in a virtual place into a more engaged and dynamic conversation.<br>...<br>Hybridity is about the moment of play, in which the two sides of the binaries [teacher/student, analog/digital, passive/experiential, etc.] begin to dance around (and through) one another before landing in some new configuration.<br>(<a href=\"http://www.digitalpedagogylab.com/hybridped/hybridity-pt-2-what-is-hybrid-pedagogy/\" target=\"blank_\">\"Hybridity, Pt. 2: What Is Hybrid Pedagogy?\"</a>)\n\nThese are helpful distinctions. Let's start with the idea of blended learning as instrumental, tactical. Isn't that how we normally hear it described, both from some of our administrators, and especially from legislators and vendors? Blended learning as a technique is often invoked as a way to increase the efficiency, decrease the cost, or up the scale of \"delivering\" education to students. Framed this way, we assume a lot. We assume that the content of education remains the same; it is the delivery method that changes. We assume that the mark of a good educational method is a high rate of return on investment. And those of us who are (or have been) faculty recognize the familiar marks of a top-down, often unfunded, mandate to \"move\" our courses into the \"digital realm.\"\n\nThis largely neoliberal and unreflective version of blended learning as an instrumental change is not uncommon. We see it from organizations like <a href=\"https://www.edutopia.org/practice/blended-learning-making-it-work-your-classroom\" target=\"blank_\">Edutopia</a> and <a href=\"https://library.educause.edu/~/media/files/library/2010/11/eli3023-pdf.pdf\" target=\"blank_\">Educause</a>, we see it in <a href=\"https://blended.online.ucf.edu/about/what-is-blended-learning/\" target=\"blank_\">university teaching resources</a>, we even see it in <a href=\"https://www.greatagain.gov/policy/education.html\" target=\"blank_\">our president's policy platform for education</a>. The assumption that education is content delivery leads quickly to the discussion of the best tools for the job, with digital tools often presented as good candidates for information delivery.\n\nBut in the liberal arts, we know that education is far more than the delivery of information. As Paulo Freire writes, in *Pedagogy of the Oppressed*, \"The more students work at storing the deposits entrusted to them, the less they develop the critical consciousness which would result from their intervention in the world as transformers of that world\" (p. 73). That critical consciousness is at the core of liberal education, not merely informational deposits. It is not, as <a href=\"https://www.youtube.com/watch?v=WwslBPj8GgI\" target=\"blank_\">Eric Mazur quips about lectures</a>, the art of transferring information from the instructor's notebook to the student's notebook without passing through the brains of either. Instead, the purpose of education is \"the practice of freedom,\" a phrase used by Paulo Freire, bell hooks, Richard Schaul, and many others. Or as my institution's vision states, \"We are a place where faculty, students, and staff share in the creation and fearless exploration of knowledge through freedom of inquiry, personal responsibility, and service.\" The goal of helping students be active, critical, creative, and engaged in their world, helping them harness their agency to have a transformative impact on the world ... these are the hallmarks of liberal education, not information transfer.\n\nThat said, digital tools can be used alongside or in place of analog tools to accomplish the purposes of liberal education, as well. In fact, as I illustrated earlier with my experience last week on Twitter, the world we want our students to engage critically, and to ultimately transform for the better, that world is increasingly digital. Blended learning environments can help us help our students have that transformative impact.\n\n<img src=\"/content/images/intersection.jpg\" alt=\"Steve Jobs, technology meets the liberal arts\" />\n\nBut how do we design a blended learning environment for the liberal arts? Were education really just the transfer of information, we would simply present students with information in-person, via a printed book, via an ebook, and via a video lecture, and test their retention on a multiple-choice test to discover the best method for information delivery. The problem is that there is no standardized test for student agency, for transformative impact, for the ability to find new answers we didn't anticipate, let alone to ask insightful questions that have never been asked before.\n\nThat's where the philosophy of hybrid pedagogy can help. Back to Jesse's article:\n\n> [H]ybrid pedagogy does not just describe an easy mixing of on-ground and online learning, but is about bringing the sorts of learning that happen in a physical place and the sorts of learning that happen in a virtual place into a more engaged and dynamic conversation.<br>...<br>Hybridity is about the moment of play, in which the two sides of the binaries [teacher/student, analog/digital, passive/experiential, etc.] begin to dance around (and through) one another before landing in some new configuration.\n\nLet me unpack that a bit more. In *The Archaeology of Knowledge*, Michel Foucault writes, \"We must question those ready-made syntheses, those groupings that we normally accept before any examination, those links whose validity is recognized from the outset.\" He is writing in the context of an exploration of history, in which the task of the modern historian is to disrupt the unities of discourse, unpack traditions that are assumed but on closer examination reveal themselves to be both fraught and ideologically grounded. Writing about written documents, he says:\n\n> The book is not simply the object that one holds in one's hands; and it cannot remain within the little parallelepiped that contains it: its unity is variable and relative. As soon as one questions that unity, it loses its self-evidence; it indicates itself, constructs itself, only on the basis of a complex field of discourse (p. 23).\n\nI'm convinced that we can say the same about the \"unities of [pedagogical] discourse\" as well. The motto of the journal *Hybrid Pedagogy* is that \"all learning is necessarily hybrid.\" Putting that motto in dialog with Foucault, we can say, \"all elements of education are necessarily hybrid.\" They are variable and relative, fraught and fragile. And as soon as we question their coherence, they lose their self-evidence. But just like Foucault's historical objects, when we question their unity, they don't just fall apart. They reveal their variability, their relativity, their hybridity \"on the basis of a complex field of discourse.\" And with knowledge of that field, and critical reflection on where the field is going, where it could be going, we can reconstruct new unities \u2015 themselves hybrids as well \u2015 as we (re)design learning objects and environments.\n\nTake the course as an example. In the course, there are typically two roles: teacher and student. (We'll leave aside TAs, graders, and tutors for the sake of illustration.) What is a teacher? We could define it in a number of ways \u2015 the person with expertise relative to the course content, the person who communicates that expertise to others, the person who designs the course, the person who assesses the progress of others in the course, etc. Likewise we could define a student as the person lacking in (and seeking) expertise in a subject, the person who listens with a view toward understanding, the person who submits to course design decisions, the person whose progress is assessed, etc. Are these coherent, self-evident unities? Or are they hybrid entities?\n\nStart with the teacher. Instead of thinking about the role of teacher, let's think about *our experience* as people in the courses we teach. Do we enter the class with expertise about the subject matter, or with a desire to learn more? Do we come to talk or to listen? What about our students \u2015 or ourselves when we were students? Did we enter the class as students lacking in expertise, or did we have insights to offer? Did we come to talk or to listen? Did we feel like we were usurping the teacher's role when we spoke? When we ask these questions, it becomes clear that these roles are necessarily hybrid at their core. The critical pedagogy of Paulo Freire asks us to deconstruct the student/teacher binary, but hybrid pedagogy recognizes that that binary never really existed. Student and teacher were always hybrid roles.\n\nWith that understanding, new questions open up. If the teacher is not the absolute bearer of expertise, should the teacher be the absolute designer of the course? the one person charged with assessing the progress of others? Or could we build on the hybridity inherent in the roles of student and teacher, and the expertise and agency *already possessed by the students*, to involve students in course design decisions, in assessment of themselves, each other, the teacher? These are the questions of hybrid pedagogy.\n\nNow let's bring them to the concept of blended learning. As a purely instrumental concept, blended learning assumes the self-evident, intrinsic unity of concepts like *digital* and *analog*, *online* and *in-person*. But let's ask what these things mean. Let's start with *online*. What is \"purely\" *online learning*? [in-person discussion] What is \"purely\" *in-person learning*? [in-person discussion]\n\nThese concepts are already hybrid. Online students rarely do all of their learning in front of a screen. And even students in a homework-free class cannot contain the cognitive and bodily work of that class entirely within the boundaries of the class. They think about it when they leave, they talk about it with others, it changes how they interact with the world, maybe even keeps them awake at night. So if they are already hybrid by nature, what does it mean to blend them? And what does it mean to hybridize online and in-person, digital and analog in the context of an already hybridized understanding of teacher and student? (I'll ask this rhetorically for now. We can cycle back to this question at the end of our time together.)\n\nLet me share a couple examples of what this might look like.\n\nAfter hearing Cathy Davidson's keynote at Digital Pedagogy Lab's 2016 Institute this summer, in which she discussed co-creating a course syllabus with her students, I was inspired to do the same for my <a href=\"https://ds106.pushpullfork.com\" target=\"blank_\">Digital Storytelling</a> course in the fall. I knew it would be difficult to co-create the entire syllabus with my students, since it was my first time teaching the course, since I was teaching it alongside two other sections and in line with a well known tradition, and since it was fully online with many students who had never taken an online course before. So I decided to create a general framework for the course syllabus, and then undertake what I called the \"Syllabus Sprint\" during the first week of class. Here's how I presented it to my students:\n\n> Welcome to Digital Storytelling/DS106! This is *your* course. So while there are elements to the course that will be required in order to be consistent with the course description and general institutional requirements, a large part of the content, assignments, and assessment scheme will be determined collaboratively as a whole class. So this week, we'll largely be focused on a Syllabus Sprint \u2015 working together to fill in the gaps in the <a href=\"https://ds106.pushpullfork.com/Syllabus\" target=\"blank_\">course syllabus</a> to line up with the goals and interests that *you* bring to the course.<br>Following are assignments for the first week that will help us get through that process. Nothing is due until Friday, September 2, but if you want to have any significant impact on the direction the course will take, you'll want to dive in right away and contribute to the discussion from the beginning.\n\nI assigned readings that would help them thing about things like online community, assessment, self-assessment, and <a href=\"http://umw.domains\" target=\"blank_\">Domain of One's Own</a>, the platform that much of their work would be published on. Then I gave them this assignment:\n\n> ### Syllabus Sprint\n\n> *Simplicity is about subtracting the obvious and adding the meaningful. -John Maeda*\n\n> The following elements of the course are up for a collaborative decision this week:\n\n> **Course theme.** Take a look at what the syllabus says about the course theme. Then go to the #coursetheme channel on Slack to offer your ideas and respond to others.\n\n> **Assignment types.** Some assignment types are already determined and listed in the syllabus. Are there others that you would like to engage in? What kind of projects do you want to do? This will, of course, be determined in part by the course theme that we choose, and both the theme and the specific assignments will be determined by *what you want to get out of the course*. The syllabus links to some places to look for inspiration. Add your thoughts on this in the #assignments channel on Slack.\n\n> **Workload.** Under \"Assignments and types\" on the syllabus, a portion (albeit a significant one) of the traditional DS106 workload is listed. How much work should be added? How often should we do Daily Create assignments? What balance of reading/annotating/creating/writing is appropriate and in line with your goals for the course? How flexible should deadlines be? Add your thoughts on this in the #assignments channel on Slack.\n\n> **Assessment schema.** How will work be assessed? By whom? What will it be worth? How will grade information be communicated? I have a lot of thoughts about this, but I want to hear what you think will be best. Then I'll share my ideas, and we'll (hopefully) come to a consensus. Add your thoughts on this in the #assessment channel on Slack.\n\n> **Anything else?** What is missing? What are you uncomfortable with? Share any other ideas or thoughts you have about the syllabus and the course plan on the #miscsyllabus channel on Slack.\n\n> **These ideas should all be provided and discussed on Slack by Friday afternoon, September 2.** I'll mull things over on the weekend, especially if there are areas where we haven't reached complete consensus, make final decisions based on our discussions, and post the results to the syllabus on Monday, September 5.\n\nIn the end, we settled on a course theme, assessment schema, and assignment schedule that I wouldn't have come up with myself. In my opinion, the collaborative results were *better* than I could have come up with on my own. There is a healthy balance of different student interests in the course theme we came up with, there is a healthy balance of instructor guidance and student self-assessment in the grading system we settled on, and students were more open to request changes to the assignment schedule once things got underway, at least in part due to the ownership they felt about the syllabus they helped create.\n\nAnother example comes from my in-person course in Computational Music Analysis, a cross-listed course for music and computer science majors at the upper-division and master's level. The first third of the course was content-oriented, with the content decisions primarily decided by me. However, after that, the course shifted to a project-oriented setup. As my syllabus states:\n\n> Before the mid-point of the course, students and instructor will negotiate a collaborative research project that builds on existing work in the field of computational musicology, is sensitive to the methodological concerns raised in reference to the digital humanities in general, will provide students an opportunity to engage with the public, and will afford students sufficient opportunity to meet the individual course objectives.\n\nThis required a flexible assessment system that still gave the students some foothold so they had an idea of where their final grade would end up as they worked on their project:\n\n> Because the course is vertically integrated and interdisciplinary, assessment of student work will use <em>contract grading</em>, tailored to individual students\u2019 levels (undergraduate or graduate) and disciplines.<br>...<br>All students will propose a course contract soon after the collaborative research project has been decided. (Model contracts are in the class shared folder on Google Drive.) This contract will articulate the grade desired and layout a work plan that is appropriate for their interests, field, level (grad/undergrad), and desired grade. Once approved by the instructor, these contracts will bind students to the work laid out. However, amendments to the contracts, if necessary, can be requested in writing well in advance of the relevant course deadlines. Students who fail to meet the requirements of their contract will receive a C if core requirements are met, or a D or F, if core requirements are not met. Students who meet the requirements of their contract will receive the grade listed on the contract.\n\nThis was the most fun I've ever had in a class, and I've done it twice. It also received the highest student course evaluation ratings of all my courses. And the work the students produced was high-quality. In fact, several of them collaborated with me on a follow-up project, and we recently submitted an article to a scholarly journal as co-authors. (Oh, and everyone got an A.)\n\nThere are smaller, simpler ways to start, if you're reticent to hand the reigns over to your students in regard to major course decisions. For example, hybridize the textbook by <a href=\"https://digitalstudies.pushpullfork.com/Week-7\" target=\"blank_\">assigning Wikipedia</a> ... and having the students find errors or gaps in those Wikipedia articles and submit corrections to Wikipedia. Hybridize your lecture by <a href=\"http://flipcamp.org/engagingstudents/shafferintro.html\" target=\"blank_\">\"flipping\" the class</a> for a unit: make instructional readings and/or videos available online, and turn the in-class portion of your lecture class into a seminar or lab course for a day or two. Or simply replace essays written only for the instructor and presentations only meant for the class with <a href=\"http://umwdtlt.com/a-brief-history-of-domain-of-ones-own-part-1/\" target=\"blank_\">digital projects that live on the web</a> and are meant for a public audience. Not only is that a great opportunity for students to engage the public with their work, but the next time around, students can build on the work of the previous class, adding to an ever-growing resource that becomes more useful to the world with each course offering. I've done all of these things myself, as have many of my colleagues at Mary Washington. I'd be happy to discuss specific ideas in more detail later, for those who are interested.\n\nDuring graduate school, I took an course on musical hybridity with ethnomusicologist Sarah Weiss. In <a href=\"https://www.jstor.org/stable/pdf/20174587.pdf\" target=\"blank_\">an article</a> she wrote following that course, she concluded:\n\n> Hybridity is only a problem if one essentializes boundaries. But boundary fetishism, as Jan Pieterse calls it (2001:238-39), is something that humans seem to crave. Boundaries define what is inside through negative comparison with that which falls outside. But over time boundaries shift and seep, burst and reform with new contours, new exclusions and inclusions. If millions of lives have been lost in twentieth-century wars over boundaries \u2015 cultural and geographic \u2015 billions more have been enhanced and created over human time through historical mergings and exchanges. Thinking about hybridity and how we perceive it is important because it makes us think about boundaries, be they cultural or genre. It makes us question our expectations for and ideas about the nature of boundaries, in particular, what they mean both to us and to others. Mapping critical responses to hybridizations in whatever form they take tells us a lot about our own perspectives and those of others on those boundaries.\n\n> (\"Permeable Boundaries: Hybridity, Music, and the Reception of Robert Wilson's 'I La Galigo,'\" *Ethnomusicology* 52/2 (2008), p. 233.)\n\nThis statement sums up my view of hybrid pedagogy to a tee. Hybridizing our pedagogy \u2015 whether digital and analog, student and teacher, passive and experiential, institutional and independent, as long as it is done carefully and critically \u2015 has the potential to enhance and enrich lives. And critical reflection on hybridity \u2015 including both the intentional hybridization of pedagogical practices and the inherent hybridity of received pedagogical objects \u2015 helps us think about the boundaries we have set around education, and question our expectations for and ideas about those boundaries. Enriching lives and better understanding the world we live in from a diversity of perspectives, that's liberal education at its best.\n\nSo let's move beyond the instrumental discussion of using blended learning to bring scale up and bring cost down. Let's not get stuck in a ballpoint pen moment, where we use a new tool to teach old content without reflecting on the what and the why. Instead, let's take the opportunity to reflect deeply on the fraught and fragile nature of the educational traditions we have received and the novelties we are encountering, and lets work together to transform the world and empower our students to do the same.\n\n*All images used in this talk, unless otherwise stated, are in the public domain.*\n",
                        "html": "",
                        "image": "/content/images/school-desk-chalkboard-inkwell.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "From blended learning to hybrid pedagogy",
                        "meta_description": "How do we design a blended learning environment for the liberal arts? The philosophy of hybrid pedagogy can help.",
                        "author_id": 1,
                        "created_at": "2016-11-18 13:00:00 -0500",
                        "created_by": 1,
                        "updated_at": "2016-11-18 13:00:00 -0500",
                        "updated_by": 1,
                        "published_at": "2016-11-18 13:00:00 -0500",
                        "published_by": 1,
                        "og_title": "From blended learning to hybrid pedagogy",
                        "twitter_title": "From blended learning to hybrid pedagogy",
                        "og_description": "How do we design a blended learning environment for the liberal arts? The philosophy of hybrid pedagogy can help.",
                        "twitter_description": "How do we design a blended learning environment for the liberal arts? The philosophy of hybrid pedagogy can help.",
                        "og_image": "/content/images/school-desk-chalkboard-inkwell.jpg",
                        "twitter_image": "/content/images/school-desk-chalkboard-inkwell.jpg"
                    },
                    {
                        "id": 0,
                        "title": "The digital and the sacred",
                        "slug": "the-digital-and-the-sacred",
                        "markdown": "\nToday at UMW, <a href=\"https://twitter.com/katemfd\" target=\"_blank\">Kate Bowles</a> recounted an experience in a cemetery with her daughter. She was inadvertently standing on a grave, and her daughter told her not to \"stand on him.\" To Kate, this was a reminder of the way in which we relate to people in history through our activity in the present, and she connected it to the way we relate to each other online. We find artifacts \u2015 photos, videos, writings \u2015 made by humans, but we engage them in their absence. In some cases, this disembodied experience causes us to forget the relational nature of that encounter. We need to be reminded that though mediated by digital technology (or the passage of time), our activity in the world is relational, connecting us to people in other places and times. We need to be reminded to respect and care for each other.\n\nAs I reflected on this, I was reminded of times when I tend to *avoid* the digital: family dinner and worship. There are other activities where it is impossible or impractical to \"go\" online (a phrase <a href=\"https://twitter.com/slamteacher\" target=\"_blank\">Sean Michael Morris</a> problematized during the same panel discussion). And there are times when I am so invested or interested in what I am doing that I don't think (or want) to \"check my webs\", as my wife and I often say. When a ding from my phone is an unwelcome nuisance.\n\nBut family dinner and worship at church are times that I, or my family in general, have decided we will not pull out our digital devices and go online, even if we feel the urge. (I will sometimes look up something said during a sermon on my phone, or if traveling I might use my tablet or phone to read Scripture instead of packing my Bible. But even then, I don't \"live tweet\" the sermon. And it feels a little weird.)\n\nReflecting on these internet-free zones in my life, and connecting them to Kate's cemetery story, I realized a common theme: *the sacred.* Whether devoutly religious, superstitious, or simply respectful of people in mourning, most people I know treat cemeteries as special places, worthy of solemnity and respect. In a word, *holy* (set apart for a special purpose). The same is true of a worship service, at least in my tradition.\n\nBut what about family dinner?\n\nWe've decided that it is important to be *fully present* with our kids, not to divide our attention. I've been in the restaurant on the weekend and walked past the child earnestly seeking the attention of the parent across the table, face buried in a \"smart\" phone. We don't want that for our family. We want our kids to know that they have our unconditional love, and that they can have our full attention *just because they're our kids*. Not because they're more interesting than some internet meme. Or because they do something to wrest our attention away from the device. And the same goes for our marriage.\n\nBut it's more than that. Our kids don't bring books to the table, we don't watch videos while we eat (except on the rarest of occasions, and even then it's probably just dessert), and we leave the phones and computers away. We do what we can to keep from overlapping schedules so that we *can* have family dinner on most nights. To us, there is something sacred, something holy about family dinner.\n\nThis perhaps partly comes from our religion. The family plays an important role in Presbyterian life, and holds a significant place in Reformed covenant theology. Fellowship around a meal is also a major focal point of (Orthodox) Presbyterian church life outside of the worship service. (Perhaps intentionally drawn from the models of fellowship in the Early Church, centered around the \"breaking of bread and prayers\" \u2015 both sacramentally and socially.)\n\nBut it's also just the kind of family we want to raise: one that can be fully present with each other, physically, emotionally, and attentively. Corporately driven online media is designed to pull at our attention and emotion. It makes claims on parts of ourselves we want to reserve for our family. There's a resistance needed, if we're to have the kind of control of our attention and relationships that we want.\n\nChristian philosopher <a href=\"https://openlibrary.org/books/OL23186199M/Desiring_the_kingdom\"  target=\"_blank\">James K. A. Smith writes</a> about the mall as a \"liturgy\" that shapes our desires and the way we think and interact with the world, a liturgy that needs to be countered in worship and educational settings. I think most of the media we consume online, and certainly corporate social media, is part of that \"mall\". It must be countered with another liturgy if we are to be able to resist its pull.\n\nAnd that's why I think that I, and likely many others, cordon off part of my life as media-free, offline spaces. And I think that's why those spaces tend to be things that hold the status of the sacred in some way. I feel, both consciously and unconsciously, a need to resist certain elements of digital, \"social\" media. I need a place, a *liturgy*, in which to reorient myself attentively, emotionally, and relationally.\n\nBut I'm still missing a piece of the puzzle here. I don't pray or eat dinner with my family just to keep myself in healthy media consumption practices. The sacred was already there. And as I hinted above, when I do pierce the veil and bring the digital in to some of these spaces, it feels wrong. Or at least odd. Why is that? Is there something inherently profane about the digital? Or is it just manufactured social pressures that I've internalized over the years? After all, I grew up in churches that would *never* use an electric guitar.\n\nAnyway, I don't have an answer here. Just connections my mind started to make as I heard Kate's story in the context of a discussion about <a href=\"http://umwdtlt.com/ethical-online-learning-a-town-hall/\"  target=\"_blank\">ethical online learning</a>. I'll keep thinking. And meditating.\n",
                        "html": "",
                        "image": "/content/images/mountains.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "The digital and the sacred",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-09-27 14:41:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-09-27 14:41:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-09-27 14:41:00 -0400",
                        "published_by": 1,
                        "og_title": "The digital and the sacred",
                        "twitter_title": "The digital and the sacred",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/mountains.jpg",
                        "twitter_image": "/content/images/mountains.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Data mining whitehouse.gov",
                        "slug": "data-mining-whitehouse-gov",
                        "markdown": "\nWhen Donald Trump became president of the United States, a lot disappeared from the administration website, <a href=\"https://whitehouse.gov\" target=\"_blank\">whitehouse.gov</a>. This is standard practice for a new administration, and the Obama administration's website is still available, both at <a href=\"https://obamawhitehouse.archives.gov/\" target=\"_blank\">obamawhitehouse.archives.gov</a> and in various snapshots in time via the <a href=\"https://archive.org/web/web.php\" target=\"_blank\">Internet Archive Wayback Machine</a>. Nevertheless, there are some significant changes and omissions, and certainly more to come. So I've decided to spend some time finding and tracking the changes on the Trump administration's official website.\n\nI'm planning on investigating differences between the public statements of the Trump and Obama administrations, as well as tracking emerging changes in language, priorities, and policies over the next four years. All my code and the (pre-processed) data that I download will be made publicly available in the project's <a href=\"https://github.com/kshaffer/whitehouse\" target=\"_blank\">GitHub repository</a>. For those who want to use the code or download the data to do your own analysis, you can find detailed instructions there. I welcome contributions that improve the code or refine the web scraping and data-mining process.\n\nI'm still early in this process, and the Trump administration is still early in the process of building out the site, but here are some high-level findings so far.\n\n## The scope\n\nThe Trump administration has a much leaner whitehouse.gov so far. I scraped and downloaded the entirety of whitehouse.gov from January 20 (before the transition), January 25, and again on January 31. The final version of the Obama administration's whitehouse.gov on January 20 constituted **411 MB** of data. Trump's whitehouse.gov on January 25 had just **44 MB** and on January 31 was up to **51 MB.** That's a pretty significant drop in content!\n\nAnd lest we assume that Obama's website contained a large amount of media that puffed up that total, here is the amount of data contained just in the <a href=\"https://github.com/kshaffer/whitehouse/tree/master/data\" target=\"_blank\">text extracted from the HTML files</a> on the site:\n\n| date | text data |\n| --: | :-- |\n| January 20 | 28.0 MB |\n| January 25 | 1.3 MB |\n| January 31 | 1.7 MB |\n\nThe drop in text content is even more precipitous. (And it's even greater when you consider that a significant portion of that 1.3 MB involves pages that are on Obama's site, too \u2015 boilerplate administrative pages, biographies of former presidents and first ladies, etc.)\n\nSo my first conclusion from comparing Obama's and Trump's websites is that the information on Trump's website is extremely sparse. Some of this is due to the early stage in which the administration finds itself. But not all.\n\n## What pages have appeared and disappeared?\n\n117 pages that existed on the Obama administration's site remain on the Trump administration's site. The bulk of <a href=\"https://github.com/kshaffer/whitehouse/tree/master/diffs\" target=\"_blank\">these pages</a> are biographies of former presidents and first ladies, with some general administrative boilerplate pages (such as \"The Executive Branch\" or \"The Constitution\").\n\n124 pages are new with the Trump administration. <a href=\"https://github.com/kshaffer/whitehouse/blob/master/diffs/pages_new_with_trump.csv\" target=\"_blank\">These pages</a> primarily include executive orders, transcripts of phone calls with foreign leaders, press briefings, and other timely announcements. (<a href=\"https://github.com/kshaffer/whitehouse/blob/master/diffs/pages_new_or_deleted_on_Jan31.csv\" target=\"_blank\">59 of these pages were added between January 25 and January 31</a>.)\n\nThere are <a href=\"https://github.com/kshaffer/whitehouse/blob/master/diffs/pages_unique_to_obama.csv\" target=\"_blank\">2583 pages</a> that are unique to the Obama administration's site. I haven't looked closely enough to see if any of these are replaced by new pages on the site with different titles, but it's clear from the difference between the size of this list and the 124 pages unique to Trump's site, that much has been removed. A lot of what was deleted involves timely updates from Obama's tenure and makes sense to be moved to an archive site, rather than remaining on whitehouse.gov. However, the deletion of pages referring to the signup deadlines for the Affordable Care Act, pages that summarize and link to federally funded research on climate change, etc. is a significant move from the administration. Time will tell what will be replaced, and in what format. But for now, the omission of some of these specific pages from whitehouse.gov is important to note. And if it's your field, to respond to.\n\n## The Judicial Branch\n\nOne page that many people noted missing from whitehouse.gov a few days ago was the page for the Judicial Branch. While The Executive Branch, The Legislative Branch, and (perhaps ironically) The Constitution still retained their pages on the Trump administration's site, the page about the judiciary and the Supreme Court was a notable absence. It is <a href=\"https://github.com/kshaffer/whitehouse/blob/master/diffs/pages_new_or_deleted_on_Jan31.csv\" target=\"_blank\">now back up</a> and contains the same text content, but was absent for several days. Under other circumstances, I would just assume an honest mistake during the process of rebuilding the site from scratch. But given the current disputes between the Executive Branch (including agencies like DHS and CBP) and the federal judiciary, the omission is at the very least more problematic, perhaps even ominous. Again, it's back up now. But these kinds of deletions and omissions are the kinds of things that I hope to track through this project.\n\n## Going forward\n\nI've done some preliminary text mining and content comparison between Trump's and Obama's site, but given how little data is on Trump's site relative to Obama's, there's not much to find there yet. However, as the site gets built up, I'm going to be diving deeper into the specific content of the new administration's site and comparing it to both Obama's site and previous versions of Trump's site. Hopefully this study can reveal emerging trends that might otherwise get missed, and can help us know how best to respond to those trends and any problematic (or missing) language on the site.\n\nIf you'd like to join in on this project, visit the <a href=\"https://github.com/kshaffer/whitehouse\" target=\"_blank\">GitHub repository</a> and play around with the code, the data, or even just read the text and see what stands out. If you notice anything, please let me know. And feel free to send a pull request or reach out if you want to collaborate more formally.\n\nI'm hoping that this and some other projects I'm working on (looking at you, Steve Bannon...) can help shed light into the darkness by opening, curating, and analyzing data about the Trump administration and its activities, and providing code and methods for those looking to dive into that data.\n\nGood night, and good luck.\n\n<i>Header image by <a href=\"https://unsplash.com/photos/X2CxUXFqKcM\" target=\"_blank\">Chris Brignola</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/columns.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Data mining whitehouse.gov",
                        "meta_description": "What has the Trump administration changed on whitehouse.gov?",
                        "author_id": 1,
                        "created_at": "2017-02-01 11:25:00 -0500",
                        "created_by": 1,
                        "updated_at": "2017-02-01 11:25:00 -0500",
                        "updated_by": 1,
                        "published_at": "2017-02-01 11:25:00 -0500",
                        "published_by": 1,
                        "og_title": "Data mining whitehouse.gov",
                        "twitter_title": "Data mining whitehouse.gov",
                        "og_description": "What has the Trump administration changed on whitehouse.gov?",
                        "twitter_description": "What has the Trump administration changed on whitehouse.gov?",
                        "og_image": "/content/images/columns.jpg",
                        "twitter_image": "/content/images/columns.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Getting started with GitHub",
                        "slug": "getting-started-with-github",
                        "markdown": "\nThere has been a lot of talk about GitHub at [Digital Pedagogy Lab](http://digitalpedagogylab.com) this week. Following are a few resources for learning and using GitHub for things like collaborative corpus-based projects, website publishing, etc.\n\n> Scholarship is, by its nature, open source.\n\nFirst is my *Hybrid Pedagogy* article, [\"Open-source scholarship\"](http://www.hybridpedagogy.com/journal/open-source-scholarship/). In this article, I argue that open-source is the fundamental nature of \"traditional\" scholarship.\n\nI followed up with [\"Push, Pull, Fork: GitHub for Academics\"](http://www.hybridpedagogy.com/journal/push-pull-fork-github-for-academics/), a guide for using GitHub, primarily with a view towards creating, editing, and maintaining open educational resources (OERs) like open-source textbooks.\n\nFinally, I created two nuts-and-bolts videos for using git and GitHub on the Mac:\n\n<iframe src=\"https://player.vimeo.com/video/133381951\" width=\"680\" height=\"382\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> <p><a href=\"https://vimeo.com/133381951\">GitHub for Mac tutorial</a> from <a href=\"https://vimeo.com/user11692346\">Kris Shaffer</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>\n\n<iframe src=\"https://player.vimeo.com/video/133382586\" width=\"680\" height=\"383\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> <p><a href=\"https://vimeo.com/133382586\">GitHub at the Mac Terminal</a> from <a href=\"https://vimeo.com/user11692346\">Kris Shaffer</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>\n\nEnjoy!\n",
                        "html": "",
                        "image": "/content/images/github.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Getting started with GitHub",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-08-12 15:35:00 -0600",
                        "created_by": 1,
                        "updated_at": "2015-08-12 15:35:00 -0600",
                        "updated_by": 1,
                        "published_at": "2015-08-12 15:35:00 -0600",
                        "published_by": 1,
                        "og_title": "Getting started with GitHub",
                        "twitter_title": "Getting started with GitHub",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/github.jpg",
                        "twitter_image": "/content/images/github.jpg"
                    },
                    {
                        "id": 0,
                        "title": "More humanity in the humanities",
                        "slug": "more-humanity-in-the-humanities",
                        "markdown": "\nWe need more humanity in the humanities.\n\nI recently attended a conference at which the teaching of undergraduate music students was a core focus. I was heartened by the many voices who advocated for building communities of trust, and for empowering students to be mindful agents. But I was also disheartened by the many who seem to view their students as the objects of their teaching, rather than subjects who act in their own right. As immature \"kids\" who need to be spoon-fed content, and watched like a hawk come assessment time. As ignorant beings worthy of derision, rather than adults with a diversity of brilliances\u2014all of which could be harnessed in service of the class's intellectual and musical growth.\n\nIn a word, these attitudes are dehumanizing. And these attitudes are all too common. Most of our students have experienced this dehumanization in some form. It should come as no surprise, then, when some of our students enter our classes predisposed to anxiety, suspicious of our aims as teachers, and with defenses in place. When that is the case, I believe our job as teachers is to *rehumanize*\u2014to restore the agency of our students, to equip and empower them to use it wisely, to awaken them to awareness of this power, and to help them feel society's need for more people to wield it well.\n\nFor some time now, I've seen this (re)humanization project as the proper work of the critical pedagogue. But I'd like to offer that it is the proper work of every teacher, particularly those who dare call themselves humanists.\n\nFor ages, we \"humanists\" have studied the artifacts of humans long departed. And we have done so primarily in scholarly isolation. In that sense, I believe that humanists, and the humanities, are inappropriately named. Those whose primary interest is the artifacts of dead humans should be called \"artifactists.\" The word \"humanist\" should be reserved for the one who sees the humanization of others as their primary goal.\n\nIn this sense, the humanist focuses on the development of students' intellect more than the presentation of content. The humanist studies the abilities, knowledge, and passions of their students more than the activities of dead celebrities. The humanist helps students to develop their own social technologies rather than requiring them to use pre-determined digital or paper technologies. The humanist cares more about the legacy of the humans in front of them than the legacy of the humans behind them.\n\nI'm sure my proposed redefinition of \"humanist\" will not gain much traction. Maybe none at all. But I hope that in whatever context we teach\u2014the arts, the sciences, the \"humanities\"; the academy, the faith community, the family, the soccer pitch\u2014we can take our minds off of the content for a time, discover the brilliance of those around us, help them cultivate it, and send them off to use it to change the world.\n",
                        "html": "",
                        "image": "/content/images/humanity.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "More humanity in the humanities",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2014-11-15 21:49:36 -0700",
                        "created_by": 1,
                        "updated_at": "2014-11-15 21:49:36 -0700",
                        "updated_by": 1,
                        "published_at": "2014-11-15 21:49:36 -0700",
                        "published_by": 1,
                        "og_title": "More humanity in the humanities",
                        "twitter_title": "More humanity in the humanities",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/humanity.jpg",
                        "twitter_image": "/content/images/humanity.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Does classical music make you smart?",
                        "slug": "does-classical-music-make-you-smart",
                        "markdown": "\n<p style=\"font-size: 0.8em; line-height: 1.25em\"><i>In response to a question from a music cognition student, I dug up this old post from \"Sound a Mind,\" a blog on music cognition that Roger Grant and I maintained during graduate school. Original posting date: Sept. 21, 2006.</i></p>\n\nA new study, soon to be published by *Brain* claims to have found a neurological connection between musical training in childhood and brain development. This study tested a number of children, some of whom took the first year of Suzuki violin training, and some of whom did not. The researchers found differing development in the brains of the children, with the Suzuki students seeing increased development in neural resources for processing sound, particularly noticeable in response to sounds of violin timbre (they developed relatively normally in their processing of &#8216;noise&#8217;). The researchers also found greater increase of working memory digit span amongst the Suzuki students than amongst those who did not receive musical training.\n\n> While improvement in musical tasks is not surprising after 1 year of Suzuki music lessons, enhanced performance in digit span gives additional evidence for transfer effect between music and non-musical abilities such as literacy (Anvari et al., 2002), verbal memory (Ho et al., 2003), visuospatial processing (Costa-Giomi, 1999), mathematics (Cheek and Smith, 1999) and IQ (Schellenberg, 2004).\n\nTheir explanation of this phenomenon, however, should dissapoint those who are desperately looking for scientific evidence that &#8220;music makes you smart&#8221;:\n\n> While it appears that music practice might have impacts on \u2018general intelligence\u2019, the underlying neural mechanism remains unclear. Our finding of enhanced performance after a year of musical training on the digit span task suggests that this experience affects working memory capacity, at least to some extent. **Alternatively, it might be also related to the more advanced perseverance skills and/or the ability to sustain focused attention observed in Suzuki children compared with children enrolled in other activity such as creative movement classes (Scott, 1992).** This interpretation is also consistent with the enhanced changes on N250m component in the Suzuki group, which could be related to attentional modulation. [emphasis added]\n\nGiven the limited &#8220;musical&#8221; nature of the first year of Suzuki training (repeated mimicry of a passage of music presented aurally \u2014 that is, rote memorization), it seems to me that all this study demonstrates is that children who copy something in their environment, repeatedly and from memory, develop memory skills and the ability to process the kind of stimulus to which they continually focus their attention. Granted, musical study is a simple and often enjoyable way for children to focus on developing these skills. But this can&#8217;t be limited to rote memorization of music, particularly when the researchers admit that this study may just demonstrate &#8220;the more advanced perseverance skills and/or the ability to sustain focused attention observed in Suzuki children&#8221; (and possibly the effect of parents who encourage/force their children to do so), not anything particularly magical about the musical experience. And of course, it should go without saying that this study says nothing of the effect that listening to &#8220;classical&#8221; music while studying/working or participating in a school band/choir/orchestra has on adolescents and adults, though some in the news media surely will atempt to make that link.\n\nHowever, I am happy that researchers are attempting to address the often made claim that &#8220;music makes you smart&#8221; in a controlled environment. I&#8217;m afraid that the control they require may elude them in the long run, but this approach may yield some valuable fruit. I am interested to see how further evidence in this direction may pan out as there are many interesting ways in which this research can further develop.\n\nWhat do you think of this study?\n\n<hr />\n\n<p style=\"font-size: 0.8em; line-height: 1.25em\">If you would like to read the article, you can find the abstract and the full text <a href=\"http://brain.oxfordjournals.org/cgi/content/abstract/awl247v1\">here</a> (you may need to access through your university&#8217;s proxy server for full text access). If the article is too technical a read (it was for me), ScienceBlog.com has a good <a href=\"http://www.scienceblog.com/cms/first-evidence-musical-training-affects-developing-brain-11528.html\">synopsis</a> of the study as well. (The synopsis has since been removed.)</p>\n",
                        "html": "",
                        "image": "/content/images/trainOfThought.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Does classical music make you smart?",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-06-01 09:36:45 -0600",
                        "created_by": 1,
                        "updated_at": "2015-06-01 09:36:45 -0600",
                        "updated_by": 1,
                        "published_at": "2015-06-01 09:36:45 -0600",
                        "published_by": 1,
                        "og_title": "Does classical music make you smart?",
                        "twitter_title": "Does classical music make you smart?",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/trainOfThought.jpg",
                        "twitter_image": "/content/images/trainOfThought.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Why grade?",
                        "slug": "why-grade",
                        "markdown": "\n![]({{ root_url }}/media/grading.jpg)\n\nI've been thinking about grading a lot lately. Not only am I doing a lot of it\u2014way more than I would like\u2014and supervising grad students who are doing it, but I've been following various debates about Common Core, standardized tests, and the like online. Assessing students is a core part of what I do as a professor, and as I have modified the way I assess over the years, I have seen both positive and negative effects of different ways it can be done, both in my assessments, and in students' attitudes and expectations surrounding assessment based on their past experiences. Poor assessment methods can do real harm to students\u2014financially, as it affects scholarships and post-college prospects, and psychologically, as it shapes their attitudes towards learning and towards specific class activities. As much as we who teach may want to simply blow it off, or do it quickly and think about it as little as possible (or pawn it off on a grad student without much consideration), we need to be careful how we assess if we care about our students' academic success and their general well-being.\n\nWith all that in mind, how can we assess students' learning in a way that is helpful and not harmful? Do we need to assess them at all? And do letter grades have anything to do with meaningful assessment? Those are some of the questions I've been thinking through for some time, and with a bit more urgency lately for whatever reason. Below I offer my insights as they currently stand. I'd love to hear more from others who have thought about these things, as well.\n\n# Assessment's purpose\n\n> Assessment and standards are elephants in almost every room where discussions of education are underway.  \n> Jesse Stommel, [\"MMDU: 'I Would Prefer Not To.'\u201d](http://www.hybridpedagogy.com/page-two/mmdu-prefer/)\n\nBefore addressing any questions about assessment, I should clarify my purpose in assessment. No system of academic assessment is intrinsically good, only good for a purpose. That purpose must be established first.\n\nI have no interest in sorting students into categories of [*optimi*, *second optimi*, *inferiores*, and *pejores*. ](http://books.google.com/books?id=R1QH0db2xGsC&pg=PA4&lpg=PA4&dq=optimi+inferiores+pejores+yale&source=bl&ots=hEtfl96o06&sig=lIzK9FIjVjp8W4cutgQ7puZ5Ezs&hl=en&sa=X&ei=hFO8U5nMCoGmyASR8IL4Dg&ved=0CB4Q6AEwAA) Assessment decisions (when and how to assess) serve pedagogical aims.\n\nThat said, my purpose as an educator is largely *not* to instill content knowledge. [My goal is for my students to learn how to learn](http://www.hybridpedagogy.com/journal/open-letter-students), and to gain skills that will allow them to continue learning independently once the course is over. Those skills will certainly require content knowledge or, at least, familiarity, but content is important primarily to the extent that it supports students' intellectual growth within the discipline.\n\nIn line with my goal of fostering self-direction and learning how to learn, assessment serves three purposes: 1) assessment determines if a student has sufficiently mastered a concept in order to apply it and build on it in subsequent coursework or professional activity; if not, 2) assessment identifies areas that need correction and provides feedback for improvement; finally 3) *my* assessment guides students to assess *themselves* better, so they can better direct their own learning and work.\n\nNote that purpose 1 aligns somewhat with what educators call *summative assessment* (assessing a student's overall, or sum, knowledge/skills at the end of an assignment, unit, or course) and somewhat with what educators call *formative assessment* (assessment that helps form or shape, or simply motivate, plans for future learning activities). While purpose 1 involves summative assessment, I find that summative assessment only has pedagogical value to the extent that it can be used as formative assessment, and so purpose 1, for me, is primarily focused on formative assessment. Purpose 2 also aligns with formative assessment. Purpose 3 is more-or-less unique to the pedagogical goal of fostering self-direction and self-teaching.\n\n# What can letter grades do? #\n\nLetter grades do an absolutely horrible job of all three of these things.\n\nConsider summative assessment (purpose 1). It boils down to one simple problem: *human knowledge is too complex to represent with a single letter/percentage/score*. This is most obvious when considering an average grade for a 15-week course that covers dozens of concepts, topics, skills, exemplars. No single letter can represent in any meaningful way what each student knows and can do at the end of that course\u2014let alone what they know and can do years later when a graduate school or prospective employer looks at their transcript! This is also true or a unit of study within a course, and even many homework assignments, where multiple concepts or skills are involved in the task assigned.\n\nThere is a second significant problem with using letter or percentage grades for summative assessment. They are artificial and they distract many students and instructors from the learning process and the content focus of the course. In light of purpose 1 (preparedness to progress to the next unit/course in a sequence), a five-point letter system (or 15-point, with pluses and minuses) or a 100-point percentage system (or 1000- or 10000-point, with decimals) is overdetermined. Preparedness is mainly a binary decision (yes/no), with a third possible \"borderline\" category, in order to differentiate students who need to retake the course in order to be prepared to succeed later from students who simply need some corrective attention before the beginning of the next semester, for example. Using an overdetermined scale pressures the instructor to rank students unnecessarily. This adds an unhealthy social dynamic to the class, and it shifts the focus of assessment from feedback on progress to reward for a job well (or better) done. We are not employers and students our employees; grades are not wages. Likewise, as instructors our job is to teach, not to rank. Proliferating possible assessment results beyond what is pedagogically necessary only adds to our workload tasks and stress that are irrelevant, and at times distracting or hindering, to our pedagogy.\n\nWhen it comes to formative assessment, letters and percentages fall on all the same points: rarely does student progress in a particular domain fall along a linear progression of 5, 15, 100, 1000, or 10000 stages. Whether or not a student has met a precisely defined learning objective is usually a binary decision (yes or not yet). But the student work that demonstrates that mastery could come in a number of different modes (in line with their background, interests, and goals), and the work along the way is complex and individualized even for students working on the same tasks. Most importantly, though, when a student has not yet achieved an objective, that student needs specific, tailored, verbal feedback\u2014not numbers or letters\u2014in order to better guide their studies.\n\nLikewise, in light of purpose 3 (helping students self-assess and self-direct their learning), letter grades are unhelpful, and percentages erroneous. What is a 76%-good self-assessment? And assuming A-level self-assessment is the goal, how does providing a student with a B\u2013 on a self-assessment help them improve? (This is a question we should ask about every percentage or letter grade was assign.) In all kinds of formative assessment, ranked grades are ancillary\u2014and potentially distracting or hindering\u2014to the feedback we must provide our students. This is especially true when the goal is an ability to self-assess.\n\nFurther, as long as the instructor has the final word on *grades*, we cannot expect our students to take us seriously when we try to impress upon them the importance and value of self-assessment. Unless their self-assessments have power\u2014either to shape future learning activities, or to change the gradebook\u2014they will not be true self-assessments.\n\n# Assessment strategies #\n\nAll that said about grades, the *assessment* of student progress is essential to guiding their learning, especially when students are still on the path towards becoming self-educators who can assess their own skills appropriately. My *ultimate* goal is to give students no top-down assessment, and certainly no top-down grades. But I've found that I need to teach my students to assess themselves just as much as I need to teach them to perform music at sight or analyze sonatas. I can't just throw them in the deep end immediately, as if I were teaching a class at Hogwarts. (I learned this by experience.) Rather, I must include in my classes pedagogical instruction to these would-be self-teachers, in order to help them learn how to self-assess.\n\n<iframe width=\"420\" height=\"315\" src=\"http://www.youtube.com/embed/nAQBzjE-kvI\" frameborder=\"0\" allowfullscreen></iframe>\n\nIn my teaching, this learning progression often begins with some form of criterion-referenced grading (also called standards-based grading or SBG). [I have implemented this in various ways over the past few years](http://kris.shaffermusic.com/tags/#criterion-referenced%20grading), and it is an excellent first move away from industrial grading practices. By providing students grades regularly, SBG speaks a familiar language (if a different dialect) while helping direct students' attention away from points towards content objectives. Even grade-grubbers are forced to be skill-grubbers, which is a significant improvement in the instructor-student social contract, and starts students on the road to self-assessment in light of those content objectives.\n\nSBG is only a starting point, though, not a satisfactory end in light of my pedagogical goals. Thus, within a course\u2014or even better, a course sequence that I (and other like-minded instructors) oversee\u2014I try to increase student freedom gradually. While a syllabus for Music Theory I may contain a long list of very specific objectives (such as recognizing scales and key signatures quickly, under pressure), Music Theory IV would likely contain a shorter list of broader objectives that provide students with a less detailed rubric and a wider variety of tasks or projects that can demonstrate high-level critical thinking in light of those content areas. This can look a lot like [contract grading,](http://www.hastac.org/blogs/cathy-davidson/how-crowdsource-grading) where students propose a (series of) project(s) that will demonstrate mastery of the course's core content areas, and instructor and student negotiate the contracts in advance of the project work. Again, this is not an end-point, but another stage of the self-assessment learning process, containing a balance between having the student generate a picture of what mastery looks like and providing the student with a clear, instructor-approved rubric in advance of their high-stakes work.\n\nThe ultimate goal, though, is not to find the perfect assessment system. The goal is to enable students to think critically about both the content *and* their intellectual development in light of that content. When that happens, I won't be grading at all. While we may not reach that destination before my students leave my class, it is my job to see that they make significant headway in that direction while we're together.\n",
                        "html": "",
                        "image": "",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Why grade?",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2014-07-08 12:21",
                        "created_by": 1,
                        "updated_at": "2014-07-08 12:21",
                        "updated_by": 1,
                        "published_at": "2014-07-08 12:21",
                        "published_by": 1,
                        "og_title": "Why grade?",
                        "twitter_title": "Why grade?",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "",
                        "twitter_image": ""
                    },
                    {
                        "id": 0,
                        "title": "Flipping an online class",
                        "slug": "flipping-an-online-class",
                        "markdown": "\nIs it possible to \"flip\" an online class?\n\nAs most people define the inverted class, no. If the inverted class involves students watching video (micro)lectures at home and then doing active work in the classroom side-by-side, then no, you cannot flip an online course. For one, providing students with video material is standard for an online class, not flipped. More importantly, there is no physical classroom in which most students are present simultaneously, along with the instructor.\n\nOf course, [as Bryn Hughes and I have argued](http://flipcamp.org/engagingstudents/shafferintro.html), the inverted class is bigger than the \"basic flip\" \u2014 video at home, \"homework\" in class. But I'm not actually interested in reproducing the *techniques* of inverted pedagogy in an online course. What I'm asking is whether or not we can tap into the specific *advantages* of inverted pedagogy in an online course.\n\nOne advantage of inverted pedagogy is that *course content takes a back seat to student activity*. As I write in [the introduction to *Open Music Theory*](http://openmusictheory.com/about.html),\n\n> In our classes, student activity takes pride of place, and it often *precedes* engagement with the textbook. The information contained in this text is secondary to that activity, and thus this text is meant to play only a supporting role in our classes.\n\nCourse content and instructor activity has a tendency to grab the focus of a lecture class, but this risk is even greater in an online course, where students engage the course (content) in physical isolation from each other. *Can an online course background the content and turn more of the focus onto student activity?*\n\nAnother advantage of inverted pedagogy is that *much of the learning that happens is emergent*. In a flipped course, the most interesting work takes place during class time. Because of that, it is easy to resist the temptation to grade that work. Many instructors grade homework because they think it is necessary to get many of their students to do it. I disagree. But when we're all in the room together, there's absolutely no problem ensuring that pretty much everyone engages the activity. Not assessing means that the task can be more open-ended, and no rigid rubric is needed. (Again, I disagree that a rigid rubric would ever be *needed*, but in this context, the possibility of one doesn't even come up.) When the task is ungraded, students are more likely to recognize the intrinsic value of the task, and when the task is open-ended (and lacking the pressure of a grade at the end), a lot of different kinds of learning happen freely. Instead of following a checklist or self-oppressing their learning by submitting to an assessment rubric, students freely take different approaches to a shared goal, compare methods and results with each other, argue with each other, argue with me...all of which can lead to some quite wonderful epiphanies.\n\nI'm not saying that I don't have specific goals in mind with these tasks. Sometimes I have rather specific learning objectives in mind, such as students developing the ability to calculate the prime form of a pitch-class set in a piece of atonal music. In such a case, student epiphanies may be incidental to the task, or different students may have different epiphanies as they each try out different methods and compare results with each other, finding the method(s) that are most reliable for them. In other cases, I have more general learning objectives in mind. For example, sometimes I have students attempt to analyze a musical passage they are only partly prepared for. The \"learning objective\" is to discover which of the skills or concepts they already understand are applicable to this new context, and to grow in their ability to ask specific, appropriate questions about what they *don't* know. Different student strengths and weaknesses will lead to different answers to the first \"objective,\" and the questions they formulate in the second will guide them to the course \"content\" in order to find at least the beginnings of answers to these questions. In both of these cases, by presenting students with a piece of music or a goal that is *no more specific than it needs to be*, rather than a checklist or a rubric, there is a lot of room for messy, emergent, different-for-each-student learning to take place.\n\nIs this messy, emergent, differentiated learning possible in an online course?\n\nYes, but...\n\nAs I'm putting together [my first fully online course](cognition.shaffermusic.com), I'm running into some assumptions about how in-person pedagogy transfers into an online learning environment. (Note: these are broad assumptions I'm engaging here, not those of any individual. In fact, I'm really digging the conversations that I'm having with the instructional designer who is helping me with my course.) One problematic assumption is that because the unscripted, emergent learning in an in-person course mostly takes place during class discussion, then the same unplanned epiphanies will likely take place in the discussion *forum* of an online course. Designing for emergent learning, then, means requiring student participation in the forum. And requiring participation means grading it, which means providing a \"clear\" rubric, yada yada...which ends up stifling the very emergence that is sought. (Again, I disagree with the logic of just about all of the assumptions in this chain, but this is a standard progression of thought.)\n\nBut what if the discussion forum isn't the place where emergent learning is most likely to happen?\n\nPerhaps more to the point, what if providing students with a checklist for forum activity isn't the best way to demonstrate its intrinsic value and encourage messy, emergent learning?\n\nWith these questions in mind, I've come to the following positions (I'm not going to call them conclusions before the course starts!):\n\n- Checklists are best at telling people when they are done. Therefore, I'm not going to provide a checklist to students for the most open and flexible piece of technology in our course, the discussion forum.  \n- Discussion forum participation is a means, not an end. I want to direct students to meaningful goals, and then guide them through ways that the discussion forum (and other tools) might help them reach those goals.  \n- Building skills of collaboration *is* a meaningful goal, but that involves much more than discussion forum participation.  \n- Sitting with one's own thoughts, listening to the thoughts of others, and collaborating in private are all valid study practices.  \n- Discussion forum participation is not the new \"seat time.\"  \n- I need to think more broadly about which aspects of my course will be [designed for emergence](http://www.hybridpedagogy.com/journal/designing-emergence-role-instructor-student-centered-learning/), rather than assume that the discussion forum is wholly analogous to in-person class discussion, or assume that whole-class discussion is the primary locus of emergent learning.\n\nThat last point, and Mary Stewart's excellent article linked from within it, has been my main focus lately. How can I design other tasks for messy, emergent, different-for-every-student learning? And then how can I help students use things like the discussion forum, Twitter, video chat, etc. to support that learning?\n\nI have a few things in mind, which I will likely blog about in more detail later. But here they are in short form:\n\n**Students will be allowed to collaborate on [reading quizzes and \"concept maps\"](http://cognition.shaffermusic.com/assessments/).** Most quiz questions will be raw information retrieval, to \"prime\" students' minds (this is a class in cognitive science, after all) with concepts that will be foundational for the activities to follow. But a few quiz questions, and all of their concept-map prompts, will be things not answered by the textbook, things that require a higher level of thinking, some basic synthesis or application. That's where students will be encouraged to use the forum or Twitter to work together to come to joint solutions. And if they don't use the forum and are having difficulty on the first couple of quizzes, rather than downgrading them *more* for not using the forum, I'll send them an email and see what's going on.  \n\n**Students will use the forum to vote on which of each other's [concept videos](http://cognition.shaffermusic.com/assessments/) will be posted to the class Vimeo channel.** This will 1) require them to use the forum at the beginning of each week (ensuring they get the hang of using it early on), 2) give them some (limited) agency over course materials, and 3) get them engaging each other, focusing on each other's strengths. I'm hopeful that this will help build a healthy community in the course, and lay the foundation for good collaborations in subsequent activities.\n\n**Students will build a fair amount of the course content as they go.** Some of this will be \"review\" material, which will make it relatively easy for students to take part in its creation. But some of the material created will allow these students \u2014 many of which have taken more courses in psychology than I have! \u2014 to draw connections I didn't anticipate, and to teach each other (and in many cases, to teach me) things that I did not plan. (Though this is a course in music cognition, it is an upper-level course in the psychology dept.) I'm hopeful that this will allow the students who do this teaching to be the targets of their colleagues' (and my) questions, with the discussion forum being the logical place for that.\n\nIn general, my thoughts are this: the discussion forum is not the new \"seat time\"; assessments are the new seat time. That is, in an in-person course, we can assume most students will be present for most meetings, and we can design for student activity to be the focus and for messy, emergent learning to take place then. But in an online course, the main thing that we can assume students will do is the assessments, not the discussion forum. So rather than make the discussion forum an assessment and hope that emergent learning will happen there organically, I'm starting to think that we should design assessments to foster emergent learning as a natural product of those activities. This makes for better assessment activities, and the forum then becomes a supportive technology.\n\nClass won't start for a couple more weeks, though. So I am eager to hear from those with experience in wholly online courses about how you have encouraged (or accidentally observed!) emergent learning. What do you think?\n",
                        "html": "",
                        "image": "/content/images/connections.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Flipping an online class",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-05-13 09:38:58 -0600",
                        "created_by": 1,
                        "updated_at": "2015-05-13 09:38:58 -0600",
                        "updated_by": 1,
                        "published_at": "2015-05-13 09:38:58 -0600",
                        "published_by": 1,
                        "og_title": "Flipping an online class",
                        "twitter_title": "Flipping an online class",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/connections.jpg",
                        "twitter_image": "/content/images/connections.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Did third-party votes lose Clinton the election?",
                        "slug": "did-third-party-votes-lose-clinton-the-election",
                        "markdown": "\n<i>Official results are not yet in from every state, but we have good enough numbers to start to tackle some of the statistical questions surrounding last month's presidential election. <a href=\"https://en.wikipedia.org/wiki/United_States_presidential_election,_2016\" target=\"_blank\">Wikipedia has curated</a> the latest official and unofficial numbers from the <a href=\"https://interactives.ap.org/2016/general-election/?SITE=NEWSHOURELN\" target=\"_blank\">Associated Press</a> in a nice table, which I have cleaned up and assembled into an easy-to-wrangle (even in Excel!) <a href=\"https://github.com/kshaffer/election2016\" target=\"_blank\">dataset</a> for your analytical pleasures. <a href=\"https://en.wikipedia.org/wiki/Electoral_College_(United_States)\" target=\"_blank\">Electoral college</a> and <a href=\"https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population\" target=\"_blank\">population</a> data was also taken from Wikipedia.</i>\n\n\nIn the wake of Trump's victory last month, many liberal voters quickly pointed the finger at Jill Stein and third party voters for stealing the election from Clinton. This is understandable given how close the election was. But is it accurate? And does it apply to all of those who voted for a third-party candidate?\n\nLet's see what the data has to say...\n\n\nTo determine if a third-party candidate played spoiler, we need to start by finding **in which states that candidate's votes exceed the winner's margin of victory.** I imported my <a href=\"https://github.com/kshaffer/election2016\" target=\"_blank\">2016 election results dataset</a> into <a href=\"https://www.r-project.org/\" target=\"_blank\">R</a> to do this analysis. Assuming the dataset is imported into a data frame called electionAnalysis (as it is in my <a href=\"https://github.com/kshaffer/election2016/blob/master/2016Election.R\" target=\"_blank\">sample R script</a>), a single line of code will tell you the states in which the amount of votes cast for third-party candidates exceeds the margin of victory for that state:\n\n~~~R\nelectionAnalysis[electionAnalysis$thirdParty >\n  electionAnalysis$marginOfVictory,]$state\n~~~\n\nThis tells us that 14 states could conceivably have been swung by third-party voting: Arizona, Colorado, Florida, Maine, Michigan, Minnesota, Nevada, New Hampshire, New Mexico, North Carolina, Pennsylvania, Utah, Virginia, and Wisconsin.\n\nHowever, the majority of third-party votes in this election went to right-leaning candidates. In fact, Gary Johnson himself brought in more than all other third-party candidates combined. So the real question is: **in which states did Jill Stein win more votes than the margin of victory?**\n\nThere were just four: Michigan, New Hampshire, Pennsylvania, and Wisconsin. Since New Hampshire went for Clinton, the question of Jill Stein spoiling the election for Clinton comes down to just three (now very familiar) states: **Michigan, Pennsylvania, and Wisconsin.**\n\nCould a swing in these three states be enough to change the election results?\n\n~~~R\nsum(electionAnalysis[election$state %in%\n  c('Michigan', 'Pennsylvania', 'Wisconsin'),]$electors2016)*2 >\n  sum(electionAnalysis$trumpElectors) -\n  sum(electionAnalysis$clintonElectors)\n~~~\n\nYes, if Trump's 306 electors were diminished by the 46 electors from these three states, and Clinton's 232 increased by 46, the electoral votes would favor Clinton. So, it's hypothetically possible. But is it likely?\n\nIn Michigan, the margin of victory (based on current counts) is 10,704 votes in favor of Trump. Jill Stein's 51,463 could easily swing this. Let's assume that if Stein were off the ballot, all of her supporters would support Clinton. (This is a *big* if \u2015 after all, if they voted third-party in a state this tight, it's unlikely that either Clinton or Trump was a close second choice. More likely, these were major protest votes from people who simply couldn't vote for the \"lesser of two evils.\") If Clinton took all of Stein's votes, what would happen to the other third-party votes? Johnson brought in 172,136 votes, and even McMullin brought in 8,177, almost enough to swing the state. If Stein were off the ballot, her supporters are more than numerous enough to win the election for Clinton, even if the majority of them stayed home. However, if there were *no* third-party candidates on the ballot, we cannot say with confidence how the state would have turned out. The right-leaning third-party candidates outnumbered her almost 4:1, and we don't know whom any of those voters would consider their second choice. So there's no way we can say with statistical confidence that removing third-party candidates would change Michigan's result. And without Michigan, Clinton couldn't win the electoral college.\n\nPennsylvania is the same story. Trump's margin of victory was just 44,121. Stein's share was only slightly larger at 49,941. And Johnson brought in 146,667. Almost every single Stein vote would have to go for Clinton, and then Johnson's supporters would have to split 50/50 (or lean Clinton) for Clinton to take Pennsylvania. Once again, we cannot say with statistical confidence that removing third-party candidates \u2015 or even just Stein! \u2015 would change the results. And like Michigan, Clinton would also need Pennsylvania to win the electoral college.\n\nLikewise Wisconsin. Trump's margin of victory was 22,177. Stein's share was 31,006, and Johnson's 106,585. Taking Stein off the ballot *might* be enough to swing the state for Clinton, but we don't know for sure, and we cannot predict with confidence the effect of removing all third-party candidates from the ballot.\n\nClinton would have needed all three of these states to win the electoral college, but we can't even come close to predicting with confidence that removing third-party candidates from the ballot \u2015 or even just removing Jill Stein from the ballot \u2015 would send one of these states into Clinton's column, let alone all three.\n\nThird-party voting may have swung the election for Trump. But for all we know, third-party candidates may have *lessened* Trump's victory. We can't make either claim with confidence based on the data. So please don't blame third-party voters, especially in the other 47 states, for losing Clinton the election and move on to more likely problems.\n\nThe electoral college and the nullifying of the Voting Rights Act come to mind...\n\n*Header image by [W H](https://www.flickr.com/photos/wolfgangfoto/2496017650).*\n",
                        "html": "",
                        "image": "/content/images/arrows.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Did third-party votes lose Clinton the election?",
                        "meta_description": "In which states did Jill Stein win more votes than the margin of victory? Could a swing in these states be enough to change the election results?",
                        "author_id": 1,
                        "created_at": "2016-12-07 16:22:00 -0500",
                        "created_by": 1,
                        "updated_at": "2016-12-07 16:22:00 -0500",
                        "updated_by": 1,
                        "published_at": "2016-12-07 16:22:00 -0500",
                        "published_by": 1,
                        "og_title": "Did third-party votes lose Clinton the election?",
                        "twitter_title": "Did third-party votes lose Clinton the election?",
                        "og_description": "In which states did Jill Stein win more votes than the margin of victory? Could a swing in these states be enough to change the election results?",
                        "twitter_description": "In which states did Jill Stein win more votes than the margin of victory? Could a swing in these states be enough to change the election results?",
                        "og_image": "/content/images/arrows.jpg",
                        "twitter_image": "/content/images/arrows.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Sustainable pedagogy",
                        "slug": "sustainable-pedagogy",
                        "markdown": "\nWhat if we focused less on scalable pedagogy and more on sustainable pedagogy?\n\nWhen someone proposes a new idea in education, that proposal is often followed by the question, \u201cBut can you do it at scale?\u201d While \u201cat scale\u201d can mean a variety of things, in this context, it usually means doing it big \u2014 maintaining the *effectiveness* while increasing the *efficiency*. But going big isn\u2019t always best for students, and it isn\u2019t even always best for the bottom line. Education-at-scale is [frequently more expensive than anticipated](http://www.educationnews.org/online-schools/moocs-may-be-free-for-students-but-theyre-expensive-for-schools/), and we end up \u201cdoing more with more\u201d or \u201cdoing the same with more\u201d instead of \u201cdoing more with less.\u201d\n\nBut what if educators started thinking small again? Doing something important, doing it well, doing it sustainably, and helping others replicate the success?\n\nConsider the typical undergraduate humanities college course. These courses in history, literature, etc. are often small, and therefore bring in less income than, say, Chemistry 101. And since they often involve heavy amounts of discussion, critique, and student writing, they don\u2019t scale as well as a lecture course with clicker-based quizzes and multiple-choice exams. As a result, humanities departments are increasingly likely to [bear the brunt of cuts](http://www.newrepublic.com/article/114616/public-universities-hurt-humanities-crisis) and adjunctification. But does this make sense?\n\nWhile the undergraduate humanities seminar may not be scalable, it is *sustainable*. Not only are humanities faculty [cheaper than their STEM counterparts](http://www.humanitiesindicators.org/content/indicatordoc.aspx?i=317) at every rank, but facility needs for these courses are at a minimum. Just about any classroom will do, as will a large table at a coffee shop, or a spot on the lawn \u2014 weather permitting. Books are cheap, especially if the focus is on primary sources \u2014 many of which are likely to be in the public domain \u2014 instead of textbooks or anthologies. And because of these low costs, recent reports have shown that traditional [humanities departments tend to operate in the black](http://newsroom.ucla.edu/stories/bottom-line-shows-humanities-really-155771), sometimes even subsidizing other departments. Even when humanities courses do not work at scale, generally speaking, they are sustainable.\n\nThe arts provide another example. I teach in a College of Music, where students participate in one or more large ensembles most semesters \u2014 orchestra, choir, or band. Large ensembles are incredibly valuable for helping the college build relationships with the community, many of whom will pay to attend a symphony orchestra concert or an opera production on campus. However, large ensembles are incredibly expensive. They require dedicated, large rehearsal facilities and performance venues, which are designed in consultation with acousticians. Those facilities are often underused \u2014 left empty when not needed or, more often, used for classes and activities that do not require such expensive facilities.\n\nMusic students also participate in small ensembles \u2014 \u201cchamber\u201d groups like string quartets or brass quintets. Most music schools require less chamber music study than large ensemble participation for a music degree. However, these small ensembles offer incredible educational benefits for students, while simultaneously being more financially sustainable for most music schools. A typical college chamber ensemble will meet for one hour per week with a faculty coach, with rehearsals often taking place in the same studio where the instructor teaches lessons and holds office hours. Unlike large ensembles, the bulk of the rehearsal is done by the students on their own, in a large practice room, a small classroom during open hours, or even an apartment or a dorm basement. Most sheet music is in the public domain, and new music is far less expensive per student than much orchestra, musical theater, or opera music. And chamber ensembles can perform just about anywhere. In terms of finance and facilities, chamber music ensembles are incredibly *efficient*, and entirely sustainable.\n\n**Chamber ensembles often come with a significant pedagogical benefit. Since the bulk of rehearsal is done away from the faculty, effective student agency is paramount to the success of the group.** Students, not a conductor, direct the artistic decisions. Unlike an orchestra, which is effectively a class, students in a chamber ensemble are free to rehearse beyond set university timetables. That promotes the freedom to experiment in rehearsals, rather than following the predetermined decisions of a conductor. In some musical styles, this also provides more freedom to improvise, an essential activity for developing musicianship, but one that is often neglected among classical musicians because of the lack of room for it in an 80-piece orchestra. And because chamber ensembles can perform nearly anywhere, students are free to pursue more live performance opportunities than in an orchestra, which needs to schedule performances well in advance as well as pay for the performance venue and support personnel. Thus, these small ensembles are both pedagogically valuable *and* financially sustainable, often more so than larger ensembles.\n\nBecause these kinds of endeavors are small, when a new experiment fails, the impact is minimal. But because these kinds of endeavors are sustainable, when a new experiment succeeds, they can be replicated. And that means we can \u201cgo big.\u201d\n\nBut there are a few key differences between going big with a single project at scale, and going big with a small, sustainable project repeated many times.\n\nFirst, **education-at-scale limits the intellectual, pedagogical, and cultural diversity of our classes.** One-size-fits-all education rarely fits anyone well, and the world would be all the poorer for having only a small number of elite institutions \u2014 or private, for-profit corporations \u2014 pursuing massive, \u201cat scale\u201d education. Just as biodiversity is good for a species\u2019 long-term survival, intellectual and ideological diversity is good for society, and a diversity of pedagogical approaches is good for our students, who come to class with a diversity of backgrounds, interests, and goals. Because of this student diversity, one-size-fits-all education means that most students will pay an increasingly high price for an education that was designed with a different kind of student in mind. However, since the sustainable practices I outlined above are small and replicable, there is more room for a variety of approaches between sections, instructors, and institutions. There is also more room for differentiated instruction within the class.\n\nSecond, in all of the sustainable practices I discussed above, there is an increase in student agency and freedom when compared with corresponding \u201cscalable\u201d practices. This may be as simple as smaller classes allowing students more freedom to choose a paper topic than in a large class, where grading is only possible with a significant degree of uniformity. Or it may mean the difference between students writing their own thoughts, or not writing at all \u2014 in favor of bubble tests and plug-and-chug problem sets. In more innovative classrooms, going small may mean that the instructor is more likely to be able to offer tech support for student work, enabling more novel projects. It may even mean that students can have a say in the kinds of projects the class undertakes. Such things are far more difficult, and thus rare, in large classes. In the arts, going small means students make more of the artistic decisions themselves, and they do so in collaboration with peers, not just faculty. **Going small doesn\u2019t always mean more student freedom \u2014 more chances for students to grow as critically minded agents \u2014 but education-at-scale almost always means increased rigidity, simply to keep the machine moving.** That rigidity often runs counter to student freedom, and therefore their intellectual growth.\n\nThis contrast of big v. small, uniform v. diverse, controlling v. empowering betrays [the industrialist roots of many of our modern educational practices](http://www.cathydavidson.com/books/now-you-see-it/). The name of the game is efficiency, effectiveness, growth, and control. However, control of the scholarly ecology is not (and should not be) part of our educational mission. As John Dewey writes:\n\n> To the growth of the [student] all studies are subservient; they are instruments valued as they serve the needs of growth. Personality, character, is more than subject-matter.  \n([*The Child and the Curriculum*](http://www.gutenberg.org/ebooks/29259))\n\nEducation-at-scale can enable widespread and efficient distribution of information. But so can the printing press and the internet. Education is more than that. Going small means a greater chance of intellectual and instructional diversity, an increased chance that students can exercise their freedom and learn to wield it well. And when going small also means going sustainable, it ensures that we can continue to do these important things for some time to come.\n\n*This article was originally published in [Educating Modern Learners](http://www.modernlearners.com/).*\n",
                        "html": "",
                        "image": "/content/images/sustainable.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Sustainable pedagogy",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-03-04 09:25:18 -0700",
                        "created_by": 1,
                        "updated_at": "2015-03-04 09:25:18 -0700",
                        "updated_by": 1,
                        "published_at": "2015-03-04 09:25:18 -0700",
                        "published_by": 1,
                        "og_title": "Sustainable pedagogy",
                        "twitter_title": "Sustainable pedagogy",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/sustainable.jpg",
                        "twitter_image": "/content/images/sustainable.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Digital minimalism \u2015 being deliberate about digital\u00a0identity",
                        "slug": "digital-minimalism-being-deliberate-about-digital-identity",
                        "markdown": "\nMinimalist lifestyle seems to be the latest craze. Minimalism is not just a genre of music or a visual aesthetic, or even an interior design scheme anymore. People are finding ways to cut back on the unnecessary aspects of their lives to focus more of their time, energy, and money on a short list of things that really matter.\n\nWhile much of the minimalist movement is focused on organization, decluttering, purging material possessions, and downsizing homes, the most thoughtful minimalists I\u2019ve come across are not just focused on having less. They\u2019re instead being *deliberate* about life decisions, orienting their possessions, homes, and habits around core needs and passions.\nSo minimalism isn\u2019t just about having less stuff to worry about cleaning. It\u2019s about being less dependent on those things, and spending less time and money on those things, so that you have more time and money to devote to the things you really care about \u2015 family, faith, travel, writing,\u00a0\u2026 or simply paying down debts and saving for a rainy day.\n\nI recently came across the term *digital minimalism* on Cal Newport\u2019s blog, <a href=\"http://calnewport.com/blog/2016/12/18/on-digital-minimalism/\" target=\"blank_\">Study Hacks</a>. He defines it as follows:\n\n> Digital minimalism is a philosophy that helps you question what digital communication tools (and behaviors surrounding these tools) add the most value to your life. It is motivated by the belief that intentionally and aggressively clearing away low-value digital noise, and optimizing your use of the tools that really matter, can significantly improve your life.\n\nCal follows with a combination of tips for being a digital minimalist and reasons for embracing digital minimalism. Likewise, the <a href=\"https://nosidebar.com/\" target=\"blank_\">No Sidebar</a> blog contains a number of articles on digital minimalism. But as someone who spends a lot of time creating content for the web, and helping students and faculty to do the same, there\u2019s something I find glaringly missing from discussions of digital minimalism: **public digital identity.**\n\nIf you search \u201cdigital minimalism\u201d, you\u2019ll find many tips for decluttering your desktop and hard drive, reasons to quit social media (posted ironically on blogs with copious *share this* options), and tips to get to Inbox Zero. But there is precious little about carefully and deliberately curating the content that forms your digital identity \u2015 the content that people find when they search for you on Google, Facebook, Twitter, or stumble across your professional (or not so professional) website.\n\nIn the age of online harassment, fake news, propaganda, and <a href=\"http://www.digitalpedagogylab.com/hybridped/truthy-lies-surreal-truths/\" target=\"blank_\">mass digital deception</a>, it is more important than ever to be deliberate about our public digital identity. As my friend <a href=\"http://cplong.org/2013/09/the-googled-graduate-student/\" target=\"blank_\">Chris Long writes</a>,\n\n> It is going to happen. Maybe not today or this week, but eventually, you will be Googled.\u00a0\u2026 When it happens, you will want content you created to appear early and often in the search results.\n\nAnd I\u2019d add to this (as Chris likely would, as well) that it\u2019s also important that our best content appears in those search results. The things we want people to find when they look us up.\n\nHow do we do that? How do we exercise deliberateness (and restraint) in our public digital work? How do we decide what to post, what to keep, what to delete? And if we do decide that social media isn\u2019t for us, is it possible to create meaningful content that people will engage if we aren\u2019t on Twitter or Facebook to get it in front of them?\n\nI\u2019ve already started this process myself. I\u2019ve deleted a few old websites and started being more deliberate about what I post. I\u2019m also taking a break from Twitter, and looking to purge content from my remaining websites. I want to make sure that my best work, work I still agree with, work I\u2019m proud of are what people find when they look me up. I also want to have less websites to keep up to date, less social networks competing for my attention, so that I can focus my time and energy on the work I care about most. I also want to be more deliberate about my consumption activity online, bringing the internet to me on my own terms.\n\nI invite you to follow along with my *digital minimalism* experiment, maybe even try one of your own. Over the next few months, I\u2019ll post some tips about digital minimalism \u2015 or, really, **deliberate digital identity** \u2015 from my work on <a href=\"http://umw.domains/\" target=\"blank_\">Domain of One\u2019s Own</a> and teaching Digital Studies at <a href=\"http://umwdtlt.com/\" target=\"blank_\">UMW</a>. I\u2019ll also reflect on my process of minimizing \u2015 or, really, **focusing** \u2015 my creative and consumptive activity online.\n\nHopefully my experiences will help others think deliberately about digital identity, how we present our public selves online, and how we spend our time and energy on \u201csocial\u201d media. I look forward to sharing this journey with you all!\n\n<i>Header image by <a href=\"https://www.pexels.com/photo/flight-nature-sky-bird-36035/\" target=\"blank_\">Pixabay</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/seagull.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Digital minimalism \u2015 being deliberate about digital\u00a0identity",
                        "meta_description": "In the age of online harassment, fake news, propaganda, and mass digital deception, it is more important than ever to be deliberate about our public digital identity.",
                        "author_id": 1,
                        "created_at": "2016-12-30 20:12:00 -0500",
                        "created_by": 1,
                        "updated_at": "2016-12-30 20:12:00 -0500",
                        "updated_by": 1,
                        "published_at": "2016-12-30 20:12:00 -0500",
                        "published_by": 1,
                        "og_title": "Digital minimalism \u2015 being deliberate about digital\u00a0identity",
                        "twitter_title": "Digital minimalism \u2015 being deliberate about digital\u00a0identity",
                        "og_description": "In the age of online harassment, fake news, propaganda, and mass digital deception, it is more important than ever to be deliberate about our public digital identity.",
                        "twitter_description": "In the age of online harassment, fake news, propaganda, and mass digital deception, it is more important than ever to be deliberate about our public digital identity.",
                        "og_image": "/content/images/seagull.jpg",
                        "twitter_image": "/content/images/seagull.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Education and discipline",
                        "slug": "education-and-discipline",
                        "markdown": "\n> To the growth of the [student] all studies are subservient; they are instruments valued as they serve the needs of growth. Personality, character, is more than subject-matter.  \n-John Dewey, [*The Child and the Curriculum*](http://www.gutenberg.org/ebooks/29259)\n\nDavid Gooblar at Chronicle Vitae published a piece this week, [\"They Haven't Done the Reading. Again.\"](https://chroniclevitae.com/news/719-they-haven-t-done-the-reading-again) It a piece filled with good ideas about how to ensure students engage material in a class without resorting to punitive quizzes that make up a large portion of the final grade.\n\nThis piece, however, really turned me off. I couldn't even engage his practical ideas, because I was so put off by Gooblar's attitude toward his students. I expressed my displeasure (rather strongly) on Twitter, and several of my friends and colleagues had a hard time seeing why I had such a negative reaction to this piece.\n\nI've been thinking about my reaction, and its source, ever since.\n\nIt starts with the context in which I saw Gooblar's piece\u2014a tweet from Jesse Stommel:\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p>In response to today&#39;s <a href=\"https://twitter.com/chronicle\">@Chronicle</a> piece about getting students to do the reading, a piece I wrote earlier this year: <a href=\"http://t.co/xwZEdUZeoO\">http://t.co/xwZEdUZeoO</a></p>&mdash; Jesse Stommel (@Jessifer) <a href=\"https://twitter.com/Jessifer/status/515000231210270721\">September 25, 2014</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nIn the article Jesse links to, he argues:\n\n> As a teacher, I try to encourage students to be honest about how much they read, what that reading looks like, when they stop reading, when they start again, etc. Most importantly, I ask why. It\u2019s often as interesting to know why we put a book down, as it is to know why we pick one up\u2014to examine our looking away and to examine our compulsion to avoid thinking about or theorizing that looking away. I don\u2019t actively discourage students from reading, but I also do not police their reading. If they\u2019re having trouble, I talk to them about reading strategies (which often involve skimming or thoughtful skipping). I never assume students aren\u2019t reading because of laziness. I always assume their reasons are as complex as my own. And I never work to fill the gaps of their not reading with shame. Like teaching and learning, reading cannot be compulsory.\n\nAlso framing my reading of Gooblar's post is [a blog post published by Sean Michael Morris](http://seanmichaelmorris.com/digital-pedagogy-a-case-of-open-or-shut/) the previous day, in which he writes...\n\n> If you can't trust students, you shouldn't be teaching.\n\nI had just printed out that quote and taped it on my office wall above my desk.\n\nSo when I saw Gooblar's exasperated title, and then the following passage early in his post, I was primed to be incensed.\n\n> Students, because they have what seems to them to be an enormous amount of responsibilities\u2014multiple courses, a budding social life, the apparent need to sleep upwards of 12 hours a night\u2014are pre-eminent prioritizers.\n\nThis flippant, sarcastic dismissal of his students and their ability to act responsibly really made me angry. It smacks of arrogance\u2014surely the professor's priorities for the students ought to be the students' priorities\u2014and insensitivity\u2014assuming that all students who don't obey the professor do so out of a personal deficiency. If I were one of his students, I would be enraged, and I would feel disrespected at the very least.\n\nNow, Gooblar back-pedals from polemic and quickly moves into practical recommendations that, as one friend put it, are quite \"Kris-like.\" He calls on instructors to ensure that assignments are *actually* important, speaks against punitive assessments, etc. And he does offer helpful advice for something that is a real practical problem. After all, not every student who comes to college comes fully ready for self-directed learning in an environment of purely intrinsic motivation. This is something that must be awakened, fostered, trained in many of them\u2014and in the meantime, we might need to use some extrinsic motivations as we \"meet them where they are\" in order to help them transcend that mindset. In fact, that's exactly what I (try to) do in most of my courses.\n\nBut still, this article really bothered me.\n\nJesse put it well when he tweeted,\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p>Best way to get a class to &quot;do the reading&quot;: treat students like collaborators and not a &quot;problem&quot; that needs to be &quot;solved&quot;.</p>&mdash; Jesse Stommel (@Jessifer) <a href=\"https://twitter.com/Jessifer/status/515001329610399744\">September 25, 2014</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nThe more I think about it, the more Gooblar's attitude towards his students bothers me. He views them as a \"problem to be solved\"\u2014a matter of discipline. That is what I was reacting to.\n\nThere's one more big piece to the context in which I read Gooblar's post. Motivated by [Adam Heidebrink](http://twitter.com/adamheid), I'm about halfway through reading John Dewey's [*The Child and the Curriculum*](http://www.gutenberg.org/ebooks/29259). I was looking through the notes I've taken so far while reading, and I found a passage that perfectly illustrates my issues with Gooblar's attitude. (Dewey writes about the education of the child, but I think we can easily substitute \"student\" of any age for most of the occurences of \"child\" in his book, and the insights will apply just as directly.)\n\n> The fundamental factors in the educative process are an immature, underdeveloped being; and certain social aims, meanings, values incarnate of the matured experience of the adult. The educative process is the due interaction of these forces. Such a conception of each in relation to the other[,] as facilitates completest and freest interaction[,] is the essence of educational theory.\n\nIn other words, education is about the free, purposive interaction of the not-yet-fully-developed intellectual/artistic/social capacity of the student on the one hand, and the maturity of intellect/artistry/social interaction that we hope to foster in our students on the other. (Ideally, the instructor would represent this maturity\u2014but only in part, and only as one of a diversity of possibilities.)\n\nDewey continues:\n\n> But here comes the effort of thought. It is easier to see the conditions [immaturity and maturity] in their separateness, to insist upon one at the expense of the other, to make antagonists of them, than to discover a reality to which each belongs. **The easy thing is to seize upon something in the nature of the child, or upon something in the developed consciousness of the adult, and insist upon *that* as the key to the whole problem.** . . . Instead of seeing the educative steadily and as a whole, we see conflicting terms. **We get the case of the child [the student] *vs.* the curriculum; of the individual nature *vs.* social culture.**\n\nThis, I think, gets at the rub of Jesse's tweet. We often view students' non-compliance or inability to succeed in the way we would like as an expression of a deficiency in the student: a problem to be fixed. However, Dewey argues (and does so in more detail elsewhere in this book) that even when undesirable behavior or results *are* the result of a student's immaturity, the educator needs to see that immaturity *as a natural, expected part of a real and normal development of an actual person*. In the case of young students, it is normal, and indeed right, for a child to view the world in terms of their social relationships rather than as a series of discrete subjects to be mastered, and to prioritize those social relationships over the \"world out there.\" That adolescents and adults do this too should surprise no one\u2014and I for one do not think we should be too quick to try to drill this social orientation out of people in favor of a formal, hierarchical, taxonomical representation of knowledge. (But that's a different discussion!)\n\nThe bottom line is this: when student behavior, skill, or progress is out of line with the educator's goals or expectations, Dewey argues that the educator must start from an understanding that there are real and valid reasons for that difference. Even when the immediate cause is a lack of work-ethic (or a prioritizing of sleeping \"upwards of 12 hours a night\"), there is a cause that is very real to the student\u2014more real than the content of our courses or our disciplines. And instead of pitting our expectations against their current mindset, we should realize that part of our role as educators is to help students develop that new mindset, to help them mature intellectually, artistically, critically, socially.\n\nIf we want to foster critical thinking and intellectual maturity, we must do more than cram content and reward/punish behavior. As Maria Montessori argues, discipline is not silence and immobility (in other words, compliance) before the teacher; discipline is the ability to exercise self-control when active. We mustn't confuse compliance with maturity, right answers with critique. If we want to foster mature, critical agency in our students, it is important that we start from a position of respect and understanding, and we must be intentional about the ways we help our students grow into maturity as critical agents.\n\n(Disclaimer: by making these arguments, I'm not meaning to imply that I have this down. Both as a teacher and as a parent, I'm still working on moving beyond a focus on content and behavior in order to foster growth towards intellectual and social maturity. By writing this, I simply hope to open up more of a discussion on how we can focus more on \"the growth of the [student] . . . more than subject-matter.\")\n",
                        "html": "",
                        "image": "/content/images/cherryClone.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Education and discipline",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2014-09-27 15:08:50 -0600",
                        "created_by": 1,
                        "updated_at": "2014-09-27 15:08:50 -0600",
                        "updated_by": 1,
                        "published_at": "2014-09-27 15:08:50 -0600",
                        "published_by": 1,
                        "og_title": "Education and discipline",
                        "twitter_title": "Education and discipline",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/cherryClone.jpg",
                        "twitter_image": "/content/images/cherryClone.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Backwards design in education?",
                        "slug": "backwards-design",
                        "markdown": "\nCan we backwards-design education? Can we determine in advance where our students *will* end up? *should* end up? And can we determine the script all our students need in order to end up there?\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"und\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/digped?src=hash\">#digped</a> <a href=\"https://twitter.com/hashtag/constructivism?src=hash\">#constructivism</a> <a href=\"http://t.co/3cbTSSPgtK\">pic.twitter.com/3cbTSSPgtK</a></p>&mdash; Kris Shaffer (@krisshaffer) <a href=\"https://twitter.com/krisshaffer/status/630852905243795456\">August 10, 2015</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nIn many ways, backwards design is simply a return to behaviorism \u2015 the idea that what matters is student output \u2015 or cognitivism \u2015 the idea that the mind is a black box, and that educating the mind means determining the appropriate inputs to attain the desired outputs.\n\nThis is problematic thinking, both from what we know about the human brain, but also from what we know as humans.\n\nThe above picture \u2015 a reflection of mine during Audrey Watters's keynote at [Digital Pedagogy Lab](http://digitalpedagogylab.com/institute) \u2015 summarizes the problem well: we are using machine learning as a metaphor, or really a *model*, for human learning, when we still don't even know how to get machines *that we invented* to learn what we want them to learn.\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"und\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/digped?src=hash\">#digped</a> <a href=\"http://t.co/XJ6alH1Enu\">pic.twitter.com/XJ6alH1Enu</a></p>&mdash; Kris Shaffer (@krisshaffer) <a href=\"https://twitter.com/krisshaffer/status/630855925423697921\">August 10, 2015</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nAnd yet goals are real. Our students have them. We have them. We have them for our students. Our students have them for us. And our institutions...\n\nBut these goals need to go beyond top-down behaviorist objectives. For example, two \"learning objectives\" for the final project in my recent course in [Music Cognition](http://cognition.shaffermusic.com) are *construct and articulate substantive, manageable goals* and *self-assess progress towards those goals*. Helping students define goals, discern which are manageable, outline steps to take in pursuit of those goals, assess their own progress towards those goals, *and rethink both the goals and the process along the way* are all important parts of education. Perhaps more important than most of the content we \"cover\" in our classes.\n\nIt's also clear from the research literature on learning, and especially on creativity, that memorization and understanding basic bits of knowledge is in many ways a precursor to higher-level thinking, critique, and creative use of that knowledge.\n\nTake musical improvisation as an example. In his keynote this morning with Amy Collier, [Jesse Stommel](http://twitter.com/jessifer) used jazz as a metaphor for learning through play and improvisation, a contrast with rigid *learning objectives* and predtermined outcomes. Absolutely, the best jazz musicians \u2015 the best *musicians*, I would say \u2015 are those who can improvise, those who can create music that has significant impact on poeple, and do it in the moment. That level of musical skill \u2015 think J.S. Bach, Thelonius Monk, Miles Davis, B\u00e9la Fleck ... the (hopefully not too apocraphal) improvisation \"duel\" between Beethoven and Liszt \u2015 is not opposed to the rote learning of pre-established outcomes. Rather, it *goes beyond it*.\n\nThe same is true for the great avant-garde artists of the twentieth century. This is Picasso...\n\n<img src=\"http://www.arthistoryarchive.com/arthistory/cubism/images/PabloPicasso-Three-Musicians-1921.jpg\" alt=\"Picasso's painting, Three Musicians\" />\n\nBut so is this...\n\n<img src=\"http://uploads4.wikiart.org/images/pablo-picasso/first-communion-1896.jpg\" alt=\"Picasso's painting, First Communion\" />\n\nAnd this isn't just a \"learn the rules so you can break them\" issue. This is a matter of internalizing the standard, accepted ways of doing things in such a robust way that we become aware of all the holes in those modes of thinking, the connections between seeming disparate things, the possibilities (and impossibilities) of what new things can be done... or at least should be tried.\n\n<blockquote class=\"twitter-tweet\" lang=\"en\"><p lang=\"und\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/digped?src=hash\">#digped</a> <a href=\"http://t.co/McaGxZM7h7\">pic.twitter.com/McaGxZM7h7</a></p>&mdash; Kris Shaffer (@krisshaffer) <a href=\"https://twitter.com/krisshaffer/status/631900237536194560\">August 13, 2015</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nI agree wholeheartedly that the movement towards measurable learning or prescripted outcomes is fraught with both pedagogical and political problems. But I don't want to throw the baby out with the bathwater. There are things that we do know about learning, and there are things that we *should learn*. But we cannot stop there. We learn the prescripted content and skills with the objective (pun intended) of using them as a launching point for deep, multifaceted, diverse, unpredicted \u2015 unpredictable \u2015 insights. Epiphanies, even.\n\nTo have an epiphany, first we need to know stuff.\n\nSo let's teach stuff. But leave lots of room for other stuff. Because that other stuff is the stuff of education. The stuff of life.\n",
                        "html": "",
                        "image": "/content/images/scaffold.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Backwards design in education?",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-08-14 11:22:02 -0500",
                        "created_by": 1,
                        "updated_at": "2015-08-14 11:22:02 -0500",
                        "updated_by": 1,
                        "published_at": "2015-08-14 11:22:02 -0500",
                        "published_by": 1,
                        "og_title": "Backwards design in education?",
                        "twitter_title": "Backwards design in education?",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/scaffold.jpg",
                        "twitter_image": "/content/images/scaffold.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Aural skills 'playbook'",
                        "slug": "aural-skills-playbook",
                        "markdown": "\nI'm currently assembling some materials for the instructional team for Aural Skills III, as we begin a new unit focusing on the transcription and improvisation of standard classical theme types. I'm assembling a \"playbook\" \u2014 a quasi-progressive set of activities \u2014 that they can choose from as they prepare for their section meetings and change plans mid-class, if necessary. These are things that I've found helpful working with students who are learning how to transcribe four-part textures, sight-read classical melodies, and improvise in the classical style.\n\nThese activities will be supported by readings/videos from [*Open Music Theory*](http://openmusictheory.com), in particular the resources under \"Classical theme types\" and \"Galant schemata.\" The page on [Improvising a sentence with galant schemata](http://openmusictheory.com/schemata-improv.html) is a staple of this unit, as well as [this video on four-part transcription](https://vimeo.com/119572881).\n\nI hope these are helpful! And I'd love to hear other ideas that I can add to this resource. What do you find most helpful when teaching transcription, classical improvisation, sight-reading?\n\n## Transcription\n\nThe following tasks are skill-development specific and are probably best done in order throughout the unit (large group early, then small group, then progressive, then individual/assessment toward the end of the unit).\n\n**Large-group transcription skill development:** Play the passage over the speakers (or at the piano) multiple times. Between each hearing, discuss and evaluate what students think they heard each time. While not neglecting right/wrong notes/rhythms/chords/schemas, focus on skills and techniques: How did you hear that? Why do you think it's Option A and not Option B? What things are you paying attention to most on the hearings in which you are most successful? Etc. End each practice task with a couple takeaway points for future transcribing. Begin subsequent transcription tasks with a reminder of past takeaway points.\n\n**Small-group transcription skill development:** Play the passage over the speakers (or at the piano) multiple times. Assemble students in groups of 3ish at the white board or at computers. Between hearings, students collaborate on their transcription. Emphasize that each student should be providing input and attempting to convince their colleagues of their transcription's accuracy. Walk from group to group between hearings. Provide specific comments to groups, but also observe trends and when necessary provide whole-class tips and feedback before the next hearing. Once groups are successful, or close, or stuck, distribute (or project) a score. Follow up with discussion: what was easy, what was hard, what worked, what didn't work, etc. End each practice task with a couple takeaway points for future transcribing.\n\n**Progressive transcription skill-development:** Provide students with a recording to transcribe on their own at home. They should take a much time as necessary in order to get it perfect (or go until they are stuck). In class, students join with a single partner or a group of three to compare notes. Where their transcriptions differ, they try to convince each other of their transcription's accuracy, then work together on a joint transcription. They should not erase their original, but keep it to compare with the group solution. At the end of class, discuss as a whole group what questions still remain about the transcription. Distribute score and discuss difficulties further. Talk techniques: what worked, what didn't, etc.\n\n**Analysis (mainly for four-part textures):** Provide scores annotated with Roman numerals/modulations. Have students study the annotated scores, looking for common or uncommon patterns. Listen to the passage multiple times, with students tracking (and possibly singing) each part in turn (Violin I, Violin II, \u2026). Ask what kinds of things they listen for that helps them track individual voices. Which lines, instruments, or musical patterns are easier or harder to hear? What strategies seem to lead to more success? If possible, take notes for the class as they work in a Google Doc and share with them after class. (Or write on the board, take a picture, and email it to them. The idea is for them to focus on the analysis, rather than writing stuff down.)\n\n## Improvisation with galant schemata\n\nEach schema, as defined by Gjerdingen (and presented on Open Music Theory), has a series of \u201cstages.\u201d Each stage contains a most-likely bass note, a most-likely melodic note, and a most-likely harmony (expressed in thoroughbass, but we can convert to Roman numerals). This stage breakdown can facilitate our practice.\n\n**Single-schema melodic skeletons:** In class, provide a single schema (and a key). Play the bass and/or standard chord for each stage, while the students sing or play the standard melodic skeleton. Early attempts can be with notes (open to the OMT cheatsheet I\u2019ll make) or even with a projected cheat sheet. Later attempts should be from memory. We can easily cycle through these quickly at the beginnings of class meetings, calling out a schema, playing it together, calling out another, playing it together, rinse and repeat, in a single key. As students become more familiar with these, after playing a schema we can ask \u201cwhat might come next?\u201d; if the answer makes musical sense, go with that one. (Thus, we\u2019ll be cycling proposte => riposte => cadence, which will help them familiarize themselves (unconsciously?) with the patterns that they\u2019ll see in their theme assessments.)\n\n**Single-schema recognition:** Provide a key. Play the bass and block chords of a schema. Have the students sing/play back the melody that matches the schema played.\n\n**Single-schema arpeggiations:** Same as above, but with three-note arpeggiations of the schema\u2019s chord. Some class discussion beforehand is good, to decide the best arpeggios to play in order to make good musical sense. That discussion will be somewhat trivial for single schemas, but will be very important when we start stringing them together.\n\n**Schema chains (group):** Propose a proposte => riposte => cadence progression (and a key/meter). Have the entire class play the one-note skeletons for each schema in progression. Then discuss any massaging that needs to take place to make them work; repeat the performance with that massaging. Then do the same with the arpeggiated versions of each.\n\n**Schema chains (individuals):** After the group schema chain, select a student to perform a melody for that progression that includes some non-chord tones. Other students should listen and decide 1) did the melody match the schemas?, 2) are the non-chord tones well formed? (See Embellishing tones.) The student who identifies the error should then attempt a correction. Continue until the melody works.\n\n**Call and response:** Pick a key/meter/form type. Student one improvises the first phrase (or basic idea). Student two listens and improvises the second phrase (or basic idea). Other students listen and judge 1) did each phrase match the form?, 2) did the second phrase follow musically from the first? The student who identifies the error should then attempt a correction. Continue until the melody works.\n\n**Partner work:** Break class into groups of 2 or 3 and have them give one or more of the above challenges to each other. Walk from group to group and offer feedback/tips. (This may not be practical in some of our rooms, especially when using instruments.)\n\n## Sight-reading/singing\n\nWhen sight-reading/singing in class, focus more on analytical thinking about the music while performing it, not just on hammering through sight-reading. Sight-reading is more an analytical skill than anything: look at the page, what structures do you see, what do they sound like, and where do you need to direct the most attention before/as you perform? Teaching them how to sight-read well is a secondary goal (in this class) to helping them be more aware of the structures they are performing and strategize their (sight) performance accordingly. Use the excerpts as opportunities to try out analysis and performance strategies, comment on their helpfulness, and tweak based on the experience.\n",
                        "html": "",
                        "image": "/content/images/oldpiano.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Aural skills 'playbook'",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-10-07 08:32:02 -0600",
                        "created_by": 1,
                        "updated_at": "2015-10-07 08:32:02 -0600",
                        "updated_by": 1,
                        "published_at": "2015-10-07 08:32:02 -0600",
                        "published_by": 1,
                        "og_title": "Aural skills 'playbook'",
                        "twitter_title": "Aural skills 'playbook'",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/oldpiano.jpg",
                        "twitter_image": "/content/images/oldpiano.jpg"
                    },
                    {
                        "id": 0,
                        "title": "bell hooks and spiritual pedagogy",
                        "slug": "bell-hooks-and-spiritual-pedagogy",
                        "markdown": "\n> To educate as the practice of freedom . . . comes easiest to those of us who teach who also believe that there is an aspect of our vocation that is sacred; who believe that our work is not merely to share information but to share in the intellectual and spiritual growth of our students. -bell hooks, *Teaching to Transgress: Education as the Practice of Freedom, p. 13*\n\nI first read these words a few weeks ago. That same week, I had a conversation with one of my pastors about the need for Christian scholars to engage both the spiritual and material in their work. Too often, Christians are content to be functional materialists \u2014 people who believe in the spiritual, indeed who devote much of their lives to it, but who work as if the spiritual has no part to play in their scholarship or their teaching.\n\nWhile talking with my pastor, I admitted to functional materialism myself. For example, when thinking about the relationship of music to human emotion, I'm convinced that there is a spiritual aspect to this relationship. Indeed, it may even be the foundational aspect. But, I told him, *I don't have any theories* upon which I can base my scholarship or teaching of this phenomenon. And so I teach it like a materialist.\n\nOf course, I don't teach my children this way. My six-year-old loves astronomy. And when learning about the universe, I use every moment of wonder to stir in his mind an awe for God and a longing for the spiritual, the eternal. Some use God as a cheap explanation when children ask hard *why?* questions. But that's not the case here. As much as his brain can handle \u2014 and usually more than it can handle \u2014 I give him the material, mechanical explanation. But, in good Kantian fashion, I use that to stir his sense of divine wonder even more intensely. (See the \"mathematical sublime\" in Kant's *Critique of the Power of Judgment*.)\n\nMany Christian academics do this. In our own minds, with our families, our congregations, we draw on what we know of the universe (or our scholarly corner of it) to inspire an experience of the sublime, and through it to meditate, and lead others to meditate, on the glory of God. But we put that lens down in the academy. This is often out of fear. Yes, fear of what outing ourselves as Christians will do to our credibility, even job security, in the academy.\n\nBut I think there is a deeper fear. Come with me for a minute . . .\n\nLooking critically at materialist theories in the academy, the spiritually minded Christian finds some very low-hanging fruit: *what does it all mean?* As a music theorist, I know the temptation to \"explain\" something simply by accounting for its structural elements and the processes that brought them forth. But that doesn't cut it. And every composer, performer, songwriter, musicologist, critic, and audience member will be quick to point that out. The meaning of a work of art is far more than \u2014 perhaps even orthogonal to \u2014 its structural characteristics.\n\nBut wait a second. It's not just Christian musicians who level that critique. Not even religious or spiritial people more generally. *Every human has the capacity to point out that materialism is not enough to explain what things mean.* Why does every human culture have a music? How can a painting, a movie, a poem make someone cry? Why do people die? Why do bad things happen to good people? *No one thinks that materialism is enough to explain these things.* (And if they say otherwise, they're lying.)\n\nSo then why do so many in the academy work as functional materialists, even if deep down we know that ultimate answers lie beyond materialism?\n\nI think the answer is still fear. Not fear of being outed as a spiritual person. Fear that *we have no theories* for any of this. Fear that we could easily spend our entire careers seeking answers that we may never find. Fear of the impact that such a lack of answers could have on us as thinking, feeling people.\n\nBut back to bell hooks. Imagine the impact we teachers could have on our students \u2014 intellectually *and* spiritually \u2014 if we simply *ask* these questions. Or better yet, if we created a space in which students could ask these questions themselves. Still better, a space where they felt the *responsibility* to ask what it all means.\n\nPaulo Freire writes that \"Any situation in which some individuals prevent others from engaging in the process of inquiry is one of violence. The means used are not important; to alienate human beings from their own decision-making is to change them into objects\" (*Pedagogy of the Oppressed*, p. 85). This is not always physical violence. In fact, often it is not. But it *is* spiritual violence. To objectify another is to reduce them to material \u2014 meaningless material. Denying the existence of someone's spirit, especially if in the process we convince *them* of its non-existence, is indeed spiritual violence. Further, Christians believe that humans are made in the image of God. And so the objectification of another, the denial of their subjectivity and spirituality, is an act of violence against that image \u2014 against God himself.\n\nAs I write this, I remind myself: *I don't have the theories* to deal with all of this. I have doctrine, but its application here is difficult. So take the previous paragraph with a grain of salt. But I think that Freire and hooks are on to something important here. To avoid violence against our students, to care for them, to love them, means to awaken their spirit. For many, their spirits lie dormant, having been lulled to sleep by cultural materialism and beaten down by academic materialism. And so as we educate, we must counter the materialist liturgy that has committed this violence against them. The compartementalization of mind, body, and spirit must be undone \u2014 and it must begin with those of us who seek to educate. And we must seek at every turn to nurture that which has atrophied in our students, in their whole being, not simply their capacity to store and retrieve information.\n\nbell hooks writes that \"The classroom remains the most radical space of possibility in the academy\" (TtT, p. 12). And caring for the spirit may be the most radical thing we do in those spaces.\n\nNow I just need to figure out how . . .\n",
                        "html": "",
                        "image": "/content/images/bookLocked.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "bell hooks and spiritual pedagogy",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-01-25 20:14:06 -0700",
                        "created_by": 1,
                        "updated_at": "2015-01-25 20:14:06 -0700",
                        "updated_by": 1,
                        "published_at": "2015-01-25 20:14:06 -0700",
                        "published_by": 1,
                        "og_title": "bell hooks and spiritual pedagogy",
                        "twitter_title": "bell hooks and spiritual pedagogy",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/bookLocked.jpg",
                        "twitter_image": "/content/images/bookLocked.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Digital privacy and the blog",
                        "slug": "blog-privacy",
                        "markdown": "\nI've been reading a lot about digital privacy lately. I've been thinking about ways to keep my data more secure from the NSA, from corporations, from my ISP, etc. And then I realized that I use services on my website that aren't privacy-friendly. So I made a couple of updates to this website today.\n\n**First, no more Google Analytics.** It's great to see which of my posts are most popular, where my visitors are from geographically, what sites are linking to my site... But I don't really need that information. And I certainly don't need it bad enough to warrant taking information about my site's visitors and sending it off to Google. So no more Google Analytics. If I decide in the future that I need analytics, I'll find a tool that I install on my server and can keep secure from others. But for now at least, I'm simply not going to collect analytics data.\n\nThe second change was to **drop Disqus comments.** Disqus was the most offending service on my site. Not only did it flag [Privacy Badger](https://www.eff.org/privacybadger) when I visited my own site, but when I visited disqus.com directly, Privacy Badger *blocked the entire site!* All I got was a blank page until I temporarily disabled Privacy Badger. I looked back over the past couple of years, and there was roughly one comment per month. (Most of the discussion happens via social media.) So I decided to keep [webmentions](http://indiewebcamp.com/Webmention) \u2015 which come from public sources on users' own platforms \u2015 and include a note at the bottom of each post with ways readers can contact me if they like.\n\nThat said, I still feel somewhat ambivalent about webmentions. While some come from services where users expect their comments to show up here, many of them are posted by users without knowing that their platform (mainly Twitter and Facebook) supports webmentions and that their comments are ending up on my site. The comments are public, but not necessarily intended to be preserved here. So I'm still thinking about that.\n\nWith that caveat about webmentions, I feel good about diminishing the amount of my readers' private data I'm sending to third parties. What do you all think? Send me a webmention! :p\n",
                        "html": "",
                        "image": "/content/images/panopticon.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Digital privacy and the blog",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-09-29 11:22:02 -0600",
                        "created_by": 1,
                        "updated_at": "2015-09-29 11:22:02 -0600",
                        "updated_by": 1,
                        "published_at": "2015-09-29 11:22:02 -0600",
                        "published_by": 1,
                        "og_title": "Digital privacy and the blog",
                        "twitter_title": "Digital privacy and the blog",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/panopticon.jpg",
                        "twitter_image": "/content/images/panopticon.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Getting data out of Medium",
                        "slug": "getting-data-out-of-medium",
                        "markdown": "\nControlling my data is important to me. It\u2019s also important that my students (and the faculty that I support) have the ability to control their own data, as well. That doesn\u2019t mean that *everything* needs to live on a Domain of One\u2019s Own. But it does mean that I want my data to be as flexible as possible, and as easy to move around as possible.\n\nIt\u2019s really easy to [download an archive](https://help.medium.com/hc/en-us/articles/214043918-Export-content-from-Medium) of your Medium posts. Like your Twitter archive, you can just unzip the archive and upload it to your domain, and you\u2019ve got it up and running.\u00a0\n\nHowever, if you want to incorporate those posts into a different platform\u200a\u2014\u200alike WordPress, Jekyll, Known, etc.\u200a\u2014\u200ait is more of a challenge.\n\nI wrote my posts on the Medium API directly in Medium. Partly as an experiment, and partly because I love the Medium post editor. (It\u2019s why I incorporated a [Medium editor clone](https://yabwe.github.io/medium-editor/) into [Peasy](https://peasy.pushpullfork.com/).) But after writing three posts\u200a\u2014\u200a complete with feature images, inline images, and code blocks\u200a\u2014\u200ain Medium, I decided to import them into [my Jekyll/GitHub Pages site](http://kris.shaffermusic.com/). That\u2019s turned out to be a challenge. Not an insurmountable one, but one that I\u2019d rather avoid going through.\n\nI downloaded my Medium archive, used [Pandoc](http://pandoc.org/) to convert the posts from HTML to MarkDown, and then copied and pasted the MarkDown into new posts on my Jekyll site. There was more post-processing than I anticipated, or would like. And it doesn\u2019t look as easy to automate the cleanup as I would like.\u00a0\n\nEven more frustrating was my discovery a couple weeks ago that the Medium API supports *posting* to Medium, but not *retrieving* posts from Medium. It is easy to write code that cross-posts from another platform to Medium, but Medium makes it more difficult to go the other way.\n\nWhy?\n\nMy guess is that their focus is on content. They want to be the place where we go to find ALL THE CONTENT. So they make it really easy to get content in. Harder to get content out. And by making a beautiful, easy-to-use editor, the temptation is strong to just use Medium from the start.\u00a0\n\nIf we just want to write, get our writings read, and have a permanent record of what we wrote. Medium can be great. But if we want to write content that we keep coming back to, content that keeps evolving, content that\u2019s part of a long-term project\u00a0\u2026 *and if we don\u2019t want that long-term project to be locked into a single platform*\u00a0\u2026 then Medium may be a problem.\n\nI say as I write this post on Medium.\n\nBecause I just can\u2019t resist this editor.\n\nTime to go add some code to Peasy so I can get it ready for prime-time sooner.\n\n*Featured image by [paul bica](https://www.flickr.com/photos/dexxus/5794905716/) (CC BY).*\n",
                        "html": "",
                        "image": "/content/images/wishbone.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Getting data out of Medium",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-09-26 10:45:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-09-26 10:45:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-09-26 10:45:00 -0400",
                        "published_by": 1,
                        "og_title": "Getting data out of Medium",
                        "twitter_title": "Getting data out of Medium",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/wishbone.jpg",
                        "twitter_image": "/content/images/wishbone.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Trumping science",
                        "slug": "trumping-science",
                        "markdown": "\nThe Trump administration has been drastically changing the way it communicates information with the public. One of the first pages people noticed missing from the Trump administration's website was the page on climate change. Deleting the previous administration's information and rebuilding whitehouse.gov is not unheard of, but this is part of a bigger trend of mis/disinformation from the Trump administration. As <a href=\"/2017/02/misinformation-trump-administration/\" target=\"blank_\">I wrote a couple weeks ago</a>,\n\n> the Trump administration is obfuscating information. Delete important content, make drastic changes to existing content, replace user-friendly web sites with PDFs and Word documents, pull back on social media usage, limit federal employee access to the press, and leave the press\u2019s inquiries unanswered by the press secretary. ... [This] harrowing picture of our current government\u2019s approach to information [is] not surprising given the media history of Trump and several of his top advisors. But it\u2019s incredibly dangerous.\n\nNow that Trump has been in office for over a month, he's had a chance to start replacing content on whitehouse.gov. So what changes has his administration been making?\n\nI've been looking into several areas - science, immigration, Russia, etc. In this post, I'll share what I've found about the Trump administration's approach to science, in particular climate change.\n\n## The data\n\nI started by searching my archives for mentions of the words *science* and *climate*, and the phrase *global warming* (I used snapshots of whitehouse.gov from January 20 and March 1). Once stop words (\"a\", \"an\", \"the\", \"of\", etc.) were removed, Obama's site had a total of almost **2.5 million words,** while Trump's site had approximately **870,000 words,** about one-third the size. In that context, **the word *science* occurred 7,402 times on Obama's site, but only 116 times on Trump's!** *Sciences* occurred 129 and 15 times, respectively. Even more stark, ***climate* occurs 6,534 times on Obama's site, but only 97 times on Trump's!** Global warming occurs 15 times on Obama's site, and not at all on Trump's.\n\nThis is embarrassing.\n\nBut raw word counts only tell us so much. *How* do Obama's and Trump's sites talk about science in the speeches, press briefings, executive orders, and other documents they contain? Here are the phrases containing \"science\" that are most characteristic of each of the administrations' sites. These are not all the two-word phrases, only those with the strongest difference in relative frequency between the two sites (based on a <a href=\"https://en.wikipedia.org/wiki/Odds_ratio\" target=\"blank_\">log odds ratio</a>).\n\n<a href=\"/content/images/science_mar1.png\" target=\"blank_\"><img src=\"/content/images/science_mar1.png\" alt=\"Top terms containing 'science' on whitehouse.gov, Trump vs. Obama, log odds ratio.\" /></a>\n\nThe terms don't seem all that different until we realize that many of the terms on Trump's list (which occur far less often than the terms on Obama's list) are largely those associated with bios of former presidents, vice presidents, their spouses, and other individuals featured on the site. \"Library science\", for example, exclusively occurs on Laura Bush's biography (which also occurs on Obama's site). While these bios occur on both sites, the terms in them show up as the science terms most characteristic of Trump's site *because there is so little about science on his site to begin with.*\n\nSimilarly, here are the most common phrases for Trump's and Obama's sites containing the word \"climate\".\n\n<a href=\"/content/images/climate_mar1.png\" target=\"blank_\"><img src=\"/content/images/climate_mar1.png\" alt=\"Top terms containing 'climate' on whitehouse.gov, Trump vs. Obama, log odds ratio.\" /></a>\n\nNot only does Trump not use the term climate very often (97 times vs. Obama's 6,534 times), but when he uses the word climate, he's not even talking about science! Oh, and the vast majority of those 97 mentions of climate change come from historical material from past administrations preserved on the current whitehouse.gov. If we remove those historical pages from both archives, Trump only uses the word *climate* 31 times, to Obama's 6,534.\n\nHere are the most characteristic phrases containing *climate* on each site, with historical archives removed.\n\n<a href=\"/content/images/climate_non_hist_mar1.png\" target=\"blank_\"><img src=\"/content/images/climate_non_hist_mar1.png\" alt=\"Top terms containing 'climate' on whitehouse.gov, Trump vs. Obama, log odds ratio, no historically preserved pages.\" /></a>\n\n*Note: \"milder climate\" is from Mamie Eisenhower's bio.*\n\nObama uses *climate* to talk about the environment. Trump uses *climate* to talk about money. Science policy is clearly not a priority for the beginning of the Trump administration.\n\nAgain, this is embarrassing. Not surprising, but embarrassing. And coupled with the cuts being proposed to NOAA, the EPA, and other agencies conducting climate and other scientific research, it's downright dangerous.\n\n## Coming next...\n\nThis isn't all I've examined. In upcoming posts, I'll dive into how Trump talks about immigration, terrorism, Russia, and a strange obsession with topics from the Cold War (?!).\n\nAnd if you're interested, check out the code and play around with this yourself. Or drop me a line and let me know what else you're interested in seeing.\n\n*All data and code for this analysis can be downloaded from my <a href=\"https://github.com/kshaffer/whitehouse\" target=\"blank_\">whitehouse GitHub repository</a>.*\n\n*Header image by <a href=\"https://unsplash.com/photos/-hI5dX2ObAs\" target=\"blank_\">NASA</a>.*\n",
                        "html": "",
                        "image": "/content/images/nasa.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Trumping science",
                        "meta_description": "Science policy is clearly not a priority for the Trump administration. Here are some stats.",
                        "author_id": 1,
                        "created_at": "2017-03-08 15:12:00 -0500",
                        "created_by": 1,
                        "updated_at": "2017-03-08 15:12:00 -0500",
                        "updated_by": 1,
                        "published_at": "2017-03-08 15:12:00 -0500",
                        "published_by": 1,
                        "og_title": "Trumping science",
                        "twitter_title": "Trumping science",
                        "og_description": "Science policy is clearly not a priority for the Trump administration. Here are some stats.",
                        "twitter_description": "Science policy is clearly not a priority for the Trump administration. Here are some stats.",
                        "og_image": "/content/images/nasa.jpg",
                        "twitter_image": "/content/images/nasa.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Exploring musical data with R: An introduction to computational music analysis",
                        "slug": "exploring-musical-data-with-R",
                        "markdown": "\nI created a series of tutorials for my Computational Music Analysis students this summer, introducing them to basic analysis and visualization of musical data in [RStudio](https://rstudio.com). RStudio is a powerful, free tool for statistical analysis, and these tutorials are based on datasets from recent publications in music theory and analysis: the McGill Billboard Dataset, Trevor de Clercq and David Temperley's Rolling Stone corpus, and a subset of the Million Song Dataset (with lyric and genre tags).\n\nIf you're interested in learning statistical analysis of musical data, getting more comfortable with RStudio, or just want some fun datasets to play around with, check out the resources below.\n\n*Note: if you don't use R, you can still download these datasets and play around with them in Excel, or write your own scripts in Python, Perl, or other language of your choice. It's simply a platform-independent, plain-text spreadsheet file.*\n\n## Learning the basics of R - summaries and plots with the Rolling Stone corpus\n\n[![]({{ site.url }}/media/dtroot.png)]({{ site.url }}/media/dtroot.png)  \n*Frequency of chord root in Temperley\u2019s Rolling Stone analyses. Integers on the X axis represent number of semitones above the song\u2019s tonic pitch.*\n\nThis tutorial is built around a cleaned up version of David Temperley's harmonic analyses of 100 'rock' songs for a 2011 article he co-authored with Trevor deClercq: [\"A corpus analysis of rock harmony.\"](http://dx.doi.org/10.1017/S026114301000067X)\n\nHere is [the data file]({{ site.url }}/media/resultsBySong.csv).  \nAnd here is [the R script file]({{ site.url }}/media/basicSummariesAndPlots.R).\n\nAssuming you have [RStudio](https://rstudio.com) installed, simply download the two files to the same folder, open the R script in RStudio, and follow the instructions in that file. You can run each individual command by selecting the line and clicking 'run'. Or \u2015 since each command is a single line \u2015 you can simply place your cursor anywhere on the line you want to run, and then press CMD-Return on a Mac or CTRL-Enter on Windows (and probably Linux).\n\n\n## Data clean-up, tables, and more plots with the McGill Billboard Dataset\n\n[![]({{ site.url }}/media/bbroot.png)]({{ site.url }}/media/bbroot.png)  \n*Frequency of chord roots in McGill Billboard dataset.*\n\nThis tutorial follows on the Temperley/de Clercq tutorial, and is built around the data from the [McGill Billboard dataset](http://ddmal.music.mcgill.ca/billboard) (version 2.0). The McGill Billboard dataset contains information about chords in over 700 songs from the Billboard Hot 100 lists from the late 1950s to the early 1990s, along with timing information and metadata. However, the dataset does not include things like chords roots *relative to the tonic of the key*, which though easily calculated, makes things easier to analyze when included explicitly. So we added that. We also had to clean up some errors in the source file. This R script walks through that process, so you can get some experience with an aspect of data cleanup. (And if memory serves me correctly, leaves in a couple errors for users to find and correct following the same methods.)\n\nHere is [the data file]({{ site.url }}/media/all_chords.csv).  \nHere is [the data file post-cleanup]({{ site.url }}/media/all_chords_fixed.csv).  \nAnd here is [the R script file]({{ site.url }}/media/BBBasicSummariesAndPlots.R).\n\nAssuming you have [RStudio](https://rstudio.com) installed, simply download the two files to the same folder, open the R script in RStudio, and follow the instructions in that file. You can run each individual command by selecting the line and clicking 'run'. Or \u2015 since each command is a single line \u2015 you can simply place your cursor anywhere on the line you want to run, and then press CMD-Return on a Mac or CTRL-Enter on Windows (and probably Linux).\n\n\n## Correlation, Chi-Squared, and ANoVA tests with the Million Song Dataset\n\n[![]({{ site.url }}/media/anova.png)]({{ site.url }}/media/anova.png)  \n*ANoVA for the association of genre song duration in the Million Song Dataset subset.*\n\nThis tutorial goes into more detail regarding data prep and statistics, based on a subset of songs from the [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/), and corresponding lyric and genre data from [musixmatch database](http://labrosa.ee.columbia.edu/millionsong/musixmatch) and the [Tagtraum genre annotation database](http://www.tagtraum.com/msd_genre_datasets.html). This tutorial walks through the process of combining the musical data from the MSD, with the user-generated genre tags from Tagtraum and the results of a (preliminary) [topic model analysis of song lyrics](https://medium.com/modeling-music/topic-modelling-song-lyrics-from-the-million-song-dataset-3c01a0d79988#.elvtus3i4).\n\nFor more details on the content of this subset of the MSD, see [A Closer Look at the Million Song Dataset](https://medium.com/modeling-music/the-intersection-between-music-and-computation-or-commonly-referred-to-as-computational-music-49d3311a95e2#.3kvpt9akq) and [Looking at Song Genres, Release Year, and Time Signature](https://medium.com/modeling-music/looking-at-song-genres-release-year-and-time-signature-ccf7a4c28e62#.n409y4q4r).\n\nHere is [the MSD data file]({{ site.url }}/media/MSDSubset-extracted-data.csv).  \nHere is [the genre and lyric data file]({{ site.url }}/media/MSDSubset-topics.csv).  \nAnd here is [the R script file]({{ site.url }}/media/dataCombine.R).\n\nAssuming you have [RStudio](https://rstudio.com) installed, simply download the two files to the same folder, open the R script in RStudio, and follow the instructions in that file. You can run each individual command by selecting the line and clicking 'run'. Or \u2015 since each command is a single line \u2015 you can simply place your cursor anywhere on the line you want to run, and then press CMD-Return on a Mac or CTRL-Enter on Windows (and probably Linux).\n\n## Further exploration\n\nIf you want to follow up with some other datasets and play around with them in a statistical analysis package like R, check out the [corpusmusic organization on GitHub](https://github.com/corpusmusic) that I set up for my collaborative research projects and those of my students. You'll find the source data and the code used to generate or clean it for the above tutorials, as well as some other interesting projects, some of which are ongoing.\n\nHave fun!\n",
                        "html": "",
                        "image": "/content/images/synapsePink.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Exploring musical data with R: An introduction to computational music analysis",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-08-24 16:02:00 -0600",
                        "created_by": 1,
                        "updated_at": "2016-08-24 16:02:00 -0600",
                        "updated_by": 1,
                        "published_at": "2016-08-24 16:02:00 -0600",
                        "published_by": 1,
                        "og_title": "Exploring musical data with R: An introduction to computational music analysis",
                        "twitter_title": "Exploring musical data with R: An introduction to computational music analysis",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/synapsePink.jpg",
                        "twitter_image": "/content/images/synapsePink.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Dr. Seuss, statistics, and the science of learning to read",
                        "slug": "dh-dr-seuss",
                        "markdown": "\nThis morning, my four-year-old was reading *Green Eggs and Ham* to me. I've heard that Dr. Seuss was intentional about using developmental psychology to inform how he wrote his books to help children learn how to read, and it is amazing to watch that process in action.\n\nThere are several things going on, but one that stood out to me today was the pace at which Seuss introduced new words to young readers. *Green Eggs and Ham* in particular seems to take advantage of the ways the brain forms new long-term memories to help children learn new words effectively.\n\nResearch in both cognitive science and in learning theory suggest that the optimal way to form a new, resilient long-term memory is to practice recall *just before you are about to forget.* If you wait longer, the memory will become inaccessible, and you will have to form it again from scratch. Don't wait long enough, and you'll be using short-term memory, not long-term memory, and thus not strengthening the right neural networks.\n\nWhat does this look like in practice? It looks like introducing a new word with several close repetitions \u2015 using short-term storage as working memory is figuring things out \u2015 and then gradually spacing occurrences of that word further and further apart as the story progresses. This helps the reader move their work from short-term to long-term memory, and accounts for the fact that each time the memory is strengthened it will take longer to (almost) forget.\n\nSince my research and upcoming courses have me thinking a lot about coding, statistical modeling, and cognition, naturally I wondered if I could model this process and analyze a few children's books. I didn't create a full-on statistical model, but I did come up with an analytical framework and write a script that would allow me to make some visualizations and compare books. The results were pretty cool.\n\nTL;DR version: Dr. Seuss *does* write (at least some) books consistent with research on memory formation, books that have a good chance of helping kids learn, and remember, new words. But not every book billed as \"learn to read\" or \"step into reading\" do the same.\n\n## The model\n\nTo analyze how well an author matches the memory-formation strategy described above, I decided to measure two things and look for trends: how many times a word occurs, and how close together. I wanted to note how these things changed over the course of the book. So for each word, I needed to know how long it had been since the previous occurrence, and what proportion that word represented of all the words read so far. Then I could simply plot these values over time and look for patterns.\n\nI decided to leave in *stop words* \u2015 articles, conjunctions, etc. \u2015 which are typically dropped from text analyses. They tend to be dropped because they pollute the data when looking for meaning. However, because stop words form a high proportion of the words a reader will encounter, and because they are often linguistically older and thus do not follow the \"rules\" of modern phonics (think *the*, *you*, *of*, etc.), they are important for young readers and thus may be points of focus in a book meant to help a child learn to read. I also did not *stem* words (treat *run*, *runs*, and *ran* as the same word) since, again, those differences may be the point of focus in learning to read.\n\n## The hypothesis\n\nWhat was I expecting?\n\nI was expecting that a book optimized for learning new words would contain certain key words (learning targets) whose distance between occurrences would increase over the course of the book, and whose proportion of occurrence (relative probability) would rapidly increase through the first few occurrences as the word was joining the repertoire, and then level off and gradually decrease as it spaced out and new words were added to the repertoire.\n\nI was expecting that a book aimed at children, *but not at teaching them to read,* would contain a similarly small repertoire of words, but would have more random-looking, or at least noisy, data in each of these domains.\n\nI wanted to test *Green Eggs and Ham*, of course, alongside a lesser-known \"learn-to-read\" book \u2015 I chose *Dragon Egg* by Mallory Loehr \u2015 and a children's book written more for the narrative \u2015 I chose Beatrix Potter's *The Tale of Peter Rabbit*.\n\n## The script\n\nMy work on [The Lieder Project](http://liederproject.shaffermusic.com) gave me a starting place \u2015 code that already does most of what I wanted to do. So I added some tweaks and functions, and created [this script](https://github.com/kshaffer/seuss) for processing a text file. Visit the script and accompanying data (original books omitted for copyright reasons, except for Potter) if you want to dig in yourself.\n\n## The fun stuff\n\nI loaded up the data files my script created in R and created some visualizations to see what patterns I might find.\n\nI began with the running probability of a word's occurrence given what had been read so far.\n\n![](/content/images/seuss/Potter-overallProb.png)\n\n![](/content/images/seuss/Loehr-overallProb.png)\n\n![](/content/images/seuss/Seuss-overallProb.png)\n\nNotice that while each book has a noisy band of low-probability words throughout, both of the learn-to-read books have separate bands of high-probability words that occur regularly. This looks to me like evidence of *learning targets* \u2015 words receiving emphasis beyond that of \"regular words.\"\n\nBut what are these words?\n\nHere are the most common words in each book:\n\n- Peter Rabbit: the, and, he, to, a (the most common non-stop words were peter, mr, mcgregor)  \n- Dragon Egg: the, rolls, egg, dragon, baby, flies (everything else occurred three times or less)  \n- Green Eggs: not, i, them, a, like (other high-probability words that are likely learning targets include you, would, eat, could, eggs, train, mouse, house, fox, box)\n\nI plotted the distance-since-previous-occurrence and the running-probability-of-occurrence for a number of these words. Here's what I found...\n\nFirst, as a baseline, the distance-since-previous-occurrence plots showed nothing but noise in *Peter Rabbit*. There was no discernible pattern in terms of how close/far occurrences of words like *peter*, *mcgregor*, *the*, or *and* were from each other. Most of the probability-so-far plots were likewise noisy. But some showed a pattern *opposite* to the proposed learning theory. For example, the probability of reading the word *peter* increases throughout the book.\n\n![](/content/images/seuss/Potter-peterProb.png)\n\nOn the other hand, I found one word that matched the learning theory hypothesis: *and*.\n\n![](/content/images/seuss/Potter-andProb.png)\n\nBut I'm fairly certain this is an anomaly. :)\n\n*Green Eggs and Ham*, on the other hand, showed evidence of several learning targets following the proposed learning theory. Here are a few of them (running probability of occurrence shown \u2015 an initial spike followed by a gradually descending tail matches the learning theory).\n\n![](/content/images/seuss/Seuss-eggsProb.png)\n\n![](/content/images/seuss/Seuss-likeProb.png)\n\n![](/content/images/seuss/Seuss-themProb.png)\n\nNot all words follow this model, but a number of them do, which makes sense if we assume that the book is meant for readers who already know some of them.\n\n*Dragon Egg* is interesting. It is part of a \"step into reading\" program from Random House that assigns levels to books based on their difficulty. While that system may help parents and kids identify appropriate books, this one doesn't seem meant to follow the learning theory we see associated with Seuss. In fact, five of the top six words *follow the opposite pattern of that suggested by the learning theory*. Here are a couple...\n\n![](/content/images/seuss/Loehr-eggProb.png)\n\n![](/content/images/seuss/Loehr-rollsProb.png)\n\nI don't mean this to be a jab at Loehr by any stretch of the imagination. There are many factors that go into word (density) choices when writing a children's book. However, the contrast between these three examples \u2015 for me, anyway \u2015 highlight the choices made by Seuss and the potential benefits they have for young readers.\n\nObviously, these are just three books, so let's not draw any conclusions here. But knowing what we know about Seuss's approach, seeing it in action with my kids as they learn to read, and then seeing this data visualized has given me a little more appreciation for these books, and for the wonder that is the brain of a child learning.\n",
                        "html": "",
                        "image": "/content/images/synapse.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Dr. Seuss, statistics, and the science of learning to read",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-01-18 20:39:00 -0700",
                        "created_by": 1,
                        "updated_at": "2016-01-18 20:39:00 -0700",
                        "updated_by": 1,
                        "published_at": "2016-01-18 20:39:00 -0700",
                        "published_by": 1,
                        "og_title": "Dr. Seuss, statistics, and the science of learning to read",
                        "twitter_title": "Dr. Seuss, statistics, and the science of learning to read",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/synapse.jpg",
                        "twitter_image": "/content/images/synapse.jpg"
                    },
                    {
                        "id": 0,
                        "title": "A journey through API programming \u2015 Part 2: Why APIs?",
                        "slug": "journey-through-api-programming-2",
                        "markdown": "\n*This is part of a series of posts in which I blog through my process of learning API programming in general and* [*the Medium API*](https://medium.com/blog/the-medium-api-is-now-open-to-everyone-3f4642e5c850#.8ehvndx6y) *in particular. For the beginning of this series, see* [*Part 1*](http://kris.shaffermusic.com/2016/08/journey-through-api-programming-1/)*.*\n\nAs I explained in Part 1, an API (Application Programming Interface) is the means by which apps talk to each other. If a human interacts with data on a server via a GUI (Graphical User Interface), an app interacts with data on (usually *another* server) via an API.\n\nSo why APIs? Why are they becoming so common? And why are they such valuable tools for (up-and-coming) developers?\n\n## Why are APIs becoming so\u00a0common?\n\nI\u2019m not a seasoned web developer. I\u2019ve been coding in some capacity for a number of years, but I\u2019m pretty new to web apps. However, I\u2019ve been researching them a lot, particularly since starting my new job (which involves a lot of web development), and several themes keep popping up.\n\nFirst, **people are increasingly working on the web rather than on their desktop.** And the more people work on the internet, the more their computers (or, at least, their browsers) need to talk to other computers and servers. That means APIs.\n\nSecond, the internet is a network of connections. This, of course, applies to people. But the more apps live on the web, the more likely they are to be networked to other apps. **Networked apps means APIs.** And since APIs facilitate both the connections between a user\u2019s computer and an app\u2019s server, *and* the connections between two apps\u2019 servers, these two kinds of connections reinforce each other. That is, once an app has an API for connecting to browser-based code, only small modifications are necessary to connect that app to *another app* on another server. And those API-based connections can make the integration seemingly seamless for the user.\n\n<img src=\"https://cdn-images-1.medium.com/max/2000/1*qco5IEeD2hequlEjszjW6g.png\" alt=\"Though APIs connect multiple code bases in the background, a user will only interact via a single graphical user interface (GUI). Good user interface/experience (UI/UX) design will mask that complexity from the\u00a0user.\" />\n\n<p style=\"text-align: center; font-size: 0.75em; font-style: italic\">Though APIs connect multiple code bases in the background, a user will only interact via a single graphical user interface (GUI). Good user interface/experience (UI/UX) design will mask that complexity from the\u00a0user.</p>\n\nThird, **API-based apps scale well.** I worked with the folks at [Trinket](https://trinket.io/) a couple years ago to develop a browser-based [music notation app](https://trinket.io/music) that we could integrate into [a web-based music textbook](http://openmusictheory.com). One thing I learned working with them is that if you make the server do all the hard work, adding users means adding (i.e., *buying*) more servers. Or, it means the app slows down, and you lose those users even before you can buy the servers. On the other hand, if you (like Trinket) let the browser do the computation, while the server gives it the data and the code to run, the server has less to do, and you can handle a lot more users on the same server(s). It also makes the app work faster for the user, since it\u2019s not continuously waiting for data to be sent back-and-forth between the browser and the server. That\u2019s a win-win. And so more and more startups are writing API-based apps, to keep costs down and user experience positive as they seek to grow their company. Of course, the same advantages make it a good option for indie projects.\n\nFinally, **in the age of mobile, APIs make it easier to build cross-platform apps.** We live in a time of myriad operating system, browser, and interface options. Think about developing a new app that you want to get into as many hands as possible. You have to write an iOS version, an Android version, a Windows Mobile version (though few do), a Windows desktop version, a Mac desktop version\u2026 oh, and if you skimp on the desktop versions by running it in a browser, the different (and sometimes outdated) browsers people use still aren\u2019t processing the same code the same way. And then there\u2019s backwards-compatibility issues\u2026\n\nThe big problem here is that just about every one of those operating systems expects a different programming language, especially if you want to make use of their latest unique features. That\u2019s a lot of work!\n\nBut, if you run the core of your app on a server and offer an API, you can write *one main code base* and then write simple apps for each platform that connect the particulars of the user interface with the API. You still build multiple mobile and desktop/browser versions, but those versions are much smaller, and when you fix a bug or add a feature to your core, you only have to do it once on your server. (And every user will have the latest update of the core, even if they don\u2019t update their app!)\n\nIn my mind, this is just a different flavor of the scaling problem. Instead of scaling to more users, an app may need to scale to a lot of interfaces. An API helps that.\n\n## Why should developers master\u00a0APIs?\n\nIf you work for, or want to work for, a company that builds with APIs, then of course you need to master APIs. That much is obvious.\n\nBut because of their scalability and compatibility, APIs are also great for indie developers, hobbyists, or people like me who are building apps more-or-less on your own, and with limited server resources. You can do a lot with a little, do the core work in the language you know best (rather than whatever Apple wants you to use this month), and you can avoid rolling out the same fix/feature multiple times on multiple platforms.\n\nPerhaps even more exciting, though, is the ability to build on others\u2019 work, and to let others build on our own work. As hackers, \u201c[We are unapologetic tinkerers](http://www.digitalpedagogylab.com/hybridped/open-source-scholarship/) who neither invent the wheel, nor are satisfied with the wheels already at our disposal.\u201d In the history of free open-source software, that has typically meant that we share code with each other, study each other\u2019s code, and copy/edit/remix it for new purposes. APIs help us extend that. Through public APIs, we can connect our projects to existing projects, not by taking their code, but by connecting our code to theirs. **APIs help us connect projects and build something bigger than either project would be on their own.**\n\nThat\u2019s the what and the why. **But how?**\n\nThat begins in [Part 3](http://kris.shaffermusic.com/2016/08/journey-through-api-programming-3/).\n\n*Header image by* [*Heather*](https://www.flickr.com/photos/58754750@N08/6137746194/) *(CC BY).*\n",
                        "html": "",
                        "image": "/content/images/apiHeader2.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "A journey through API programming \u2015 Part 2: Why APIs?",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-08-30 12:34:00 -0400",
                        "created_by": 1,
                        "updated_at": "2016-08-30 12:34:00 -0400",
                        "updated_by": 1,
                        "published_at": "2016-08-30 12:34:00 -0400",
                        "published_by": 1,
                        "og_title": "A journey through API programming \u2015 Part 2: Why APIs?",
                        "twitter_title": "A journey through API programming \u2015 Part 2: Why APIs?",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/apiHeader2.jpg",
                        "twitter_image": "/content/images/apiHeader2.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Trump, Russia, bots, and Breitbart: tl;dr edition",
                        "slug": "trump-russia-bots-breitbart",
                        "markdown": "\nI've been researching and publishing a lot lately on misinformation, disinformation, ad-tech, sockpuppets, bots, media literacy, election hacking, and the Trump administration. There's a lot there. And it's scary stuff. But in the midst of the current head-spinning, scandal-a-minute news cycle, I can understand how this stuff can quickly get overwhelming.\n\nSo here's the too-long-didn't-read version.\n\n> There are multiple, powerful forces waging a psychological information war against the American people. The Trump campaign (and administration) has made use of it and contributed to it, but they don't control it, and possibly don't fully understand it.\n\nA student of mine asked me if I *really* thought all the news about Russia, social media bots, propaganda, etc. was connected. After all, it sounds like a pretty unbelievable conspiracy theory.\n\nMy answer to her was: **\"There's not a single conspiracy behind it all. There are several conspiracies that, for a time at least, have overlapping interests.\"**\n\nAnd here are the several forces at work that I've uncovered in my research (with <a href=\"https://twitter.com/funnymonkey\" target=\"blank_\">Bill Fitzgerald</a> and my <a href=\"http://datafordemocracy.org/\" target=\"blank_\">Data4Democracy</a> colleagues) and what I've read from sources I trust:\n\n- A large, powerful actor on the level of a major state coordinated a massive disinformation campaign on social media in the spring and summer of 2016, with the goal of electing Donald Trump. <a href=\"https://medium.com/data-for-democracy/sockpuppets-secessionists-and-breitbart-7171b1134cd5\" target=\"blank_\">This powerful actor is likely Russia.</a>  \n- Far-right extremists, and anonymous individuals playing white nationalist collective \"for the lulz\", sought to use their digital media skills to elect Donald Trump. Some were <a href=\"http://www.politico.com/magazine/story/2017/03/memes-4chan-trump-supporters-trolls-internet-214856\" target=\"blank_\">extremists who believed his [White] America First message</a> (and want to bring the \"Make [whatever] Great Again\" message to Europe next), others were punks with nothing better to do than <a href=\"https://medium.com/@DaleBeran/4chan-the-skeleton-key-to-the-rise-of-trump-624e7cb798cb\" target=\"blank_\">prank the free world</a> and put liberal democracy at risk.  \n- Online media entities have hacked the pay-per-click advertising model of internet publishing by creating networks of sites that <a href=\"https://funnymonkey.com/2017/adtech-and-misinformation-the-middlemen-who-sell-to-all-sides\" target=\"blank_\">collect, share, and sell user data</a>. They have found that generating right-wing-friendly \"fake news\", or simply combining existing fake-news headlines with links to their own sites, is <a href=\"http://pushpullfork.com/2017/03/fake-news-adtech-misinformation/\" target=\"blank_\">a great way to generate clicks</a> \u2015 and therefore, cash.  \n- <a href=\"https://www.americanpressinstitute.org/publications/reports/survey-research/trust-social-media/\" target=\"blank_\">The psychological impact of casually scrolling</a> through all of these social media posts from Russia, white nationalist, diabolical pranksters, and ad-tech \u2015 *even if you never click on any of them* \u2015 is that the outlandish claims in them become more plausible, <a href=\"http://www.digitalpedagogylab.com/hybridped/truthy-lies-surreal-truths/\" target=\"blank_\">more \"truthy\"</a>, even more so <a href=\"https://www.americanpressinstitute.org/publications/reports/survey-research/trust-social-media/\" target=\"blank_\">when someone you know, trust, and care for shares them</a>. No matter our education, or our media literacy, we are *all* susceptible to this.\n\nMany of us want a single smoking gun to explain all of this. (And many of us want it to be Trump, Bannon, or Breitbart.) But the reality is that Trump/Bannon only control one of these forces, and only in part. To paraphrase William Gibson, *sometimes the simplest explanation is that there is more than one explanation*. Or to paraphrase any scholar worth their salt, *it's more complicated than it looks on the surface*.\n\nI was talking to Jonathon Morgan about this over a beer last night. (that was super-fun, let's do that again soon!) We both ruminated on how comforting it would be \u2015 frightening, but comforting \u2015 if there were a single cause, and if it were an American (even if it were the president or his chief of staff). When there's one cause, you can cut off the head. And when they're American, we have clear legal actions to take. When there's a singular cause and it's a foreign power, things are a lot more messy. When there are multiple causes and one of them is the problematic funding model of internet-based media \u2015 a model that <a href=\"https://blog.medium.com/renewing-mediums-focus-98f374a960be\" target=\"blank_\">the entire industry is trying to re-think and replace</a>, to no avail \u2015 the necessary response is far less clear.\n\nEven more frightening is that all of these root causes capitalize (literally) on the loss of America's collective grip on reality. **Gaslighting is their business model.**\n\nThere are actions we can take. (And I'll be posting in the future about some of those actions being taken by groups I'm working with.) But the first step in combating a mass gaslighting campaign is to take a breath and help everyone get oriented and grounded in the truth. More than anything, that's what I'm trying to do. Shine light in nasty, dark places so we know exactly where we are and what we're dealing with.\n\nBut sometimes even that is disorienting. There are so many nasty things, we just don't know what we can do. I'm spending a lot of time researching them, and it's hard for me to keep it all in my head. So hopefully this post will help. I wrote it as much for myself as anyone.\n\nKeep fighting the good fight.\n\n<i>Header image by <a href=\"https://unsplash.com/photos/E0Spm6XXn2Y\" target=\"blank_\">Sergey Zolkin</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/oldKeyboard.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Trump, Russia, bots, and Breitbart: tl;dr edition",
                        "meta_description": "There are multiple, powerful forces waging a psychological information war against the American people.",
                        "author_id": 1,
                        "created_at": "2017-03-31 14:11:00 -0400",
                        "created_by": 1,
                        "updated_at": "2017-03-31 14:11:00 -0400",
                        "updated_by": 1,
                        "published_at": "2017-03-31 14:11:00 -0400",
                        "published_by": 1,
                        "og_title": "Trump, Russia, bots, and Breitbart: tl;dr edition",
                        "twitter_title": "Trump, Russia, bots, and Breitbart: tl;dr edition",
                        "og_description": "There are multiple, powerful forces waging a psychological information war against the American people.",
                        "twitter_description": "There are multiple, powerful forces waging a psychological information war against the American people.",
                        "og_image": "/content/images/oldKeyboard.jpg",
                        "twitter_image": "/content/images/oldKeyboard.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Three-dimensional calculations in Python 3",
                        "slug": "three-dimensional-calculations-python",
                        "markdown": "\nI have to do some Python coding for [The Lieder Project](http://liederproject.shaffermusic.com) that involves Euclidean distance and variance in three-dimensional space. I'm positive that modules for these things exist out there somewhere, but I couldn't find them quickly. So I wrote the code myself. In case you also need to calculate distances between points in three-dimensional space, and/or the mean locations and standard deviation for points in three-dimensional space, here is the code for Python 3.\n\nFirst, I created a class for three-dimensional objects.\n\n~~~ python\nclass threedim(object):\n    def __init__(self, x, y, z):\n        self.x = x\n        self.y = y\n        self.z = z\n~~~\n\nThe following function measures the Euclidean distance between two points in three-dimensional space:\n\n~~~ python\ndef threedimdistance(i, j):\n    deltaxsquared = (i.x - j.x) ** 2\n    deltaysquared = (i.y - j.y) ** 2\n    deltazsquared = (i.z - j.z) ** 2\n    return (deltaxsquared + deltaysquared + deltazsquared) ** 0.5\n~~~\n\nTo use these, declare new points in three-dimensional space:\n\n~~~ python\nlineone = threedim(1,1,1)\nlinetwo = threedim(1,1,0.5)\nlinethree = threedim(0,0,0)\nlinefour = threedim(0.5, 0.5, 0.5)\n~~~\n\nand use threedimdistance() to measure the Euclidean distance between any two of them:\n\n~~~ python\nthreedimdistance(lineone, linetwo)\n~~~\n\nTo find the mean position and standard deviation of a list of points in three-dimensional space, first declare the following functions:\n\n~~~ python\ndef threedimmean(threedimlist):\n    xvalues = []\n    yvalues = []\n    zvalues = []\n    for point in threedimlist:\n        xvalues.append(point.x)\n        yvalues.append(point.y)\n        zvalues.append(point.z)\n    xmean = sum(xvalues)/float(len(xvalues))\n    ymean = sum(yvalues)/float(len(yvalues))\n    zmean = sum(zvalues)/float(len(zvalues))\n    return threedim(xmean, ymean, zmean)\n\ndef threedimSD(threedimlist):\n    squareddistances = []\n    listmean = threedimmean(threedimlist)\n    for point in threedimlist:\n        squareddistances.append(threedimdistance(point, listmean) ** 2)\n    return (sum(squareddistances)/float(len(squareddistances)) ** 0.5)\n~~~\n\nThen create a list of points in three-dimensional space (from points already created \u2015 see above) and call the above functions on that list:\n\n~~~ python\nlistoflines = [lineone, linetwo, linethree, linefour]\nthreedimmean(listoflines)\nthreedimSD(listoflines)\n~~~\n\nThat's it!\n\nUnless I missed something?\n",
                        "html": "",
                        "image": "/content/images/structure.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Three-dimensional calculations in Python 3",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-03-26 22:40:00 -0600",
                        "created_by": 1,
                        "updated_at": "2016-03-26 22:40:00 -0600",
                        "updated_by": 1,
                        "published_at": "2016-03-26 22:40:00 -0600",
                        "published_by": 1,
                        "og_title": "Three-dimensional calculations in Python 3",
                        "twitter_title": "Three-dimensional calculations in Python 3",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/structure.jpg",
                        "twitter_image": "/content/images/structure.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Emergence and instructor guidance",
                        "slug": "emergence-and-guidance",
                        "markdown": "\nLee Skallerup Bessette published a blog post today, [Teaching and Learning as Paying Attention,](http://readywriting.org/uncategorized/teaching-and-learning-as-paying-attention/) which I very much appreciated. Lee writes:\n\n> I believe very deeply in empowering students to take ownership of their learning. But I also believe in my role as someone who facilitates a space where they are confronted with the limits of their knowledge, of their perspective, of their worldview. This class, then, for me, is an opportunity for all of us to pay attention, notice, and then create and learn.\n\nThis point really resonated with me, in large part because of some discussions I've had recently about predetermined learning outcomes, emergent learning, and \"content\" in higher education (most recently with Sean Michael Morris and Jesse Stommel on Twitter). Typically, an educator who, like me, espouses *critical pedagogy*, is pushing for less emphasis on content and predetermined learning objectives. However, when discussing educational philosophy \"in-house\" with other critical pedagogues, I've found myself increasingly defending the idea of predetermined content objectives. This is not because I believe that \"content is king,\" or anything of the kind. This is because I believe that the kind of emergent learning we seek is facilitated by the presence of a large, diverse base of knowledge within the learning community, and that, as Lee expresses, the critically minded agency we seek to foster in our students can often be best facilitated by the careful, expert guidance of an instructor.\n\nI wrote about my educational goals in my blog post, [Student-centered curriculum:](http://kris.shaffermusic.com//2014/12/student-centered-curriculum/)\n\n> the goal of education is to empower and equip students to be critically minded agents. While instructor guidance may be appropriate, the ultimate goal is for students to be capable of making these decisions.\n\nAnd with a bit more detail about \"content\" (from [Why grade?](http://kris.shaffermusic.com//2014/07/why-grade/)):\n\n> [M]y purpose as an educator is largely not to instill content knowledge. My goal is for my students to learn how to learn, and to gain skills that will allow them to continue learning independently once the course is over. Those skills will certainly require content knowledge or, at least, familiarity, but content is important primarily to the extent that it supports students' intellectual growth within the discipline.\n\nFor me, content knowledge is absolutely not the end goal. The goal is independence, agency, intellectual maturity, an ability to...\n\n> internaliz[e] the standard, accepted ways of doing things in such a robust way that we become aware of all the holes in those modes of thinking, the connections between seeming disparate things, the possibilities (and impossibilities) of what new things can be done... or at least should be tried ([Backwards design in education](http://kris.shaffermusic.com//2015/08/backwards-design/)).\n\nWhile I agree with Sean and Jesse that we should be suspicious of every claim regarding the importance of content, content knowledge plays an important role in accomplishing these critical pedagogical goals.\n\nSo does instructor guidance. In [Designing for Emergence: The Role of the Instructor in Student-Centered Learning,](http://www.hybridpedagogy.com/journal/designing-emergence-role-instructor-student-centered-learning/) Mary Stewart writes:\n\n> [A]uthority also exists in student-centered classrooms, and that\u2019s not necessarily a bad thing. ...\n\n> The reality is that the instructor always designs an environment that allows for emergence. No matter how much we may want it to be the case, the environments in which students guide themselves do not spontaneously spring into existence in the courses that most of us teach because students do not come to our classes in the same way they would come to a community of practice. Instead, such environments are carefully designed and the instructor presence within them is deliberately crafted. It also requires a considerable amount of skill and finesse to be a teacher in these environments \u2014 as Keith Brennan has argued, \u201cmany of Connectivism\u2019s finest practitioners \u2026 model and cheerlead. They scaffold and support.\u201d They are active participants and leaders in the courses they teach.\n\nWhat Mary describes here is exactly my experience. While I'd love for my classes to immediately take shape as emergent, collaborative learning environments, that often takes time and hard work, from me and from the students. And for profound epiphanies to take place, we need a strong base of content to build on, critique, and at times, reject. But, again, this content \u2015 even the instructor \u2015 is not central.\n\nUltimately, designing a student-centered, even student-driven, course aimed at emergent, epiphanal learning is really hard. And it's easy to slip into old instructor-centered habits. These kind of learning environments don't \"spontaneously spring into existence.\" This kind of pedagogy takes both nuance in design, and a lot of self-discipline in implementation. And this is the expertise that teachers really need to cultivate, and then bring into the classroom.\n\n\n## Related posts\n\n- [Academic freedom is for students, too](/2015/04/academic-freedom-is-for-students/)  \n- [Starting off with critical pedagogy](/2014/09/starting-off-with-critical-pedagogy/)  \n- [More humanity in the humanities](/2014/11/more-humanity-in-the-humanities/)  \n",
                        "html": "",
                        "image": "/content/images/lemmeout.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Emergence and instructor guidance",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-01-18 19:38:00 -0700",
                        "created_by": 1,
                        "updated_at": "2016-01-18 19:38:00 -0700",
                        "updated_by": 1,
                        "published_at": "2016-01-18 19:38:00 -0700",
                        "published_by": 1,
                        "og_title": "Emergence and instructor guidance",
                        "twitter_title": "Emergence and instructor guidance",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/lemmeout.jpg",
                        "twitter_image": "/content/images/lemmeout.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Student-centered curriculum",
                        "slug": "student-centered-curriculum",
                        "markdown": "\nWhat does a student-centered music theory curriculum look like?\n\nThis question has been posed to me a few times on Twitter by David Kulma. It has also come up in various conversations on critical pedagogy, as well as a recent debate at the Society for Music Theory over the AP music theory exam. Most recently, Jesse Stommel published an article on Hybrid Pedagogy, [\"Critical Digital Pedagogy: A Definition\"](http://www.hybridpedagogy.com/journal/critical-digital-pedagogy-definition/), which reignited the discussion on Twitter. Add to all this that the CU music theory department is currently collaborating on a revision to the topics and skills covered by the four-semester theory and aural skills core curricula.\n\nMuch of what we discuss about music theory pedagogy involves incremental changes and minor improvements. But improvements to what end? What is our goal? If we could design a student-centered music theory curriculum completely from scratch, what would it look like?\n\nI think the answer is impossible.\n\nFirst, a student-centered curriculum cannot exist. That is, if what we mean by curriculum is a consistent set of concepts, skills, activities that every student pursues, that curriculum is by definition content-centered. Because of the diversity of our students, their backgrounds, and their goals, there is no way that a universal progression of topics can be fully directed around that diversity.\n\nSecond, a student-centered curriculum is not determined entirely by faculty. At least from the perspective of critical pedagogy, the goal of education is to empower and equip students to be critically minded agents. While instructor guidance may be appropriate, the ultimate goal is for students to be capable of making these decisions. A student-centered curriculum at the very least will help bring students to that autonomy. At the most, a student-centered curriculum will involve students in major decision-making throughout. Thus, the idea of faculty determining the \"best practice\" for creating a student-centered curriculum is foundationally flawed.\n\nFinally, I think that a student-centered music curriculum doesn't cordon off music theory on its own. The separation of discrete subjects is an industrialist practice, one that, as John Dewey would say, poses the student *against* the curriculum. A student-centered education will not divide subjects according to external logic, but will order studies in a way that supports and is supported by the student's interests, abilities, goals, and even their existing social environment. Separating music theory from aural skills from performance from improvisation from composition from writing from mathematics from science, etc., imposes an external logic that precludes both student agency and student orientation. A student-centered curriculum does not arrange content into subjects to be imposed on the student-as-object. A student-centered curriculum treats the student as a subject, and the materials as the objects to be engaged, manipulated, even constructed by the student-as-subject.\n\nIn light of this, I don't see the ultimate goal of our incremental changes to be the creation of a singular, student-centered music theory core curriculum. The ultimate goal is an entirely new way of approaching \"school\" for musicians \u2014 one that empowers the student as agent, and guides them through their own intellectual and musical development, helping them to navigate the world of music via a logic (or lack thereof) that makes sense for their own interests, experiences, and goals. And helps them to rethink their goals. (Whether that can be done in the context of existing university structures is an open question.)\n\nAll of that said, there are some things that we can do to make the theory core, as it currently exists, into something more student-centered.\n\nJesse writes that \"A Critical Digital Pedagogy demands that open and networked educational environments must not be merely repositories of content. They must be platforms for engaging students and teachers as full agents of their own learning.\" He offers four broad things that characterize this pedagogy. It...\n\n- centers its practice on community and collaboration;  \n- must remain open to diverse, international voices, and thus requires invention to reimagine the ways that communication and collaboration happen across cultural and political boundaries;  \n- will not, cannot, be defined by a single voice but must gather together a cacophony of voices;  \n- must have use and application outside traditional institutions of education.\n\nThere are a number of things that we can do in the typical music theory core that accomplish these purposes, even without changing the content.\n\nFor instance, to foster community and collaboration, we can involve ensemble performance, collaborative analysis and writing projects, or even small things like think-pair-share activities. Faculty attending student performances and inviting small groups for coffee can also foster another kind of valuable community. Music is not, should not, be an isolated activity, and community-building can easily be part of our music theory classes if we think beyond the workbook activities that come with standard textbooks. In fact, many music theory instructors already do these kinds of things.\n\nAside: the more diverse our classes are, the more difficult outside-of-class collaboration can be. Not only do some students prefer (and thrive on) individual work, but a diversity of living situations, ensemble commitments, and work schedules can make major collaborative projects difficult for some, even discriminatory. I tend to focus most collaboration into class time, or assign small or optional out-of-class collaborations. This allows students to reap the advantages of collaboration while still developing their ability to work individually and not hindering the learning of students who live off-campus, work 20 hours per week, and/or have introverted personalities.\n\nMaking room for diverse and international voices is more difficult. At the very least, we should be judicious about the kinds of assignments we require of students. For example, requiring a written essay when the learning goal has nothing to do with writing may discriminate against international students who are still building fluency in English, or students with disabilities or differences that affect their writing but not their music. Generally speaking, offering a diversity of \"ways in\" to the musical material and a diversity of ways to assess knowledge and skills is important for us to serve our diverse students.\n\nGiving voice to that diversity is harder. But I have found that there are often situations where different students have unique experiences or expertise that they can use to lead the class through something. For example, a music-English double-major could be asked to lead class exploration of poetry set to music. We could even ask them to help us choose and plan class activities. Asking students to write (text or music) for each other, rather than just for the instructor, also helps give voice to students of different backgrounds. And while it may be uncomfortable, it gives students an opportunity to grow socially, as they respond to each other \u2014 and not always in respectful ways, by default.\n\nNot centering class around a single voice is a related idea. And music analysis and (model) composition are easy domains in which we as the instructors can step back and let student ideas be voiced and heard. (See [my previous blog post](http://kris.shaffermusic.com/2014/12/lecture-in-a-flipped-class/) for ideas on how to enable this without completely abandoning lecture.)\n\nFinally, and often most problematic in music theory, Jesse advocates relevance of class activity outside the educational institution. We can start by [de-emphasizing part writing](http://www.flipcamp.org/engagingstudents2/essays/kulmaNaxer.html), making room for other activities that are more directly related to modern musical experience. But even without making major content changes, we can add substantial relevance. For example, we could add to major composition projects the option to arrange a piece for a non-standard ensemble. This is a useful and money-making ability for gigging musicians, especially for those playing and contracting for weddings. Writing program notes or blog posts explaining music, rather than an academic essay, can help students hone their skills communicating about music to non-musicians \u2014 another important skill for professional musicians that still allows students to engage the theory \"content\" in the same level of detail. In many ways, communicating complex ideas about music simply and without jargon is *more* challenging. Additionally, using a piece being performed by the college orchestra, choir, opera, etc. in class can help students connect course content to other professional activity \u2014 and can empower them to contradict both the theory instructor and the orchestra conductor, as the students are in the midst of engaging the piece from multiple perspectives simultaneously.\n\nNone of this is to discount rethinking content. As I tweeted earlier this week, focusing on critical thinking over content is no excuse for irrelevant content. We can empower students as agents *while* engaging music and activities that are more likely than others to be interesting or professionally relevant. But we usually cannot change the content overnight, and as long as we teach a broad diversity of students in each class, we cannot select content that is equally relevant for everyone. Even if our students were identical, we don't know what will be professionally relevant for them in 30 years anyway. Thus, in order to serve our students well, we must think far beyond simple content decisions.\n\nBut developing a curriculum that is student-centered is far more than a negotiation of content. In fact, content decisions will be nearly meaningless if we don't address student backgrounds, interests, and goals. And all of that will be meaningless if treat students as objects of *our* work instead of subjects in their own right.\n\nMusicians commonly talk about \"art for art's sake\" and working \"in service of the music.\" That's nonsense. People make music for people \u2014 even if primarily for themselves. As teachers, in particular, it is the work of our students, not the work of dead composers, that should be our primary focus. That's what it means to be student-centered. More importantly, that's what it means to be an educator.\n",
                        "html": "",
                        "image": "/content/images/pegs.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Student-centered curriculum",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2014-12-06 17:43:08 -0700",
                        "created_by": 1,
                        "updated_at": "2014-12-06 17:43:08 -0700",
                        "updated_by": 1,
                        "published_at": "2014-12-06 17:43:08 -0700",
                        "published_by": 1,
                        "og_title": "Student-centered curriculum",
                        "twitter_title": "Student-centered curriculum",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/pegs.jpg",
                        "twitter_image": "/content/images/pegs.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Something is rotten in the state of Arizona",
                        "slug": "omething-rotten-arizona",
                        "markdown": "\nLast night, President Trump gave a speech in Phoenix that sounded a lot like his campaign speeches in 2016. *Build a wall... Radical Islamic terrorism... Crooked media...*\n\nBut something else pernicious jumped out at me as I read reports (and <a href=\"http://time.com/4912055/donald-trump-phoenix-arizona-transcript/\" target=\"blank_\">the transcript</a>) of his speech.\n\n> It's time to expose the crooked media deceptions, and to challenge the media for their role in fomenting divisions. And yes, by the way \u2015 and yes, by the way, they are trying to take away our history and our heritage.\n\nThat last sentence \u2015 \"trying to take away our history and our heritage\" \u2015 is clear white nationalist rhetoric. And that's incredibly frightening coming from a sitting US president.\nBut what jumped out at me was the first sentence, specifically the phrase **\"fomenting divisions\".**\n\n<a href=\"/content/images/bigrams_logodds_10.png\" target=\"blank_\" target=\"blank_\"><img src=\"/content/images/bigrams_logodds_10.png\" alt=\"Most distinctive bigrams shared on #unitetheright by low- and high-volume accounts\" /></a>\n\nIn <a href=\"http://pushpullfork.com/2017/08/twitter-propaganda-during-unite-the-right/\" target=\"blank_\">my blog post last week</a>, I noted a number of two-word phrases that were distinctive of high-volume Twitter accounts (<a href=\"https://medium.com/data-for-democracy/spot-a-bot-identifying-automation-and-disinformation-on-social-media-2966ad93a203\" target=\"blank_\">bots, sockpuppets, and the like</a>) in tweets tagged #unitetheright, during and surrounding the Charlottesville rally and terror attack. The phrase most distinctive of high-volume accounts when contrasted with \"regular\", low-volume users was **\"media fomenting\".** Two other phrases among the most distinctive of high-volume accounts were **\"fomenting anger\"** (#5) and **\"fomenting division\"** (#9).\n\n*Fomenting divisions* is a new phrase for Trump. The word \"foment(ing)\" never appears in his tweets, and it appears only three times in his statements curated by <a href=\"http://www.presidency.ucsb.edu/\" target=\"blank_\">The American Presidency Project</a>. Until last night, he only used \"foment\" to refer to violence and terrorism, and only once to refer to Americans (those promoting \"anti-police\" attitudes and actions in Ferguson). He has not used it in any official speech or statement during the past six months.\n\nNow the *idea* of the press fomenting division is not new for Trump. He's been playing variations on the *L\u00fcgenpresse* theme his entire campaign. (*L\u00fcgenpresse* was a popular propaganda term in Nazi Germany. It literally means \"lying press\". Trump tends to use English variants of it \u2015 \"lying media\", \"crooked media\", and the like. Only hard-core white nationalists like Richard Spencer <a href=\"http://www.cnn.com/2016/11/21/politics/alt-right-gathering-donald-trump/index.html\" target=\"blank_\">use the original German</a> these days.)\n\nIt is incredibly disturbing that a white nationalist, Nazi-originating idea saturates Trump's rhetoric (and, it seems, his political outlook in general). But I shudder at the fact that a largely automated, Twitter-based, disinformation campaign may be affecting the president's speaking points \u2015 and so rapidly.\n\n## Antifa\n\nThe media fomenting division isn't the only phrase from that Twitter campaign that made its way into Trump's speech last night. Immediately after the Charlottesville terror attack, President Trump decried the violence and hatred \"on many sides\". He later used the term \"alt-left\" to describe what he sees as the aggressive, violent strain of leftist politics. (I don't have the data to know if this term originated with him, or elsewhere.) But last night, for the first time, President Trump directly called out the **Antifa** movement.\n\n> How about -- how about all week they're talking about the massive crowds that are going to be outside. Where are they? Well, it's hot out. It is hot. I think it's too warm.You know, they show up in the helmets and the black masks, and they've got clubs and they've got everything -- Antifa!\n\nAntifa is one of the most used words in the #UniteTheRight corpus I collected, and even more so in the tweets I collected (but haven't published) from the Boston \"Free Speech\" rally the following weekend. However, Trump has never tweeted the word \"Antifa\", nor does that word occur in any speech (including campaign speeches), radio address, policy statement, etc. curated by the American Presidency Project before last night's Phoenix rally.\n\nThe Antifa (anti-fascist) movement <a href=\"https://jacobinmag.com/2017/05/antifascist-movements-hitler-nazis-kpd-spd-germany-cold-war\" target=\"blank_\">originated in 1930s Central Europe</a>, as a unifying movement of communists and social democrats who opposed the rise of national socialism (Nazism). While they are experiencing growth today in opposition to the rise of far-right movements in North America and Europe, they are still a relatively small movement. And though the far-right portrays them as a violent group \u2015 and they *are* more willing to engage in physical confrontation than most leftist protesters \u2015 very little Antifa violence has actually taken place in the US.\n\nThe far right has latched onto Antifa as the focus of their anger. Antifa gives them a target for their ire, as well as a channel through which to portray themselves as victims. And <a href=\"https://www.patreon.com/posts/13515855\" target=\"blank_\">that victim narrative is essential</a> to the white nationalist recruiting message, as well as Trump's campaign. So while Trump's party controls two (arguably three) branches of the federal government, both the \"crooked\" media and \"violent\" left are an essential backdrop to the Trump (and white nationalist) message.\n\nIn addition to these key phrases about the media and Antifa, Trump also began his speech by drawing attention to the activity of the anti-Trump protesters outside the arena. This narrative shift \u2015 latching onto any violence, when possible, and <a href=\"http://www.msn.com/en-us/news/us/ap-fact-check-viral-photo-doesnt-show-antifa-beating-cop/ar-AAqcDqT?li=BBmkt5R&amp;ocid=spartanntp\" target=\"blank_\">making it up</a> (or noting the lack of a police permit), when not \u2015 is also common in far-right disinformation campaigns. It further portrays Trump and his people as embattled victims, and using this to begin and frame his speech allows him to capitalize on that victim narrative throughout.\n\n## White nationalism\n\nI've noted two problems so far: 1) President Trump's appropriation of (toned down) white nationalist rhetoric, and 2) the ease and speed with which a social-media-based disinformation campaign can impact Trump's agenda. But there's a third problem:\n\n***This social media campaign is an unrepresentative sample of his base that tilts towards the extreme.***\n\nAs I note in my post, <a href=\"http://pushpullfork.com/2017/08/twitter-propaganda-during-unite-the-right/\" target=\"blank_\">Twitter propaganda during 'Unite the Right'</a>, \"The top 10% of accounts by tweet volume account for approximately 50% of the tweets, and the top 5% of accounts generated approximately 37% of the tweets.\" And this is not the first time my research colleagues and/or I have seen such disproportion. As a rule, a very small number of accounts, which tend to show <a href=\"https://medium.com/data-for-democracy/spot-a-bot-identifying-automation-and-disinformation-on-social-media-2966ad93a203\" target=\"blank_\">signs of automation</a> (e.g., bots), generate the large majority of tweets \u2015 far more than we would expect from a typical <a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\" target=\"blank_\">Zipf distribution</a>.\n\nPerhaps more importantly, I noted that \"the highest-volume accounts are disproportionately pushing sites known for spreading **disinformation, misinformation, and propaganda**\", and \"the high-volume accounts are generally pushing the narrative of **the alt-right, white nationalist agenda** of the #unitetheright organizers.\"\n\nIn other words, President Trump seems to be building his key talking points from tweets generated by a small number of automated accounts that push an extremist agenda and share information from discredited sources.\n\nOr put more starkly, ***white nationalist propaganda seems to exert a significant influence on President Trump's values and rhetoric.***\n\nAnd that's truly frightening.\n\n<i>Note: the Twitter scrape and analysis were conducted with an extension of my <a href=\"https://github.com/kshaffer/tweetmineR\" target=\"blank_\">tweetmineR</a> utility for Python and R. Trump's speeches and statements were downloaded and analyzed with my <a href=\"https://github.com/kshaffer/presidencyproject\" target=\"blank_\">presidencyproject</a> scripts for R.</i>\n\n<i>Header image by <a href=\"https://www.pexels.com/photo/brown-cardboard-robot-artwork-176842/\" target=\"blank_\">InstaWalli</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/brownbot.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Something is rotten in the state of Arizona",
                        "meta_description": "White nationalist propaganda seems to exert a significant influence on President Trump's values and rhetoric.",
                        "author_id": 1,
                        "created_at": "2017-08-24 19:23:48 -0400",
                        "created_by": 1,
                        "updated_at": "2017-08-24 19:23:48 -0400",
                        "updated_by": 1,
                        "published_at": "2017-08-24 19:23:48 -0400",
                        "published_by": 1,
                        "og_title": "Something is rotten in the state of Arizona",
                        "twitter_title": "Something is rotten in the state of Arizona",
                        "og_description": "White nationalist propaganda seems to exert a significant influence on President Trump's values and rhetoric.",
                        "twitter_description": "White nationalist propaganda seems to exert a significant influence on President Trump's values and rhetoric.",
                        "og_image": "/content/images/brownbot.jpg",
                        "twitter_image": "/content/images/brownbot.jpg"
                    },
                    {
                        "id": 0,
                        "title": "So you want to be a music theory professor...",
                        "slug": "music-theory-job-market",
                        "markdown": "\nHow many tenure-track professorships are available each year in music theory? How many music theory PhDs graduate and go on the market each year? How many people apply for the typical music theory faculty position?\n\nThese questions are essential for graduate students, and those considering graduate school, to explore. A PhD is a major life investment, even if it is funded, and it's important to have the facts at the outset. But this information is difficult to find, and it is often difficult to compare institutions' claims about job placement when they are available, as institutions sometimes \"count\" things differently. (For example, does that 100% job placement include those who left the profession? And did they do so because of bleak job prospects? Perhaps even after a couple years on the market?) Job postings often don't help much, either, as they don't hang around long enough to do multi-year studies.\n\nBut we can get a little bit of an idea of what the job market for music theorists looks like, thanks to some crowdsourcing work on the [Music Theory/Composition Job Wiki.](http://academicjobs.wikia.com/wiki/Music_Theory/Composition_2016-17) This data is certainly not complete, but it's the best *single* source for job postings in music theory, and many of those postings are accompanied on the wiki by the name of the person who won the position, their PhD-granting institution, the year they finished the PhD, and where they are currently employed. As with any real-life data set, there are missing entries, and missing data for some of the entries included. However, it's a start. And hopefully, as humanities jobs in higher ed become scarcer and more precarious in general, the Society for Music Theory (or some other research group) can collect information that paints a more complete picture.\n\nBut for now, here are some trends we can see on the wiki...\n\n\n## How many (tenure-track) jobs are there? ##\n\nFor jobs beginning in the academic years 2014\u201315, 2015\u201316, and 2016\u201317, there are a total of **116 *theory only* jobs** in the US and Canada on the wiki. (To keep things clean, I'm leaving off theory/composition, theory/musicology, and theory/applied positions. While there are a large number of those, a given individual will only qualify for a small subset of them, and will also have additional competition from non-theorists.) That's roughly **39 theory jobs per year.**\n\nBut you're after a *tenure-track* position, you say? The good news is that most of those jobs are tenure-track. There are **78 tenure-track jobs** in theory only listed on the wiki (two-thirds of the total jobs). However, that only comes to **26 tenure-track jobs in music theory per year.**\n\nT W E N T Y - S I X\n\nIt wouldn't be difficult for the 5\u20137 largest graduate programs in North America to produce that many PhDs each year. But, of course, there are more than 5\u20137 programs. And there are people who graduated in the past few years still looking for a job. And people with a job looking for a new one...\n\n\n## Who gets the jobs?\n\nWhere did those lucky 26 people get their PhDs? What were there research areas? Are some demographics favored over others on the market? Again, there's not a lot of data assembled. But we can find some interesting starting points on the wiki, looking at PhD-granting institutions.\n\nOf the 116 theory-only jobs on the wiki from the past three years, only 66 list the job winner and their PhD-granting institution. That may or may not be a representative sample \u2014 there may be certain types of jobs and/or graduate schools where students/candidates are less likely to report their new job on the wiki or be reported by a colleague. That caveat in place, here are the numbers.\n\nHere is the PhD-granting institution breakdown for candidates winning jobs (tenure-track and non-tenure-track together):\n\n| PhD-granting institution | jobs won by graduates in 2014\u201316 |\n| ---: | :--- |\n| Eastman | 9 |\n| Florida State University | 8 |\n| CUNY | 7 |\n| Yale University | 7 |\n| Indiana University (incl. 1 DMA) | 5 |\n| UT\u2013Austin | 4 |\n| Northwestern University | 3 |\n| University of Michigan | 2 |\n| University of Minnesota | 2 |\n| University of Washington | 2 |\n| institutions with one job won | 17 |\n| jobs won with no PhD institution listed on the wiki | 50 |\n\nAnd for tenure-track assistant professor (or equivalent) only:\n\n| PhD-granting institution | jobs won by graduates in 2014\u201316 |\n| ---: | :--- |\n| CUNY | 6 |\n| Eastman | 6 |\n| Florida State University | 5 |\n| Yale University | 5 |\n| Indiana University | 4 |\n| UT\u2013Austin | 4 |\n| Northwestern University | 2 |\n| University of Michigan | 2 |\n| University of Minnesota | 2 |\n| institutions with one job won | 11 |\n| jobs won with no PhD institution listed on the wiki | 31 |\n\nThere are 47 tenure-track jobs for which the wiki lists the winner's PhD-granting institution. Notice that **55% come from the top five institutions.** In fact, **25% of them come from just two schools** in a single US state \u2014 CUNY and Eastman.\n\nIt's worth noting that in the case of Eastman, FSU, and Indiana, these are big programs \u2014 lots of faculty, lots of students. It makes sense that they would be high on the list. However, Yale and especially CUNY are fairly small programs, with 2\u20133 students graduating with music theory PhDs most years. I don't have data on graduation/matriculation ratios, time to degree, or job placement from these institutions, so don't take an institution's being high on this list as a recommendation from me for graduate study \u2014 nor the opposite. There are a lot more factors (and some missing data points) to consider when making a decision about whether, where, and when to attend graduate school.\n\n\n## How quickly do music theorists get jobs?\n\nThis is a pressing question often overlooked in these discussions. I had a family when I graduated \u2014 a wife (who worked), two young kids, and a condo to sell. Especially when working outside of academia between the PhD and the job application can be a point against you in some eyes, going straight from grad school to at least a decent \"starter\" job is a high priority for many. And if you already have a family that you don't want to move across the country three times in as many years, a stable job at graduation is incredibly important.\n\nFor all the jobs on the wiki where PhD date is listed (plus a few that I knew personally and was able to add without doing extra research), the median graduation-to-job time was 2 years. (Keep in mind that some people are on here twice, getting a 1- or 2-year gig right away, then a more permanent position later.) Seven people got jobs the same year (SEVEN!) as their PhD, and the duration went all the way up to 20 years (for a senior scholar landing a senior gig). If we limit it to tenure-track assistant professorships (or equivalent, as best as I can tell), here's the breakdown:\n\n![](/media/phdtojob.png)\n\nFour candidates **(FOUR!)** won a job the same year they received their PhDs, and seven more won it a year after receiving their PhDs. The median is still two years.\n\nNow there are many reasons why someone might win a tenure-track assistant professorship seven years after receiving their PhD. They may have had a tenure-track gig for several years, but wanted to move to a better institution, or closer to family, or where there are better professional opportunities for their spouse, etc. However, it is important to note how few of these 28 jobs are won by people graduating that year \u2014 just four \u2014 and how after four years post-graduation, there are very few people winning tenure-track gigs. The missing data may totally change this profile, of course. But if this is at all representative, those are important things to consider.\n\n\n## What to do?\n\nAgain, this is incomplete data, so please don't base any major life decisions solely on the charts and tables in this post! However, I do strongly encourage anyone pursuing or thinking about pursuing a PhD in music theory \u2014 or considering a move to a new job \u2014 to get as much of this data as early in the process as possible. I also encourage those who are recruiting and advising graduate students to help them find this information, too. And notice that I left out the theory-and-something-else jobs. There are a large number of those. (Aspiring) theorists who want to brave this market should take time to build cognate skills and forge interdisciplinary collaborations. These will expand the job possibilities within music academia, as well as help you explore other ways to apply your music analysis, writing, and teaching skills in other domains where the jobs may be more plentiful.\n\nLet me emphasize that last point. We tend to think of getting a PhD in a field, but not landing a tenure-track gig in that field, as a failure. Nothing could be further from the truth. There are numerous fiscal and political factors that have led to an increasing number of those scenarios in the past few years. It's not true that \"all good people get jobs\" anymore \u2014 if it ever was. Not getting a tenure-track gig doesn't mean you're not \"good people.\" Further, a good PhD program *should* instill skills and modes of thinking that are applicable in a wide range of fields. Finding a job where you make a good living, can choose where you live, and have an appropriate work/life balance is a success, *even if the job is outside the field in which you specialized in grad school.*\n\nGet as much info as you can. If you have students, share it with them. And then keep your options open. Music theorists are, if anything, people who can think creatively and find patterns behind the mystery. Let's use those skills to help ourselves, and each other, make a go of this thing called life after graduate school.\n",
                        "html": "",
                        "image": "/content/images/piano.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "So you want to be a music theory professor...",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-03-18 13:14:00 -0700",
                        "created_by": 1,
                        "updated_at": "2016-03-18 13:14:00 -0700",
                        "updated_by": 1,
                        "published_at": "2016-03-18 13:14:00 -0700",
                        "published_by": 1,
                        "og_title": "So you want to be a music theory professor...",
                        "twitter_title": "So you want to be a music theory professor...",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/piano.jpg",
                        "twitter_image": "/content/images/piano.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Uncanny Number Six (the music of Battlestar Galactica)",
                        "slug": "battlestar-galactica",
                        "markdown": "\nI just finished watching the first season of the 2005 remake of Battlestar Galactica. Generally the music stays out of the way, especially compared with the 1978 original. However, there is one interesting feature in Bear McCreary's music for the remake that has stood out to me. Whenever the Cylon \"Number Six\" appears, a musical motive also appears. This motive alternates the tonic major triad (the main, stable, \"home\" chord for a musical key) with a major flat-VI chord (a major chord built on the sixth note of the minor scale). This is both really interesting imagery, and a substantial missed opportunity. Here's what I mean...\n\nFirst the imagery. The alternation of \"home\" (the tonic chord) with a VI chord is a fairly obvious reference to Number Six as she interacts with a human. There is also an interesting sense of mixture here. By using major flat-VI instead of the minor vi chord that normally comes in a major key, McCreary is mixing major and minor \u2015 what music theorists call *modal mixture*. The mixture of major and minor is a great accompaniment to this new breed of Cylon \u2015 a machine at its core, but fully human in appearance. In other words, *uncanny*.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YkOjeYY6vxc\" frameborder=\"0\" allowfullscreen></iframe>\n\nThe uncanny has a long tradition in musical drama. In particular, theorist [Richard Cohn](https://www.jstor.org/stable/10.1525/jams.2004.57.2.285?seq=1#page_scan_tab_contents) has pointed to one very prominent musical motive that repeatedly appears in support of the uncanny in the music of Richard Wagner: the alternation of major tonic with *minor flat-VI* \u2015 just one note different from McCreary's Number Six motive. There are a number of reasons why I \u2013 &#9837;vi represents the uncanny well. Like I \u2013 &#9837;VI, it mixes major and minor. Also, as Cohn points out, both chords contain the other's *leading tone* \u2015 the note that more than any other in a musical key says \"please resolve me!\" \u2015 making it difficult to discern which chord is the \"real\" tonic chord, the stable chord that resolves all harmonic tension. (This leading tone is the one note missing from McCreary's Number Six motive, which uses major flat-VI.)\n\nCohn also points out that this uncanny motive, while mixing aspects of major and minor, belongs to a scale that is neither major nor minor. The six notes of the tonic and minor-flat-VI chords make up what is known as the [*hexatonic scale*](http://openmusictheory.com/atonal.html). In addition to mixing aspects of major and minor, the hexatonic scale is perfectly symmetrical. And [symmetrical scales](http://openmusictheory.com/symmetryAndCentricity.html) are often used in music with ambiguity or uncertainty, as their symmetry makes multiple tonics (multiple stable points of arrival) equally likely. Perfect for the uncanny. Since these chords form compliments in the hexatonic scale and each contain the other's leading tones, Cohn calls this chord pairing [*hexatonic poles*](http://muse.jhu.edu/login?auth=0&type=summary&url=/journals/opera_quarterly/v022/22.2cohn.pdf).\n\nOf course, McCreary can compose whatever he likes. And by using modal mixture, an unstable \"asymmetrical\" meter, and prominent VI chords, he can effectively portray the uncanny while making a clear \"Number Six\" reference to those who know basic music theory. However, if he had changed *just one note*, he could have amplified the uncanny effect and connected to a larger musical tradition. Even more, McCreary could have added to the six-ness and Cylon-ness of his Number Six motive by using hexatonic poles \u2015 an unstable, artificial, six-note collection.\n\nThe Number Six motive works. But with one small change, McCreary could have given Battlestar Galactica nerds so much more fodder for their nerdiness. Oh well. This blog post will have to do. :p\n",
                        "html": "",
                        "image": "/content/images/bsgNumberSix.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Uncanny Number Six (the music of Battlestar Galactica)",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-12-31 11:22:00 -0700",
                        "created_by": 1,
                        "updated_at": "2015-12-31 11:22:00 -0700",
                        "updated_by": 1,
                        "published_at": "2015-12-31 11:22:00 -0700",
                        "published_by": 1,
                        "og_title": "Uncanny Number Six (the music of Battlestar Galactica)",
                        "twitter_title": "Uncanny Number Six (the music of Battlestar Galactica)",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/bsgNumberSix.png",
                        "twitter_image": "/content/images/bsgNumberSix.png"
                    },
                    {
                        "id": 0,
                        "title": "Installing a Known blog on a private server",
                        "slug": "installing-a-known-blog-on-a-private-server",
                        "markdown": "\nI've been really getting into the IndieWeb lately. I've always been a fan of open-source software ([and textbooks!](http://openmusictheory.com)), but there are some cool new tools for [reclaiming your domain](https://reclaimhosting.com/) and taking ownership of your social media data. One thing I've been using a lot lately is the [Known](http://withknown.com) web publishing platform. Back in February, [I wrote a bit about it](http://kris.shaffermusic.com/2015/02/my-new-social-media-posse/):\n\n> Known is a blog-like, social-media-like platform designed with [POSSE](https://indiewebcamp.com/POSSE) in mind: Publish on your Own Site and Syndicate Elsewhere \u2014 a growing trend on the [IndieWeb](https://indiewebcamp.com). Known double-publishes on Tiwtter (and other platforms) and uses [webmentions](http://indiewebcamp.com/Webmention) to collect the ensuing conversations onto the original Known site. ([Bridgy](http://www.brid.gy) helps, too.) It also differentiates Tweet-like status updates from Facebook-like mini-blog entires without imposing character limits. It also integrades with social media conversations and @-replies pretty well. In short, it's a pretty smooth way to own and control your content while connecting on proprietary social media networks.\n\nAnyone can set up a Known blog for free at [withknown.com](http://withknown.com), but it has a limited feature set. For the full-featured \"Known Pro,\" you either have to pay for a pro account or download the open-source software for free and install it on your server.\n\nIn the past few months, I've installed and maintain several Known sites: [my POSSE site](http://sketches.shaffermusic.com), [a class blog](http://cubouldertheory.shaffermusic.com), [a freelance writing portfolio](http://portfolio.shaffermusic.com) (which I'm having a hard time keeping up-to-date!), and [a collaborative research blog](http://liederproject.shaffermusic.com). Every time I install Known, though, I miss a step \u2014 or I find something that I wish where in their documentation. So I thought that while I put together a community blog for my upcoming course on [The Flipped Classroom](http://www.digitalpedagogylab.com/blog/course/the-flipped-classroom/), I'd write up the steps that I go through in installing a Known site.\n\n# Initial setup\n\nIn its current version, Known has to be installed on a root domain (example.com) or a subdomain (known.example.com), not in a folder (example.com/known). A subdomain is the most common setup (and what I've done every time), so the following instructions will assume an installation in a subdomain.\n\n## Subdomain setup\n\nLogin to your web host or registrar \u2014 wherever you setup your Domain Name Servers (DNS) \u2014 and create an **A record** that points the subdomain you want to your server's IP address. (Here are [Digital Ocean's detailed instructions](https://www.digitalocean.com/community/tutorials/how-to-set-up-and-test-dns-subdomains-with-digitalocean-s-dns-panel) for setting up subdomains using their DNS tool. Every web host uses a different interface, but the technical details are the same.)\n\nIn Blue Host (Digital Pedagogy Lab's web host), the DNS manager looks like this:\n\n![](/images/bluehostAdd.png)\n\nI enter \"flipclass\" for the Host Record, and my server's IP address for \"Points to\" and then click \"add record\". Note that while it says it will take up to four hours for propagation, a new *subdomain* should be ready to go in a matter of seconds, if the root domain is already active.\n\nThe new subdomain should now show up in the ZONE file or the subdomain list.\n\n![](/images/bluehostDone.png)\n\nYou can also open a terminal and ping the server to make sure it's working properly.\n\n~~~ bash\nping flipclass.digitalpedagogylab.com\n~~~\n\n## Prepping the server\n\n*This is where I always get mixed up.*\n\nThe next step is setting up the server itself. I run a Debian Linux virtual private server with Apache 2. The following instructions should work for any Debian-based Linux setup, like Ubuntu \u2014 in fact, I figured out the steps I need to take from [Digital Ocean's Ubuntu tutorial](https://www.digitalocean.com/community/tutorials/how-to-set-up-apache-virtual-hosts-on-ubuntu-12-04-lts).\n\nFirst, if you don't have [the Apache server application](http://httpd.apache.org/) installed, you need to do so. Login to your server in a terminal via SSH, and enter:\n\n~~~ bash\nsudo apt-get install apache2\n~~~\n\nOnce Apache is up and running, make a directory in which to install Known. It's best to keep this with other websites and away from any user data that may be frequently changed or deleted. Since I have root access, I use /var/www, but you can also run it from a user folder.\n\n~~~ bash\nsudo mkdir -p /var/www/flipclass.digitalpedagogylab.com/public_html\n~~~\n\n(Note that you should replace flipclass.digitalpedagogylab.com with your site's URL.)\n\nNow we need to set permissions. This is where I've found I need to break from Digital Ocean's instructions in order to get Known to work properly. When setting permissions, I grant my user ownership (for security reasons, I'll use \"myuser\" as my fake user name in this tutorial), but I give the directory to the www-data group. This allows the app itself to make changes on the fly.\n\n~~~ bash\nsudo chown -R myuser:www-data /var/www/flipclass.digitalpedagogylab.com/public_html\n~~~\n\nThen make sure that everyone can read the files (in order to see the website).\n\n~~~ bash\nsudo chmod -R 755 /var/www\n~~~\n\nNow create a test page to make sure it's working.\n\n~~~ bash\nsudo touch /var/www/flipclass.digitalpedagogylab.com/public_html/index.html\nsudo vi /var/www/flipclass.digitalpedagogylab.com/public_html/index.html\n~~~\n\n(You can use nano or any other command-line text editor you have installed.)\n\nThen put the following content into that index.html file:\n\n~~~ html\n<html>\n  <head><title>Test</title></head>\n  <body>Yay!</body>\n</html>\n~~~\n\nBefore we can see that test file, though, we need to get Apache running. Otherwise, visiting the site will return an error, or if Apache is running, it will point to the root website on that server (which in my case is [www.shaffermusic.com](http://www.shaffermusic.com), not even associated with Digital Pedagogy Lab!).\n\n## Configuring Apache\n\nBegin by creating an Apache configuration file for the new site.\n\n~~~ bash\nsudo cp /etc/apache2/sites-available/default /etc/apache2/sites-available/flipclass.digitalpedagogylab.com\nsudo vi /etc/apache2/sites-available/flipclass.digitalpedagogylab.com\n~~~\n\nThis will open the site's configuration file. (Again, you can use nano or another command-line editor if you like.) It will look something like this:\n\n~~~ html\n<VirtualHost *:80>\n        ServerAdmin webmaster@localhost\n\n        DocumentRoot /var/www\n        <Directory />\n                Options FollowSymLinks\n                AllowOverride None\n        </Directory>\n        <Directory /var/www/>\n                Options Indexes FollowSymLinks MultiViews\n                AllowOverride None\n                Order allow,deny\n                allow from all\n        </Directory>\n\n        ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/\n        <Directory \"/usr/lib/cgi-bin\">\n                AllowOverride None\n                Options +ExecCGI -MultiViews +SymLinksIfOwnerMatch\n                Order allow,deny\n                Allow from all\n        </Directory>\n\n        ErrorLog ${APACHE_LOG_DIR}/error.log\n\n        # Possible values include: debug, info, notice, warn, error, crit,\n        # alert, emerg.\n        LogLevel warn\n\n        CustomLog ${APACHE_LOG_DIR}/access.log combined\n</VirtualHost>\n~~~\n\nMake the following edits to this file and save it:\n\nUnder the ServerAdmin line, add the following:\n\n~~~ bash\nServerName flipclass.digitalpedagogylab.com\n~~~\n\nNext, change the DocumentRoot line:\n\n~~~ bash\nDocumentRoot /var/www/flipclass.digitalpedagogylab.com/public_html\n~~~\n\nFinally, change all instances of\n\n~~~ bash\nAllowOverride None\n~~~\n\nto read:\n\n~~~ bash\nAllowOverride All\n~~~\n\nNow save and exit. (Esc followed by :x if you are using vi or vim.)\n\n## (Re)start Apache\n\nOnce the configuration file is complete and correct, add the site to Apache and restart the server application:\n\n~~~ bash\nsudo a2ensite flipclass.digitalpedagogylab.com\nsudo service apache2 restart\n~~~\n\nApache will likely throw the following message:\n\n~~~ bash\napache2: Could not reliably determine the server's fully qualified domain name, using 127.0.0.1 for ServerName\n~~~\n\nThat's normal. Don't worry about it.\n\nNow, go to your website address, and you should see the test file created earlier:\n\n~~~ bash\nYay!\n~~~\n\nIf so, you're good to go!\n\n## Installing Known\n\nFirst, we need to get the Known files onto the server. Go to the [Open Source](https://withknown.com/opensource/) page on Known's website. Do *not* click \"Download Known\". Instead, right-click (ctrl-click on a Mac) on \"Download Known\" and click \"Copy Link Address\" (or whatever your browser calls it). Then go back to your server's terminal and use wget and the link you just grabbed from Known's site to copy the zip file to your server directly. (I put it in my user's downloads folder.)\n\n~~~ bash\ncd ~/Downloads\nwget http://assets.withknown.com/releases/known-0.7.8.5.1.zip\nunzip known-0.7.8.5.1.zip -d known\n~~~\n\nNow you'll have a folder in Downloads called known, which contains the entire Known installation. Simply copy it (or move it) to the public_html folder created earlier.\n\n~~~ bash\ncd known/\nsudo cp -R ./* /var/www/flipclass.digitalpedagogylab.com/public_html/\n~~~\n\nNow set permissions on the new files:\n\n~~~ bash\nsudo chown -R myuser:www-data /var/www/flipclass.digitalpedagogylab.com/public_html\nsudo chmod -R 755 /var/www\nsudo chmod -R 775 /var/www/flipclass.digitalpedagogylab.com/public_html/Uploads\n~~~\n\nAnd remove the test index.html file from earlier:\n\n~~~ bash\nsudo rm /var/www/flipclass.digitalpedagogylab.com/public_html/index.html\n~~~\n\nAnd take Known's sample .htaccess file and make it into a real .htaccess file.\n\n~~~ bash\nsudo mv /var/www/flipclass.digitalpedagogylab.com/public_html/htaccess.dist /var/www/flipclass.digitalpedagogylab.com/public_html/.htaccess\n~~~\n\nNow we can use Known's automatic installer. Simply point your browser to the website \u2014 in this case flipcamp.digitalpedagogylab.com. Known will let you know if any prerequisite setup is necessary. Take care of that before proceeding. (My first time, there were a couple things that needed installing before I could continue, but now I'm good to go.)\n\nOn the next page, provide a name for the site in the blank provided. But before going onto the MySQL settings, you need to setup MySQL.\n\n[Install MySQL](https://www.digitalocean.com/community/tutorials/a-basic-mysql-tutorial) and set up an account, if you haven't already. Then do the following to setup a Known database.\n\nLogin to mySQL.\n\n~~~ bash\nmysql -u root -p\n~~~\n\nCreate a database.\n\n~~~ mysql\nCREATE DATABASE flipclass;\n~~~\n\nCreate a database user and grant it privileges (choose a better password than \"password\").\n\n~~~ mysql\nCREATE USER 'myuser'@'localhost' IDENTIFIED BY 'password';\nGRANT ALL PRIVILEGES ON flipclass.* TO 'myuser'@'localhost';\nFLUSH PRIVILEGES;\nexit;\n~~~\n\nFor more details on MySQL users, see this [Digital Ocean tutorial](https://www.digitalocean.com/community/tutorials/how-to-create-a-new-user-and-grant-permissions-in-mysql).\n\nNow add the database settings you just created into the Known setup page. Then click \"Onwards!\"\n\nYou may receive an error \u2014 you *should* receive an error \u2014 if your server is setup securely. Known will be unable to write these settings to a new file, and you need to do it manually.\n\n~~~ bash\nsudo touch /var/www/flipclass.digitalpedagogylab.com/public_html/config.ini\nsudo vi /var/www/flipclass.digitalpedagogylab.com/public_html/config.ini\n~~~\n\nPaste the info Known provided into the file, then save and exit.\n\nThen reload the Known setup page.\n\nIf all was successful, you should be good to go!\n\n## Configure Known\n\nKnown will prompt you to create an adminstrator user. Once you've done so, you can start to configure your site's visual layout and functionality, and start posting content!\n\nI highly recommend connecting Known with Brid.gy, in order to sync with social media services. You will also need to setup an email connection so that Known can forward notifications. Finally, I recommend the Cherwell theme, since it allows you to include a nice background image.\n\nTo cross-post to Facebook, Twitter, Flickr, etc., you can either setup Convoy (a paid service that can be configured within Known), or [download the plugins you want](https://github.com/idno) from GitHub and install them locally. To do so, use wget to download the zip file from GitHub to the server and unzip it (same as with Known itself). Then copy the files into the IdnoPlugins folder in your Known installation, and update the permissions if necessary (see instructions above). Then you can activate then from the Plugins menu within Known.\n\nI hope this tutorial was helpful! At the very least, I now have detailed notes in a single place for the next time I setup a new site!\n",
                        "html": "",
                        "image": "/content/images/cables.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Installing a Known blog on a private server",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2015-07-15 08:54:07 -0600",
                        "created_by": 1,
                        "updated_at": "2015-07-15 08:54:07 -0600",
                        "updated_by": 1,
                        "published_at": "2015-07-15 08:54:07 -0600",
                        "published_by": 1,
                        "og_title": "Installing a Known blog on a private server",
                        "twitter_title": "Installing a Known blog on a private server",
                        "og_description": "",
                        "twitter_description": "",
                        "og_image": "/content/images/cables.jpg",
                        "twitter_image": "/content/images/cables.jpg"
                    },
                    {
                        "id": 0,
                        "title": "Detecting terrorism with AI",
                        "slug": "detecting-terrorism-with-ai",
                        "markdown": "\nCan artificial intelligence identify potential terrorist activity?\n\nThis past week, <a href=\"http://www.bbc.com/news/technology-38992657\" target=\"blank_\">BBC News published an article</a>, \"Facebook algorithms 'will identify terrorists'\". It describes the content of <a href=\"https://www.facebook.com/notes/mark-zuckerberg/building-global-community/10154544292806634\" target=\"blank_\">a 5,500-word letter</a> posted by Facebook head, Mark Zuckerberg, about the future of the company, including the role that machine-learning algorithms will play in filtering and classifying content. The headline, like much journalism surrounding machine learning and artificial intelligence, is overblown and sensationalized. In fact, the quote in the headline isn't even a quote! Zuckerberg never says \"will identify terrorists\" in the letter described in the article.\n\nHowever, Zuckerberg does indicate that Facebook is\n\n> starting to explore ways to use AI to tell the difference between news stories about terrorism and actual terrorist propaganda so we can quickly remove anyone trying to use our services to recruit for a terrorist organization.\n\nTo those worried about the role of artificial intelligence in society, using social media data to identify people who may perpetrate an attack in the future is a frightening proposition. Now, Zuckerberg writes that a system to distinguish news about terrorism from actual terrorist propaganda is \"technically difficult\" and \"will take many years to fully develop\". But <a href=\"https://www.vice.com/en_us/article/this-algorithm-will-try-to-predict-which-gang-threats-on-twitter-turn-into-irl-violence\" style=\"background-color: rgb(255, 255, 255);\" target=\"blank_\">researchers at Columbia university are already working</a> on tools to monitor Twitter activity from known gang members and cross-reference that activity with criminal activity committed by gang members. The hope is that they will uncover patterns in the relationship of social-media data to known crimes that will help them predict, and prevent, future crimes. So it makes sense that law enforcement officials would want to extend this possibility to Facebook, as well as to major terror attacks.\n\nBut is it even possible? And if it is, what ethical limits would we need to put into place?\n\n\n## How machines learn\n\nWhen Zuckerberg talks about using algorithms to parse terrorist propaganda from news about terrorism, he's talking about a *classification algorithm.* A classification algorithm is an example of a *supervised* machine-learning algorithm, meaning that the potential outcomes are known beforehand. Think of Google Books. When Google takes a scan of a book page, its optical character recognition algorithm looks for elements in the image that match closely with one or more *known* possible characters (all the letters A\u2013Z, upper- and lower-case, with various diacriticals, along with numerals, symbols, etc., all in a variety of font types). Facebook's face recognition is similar. It scans an image for objects that fit the known category of *human face*, and then compare those objects to a set of known faces (likely starting with the friends list of the person who uploaded the image).\n\nIn each of these cases, the output categories are known: letters, numbers, and symbols; faces of a set of individuals; etc. And while the specific algorithm used to classify elements as belonging to one of those categories may vary, the general process is the same:\n\n<ul><li>Take a large set of examples where the outcome is already known (such as 1000 images of upper-case A's, 1000 images of upper-case B's, etc.).\n</li><li>Divide the set of examples into *training* data and *test* data (often a 70/30 split).\n</li><li>\"Train\" the algorithm on the training data. Or in statistical terms, fit a model that corresponds to the data.\n</li><li>Test the accuracy of the model by running the test data through it and comparing the algorithm's classification results to the already known answers.\n</li><li>Refine the model and/or augment the dataset to attempt to improve the results (taking care to avoid <a href=\"http://psychology.wikia.com/wiki/P-hacking\" target=\"blank_\">p-hacking</a>).\n</li><li>Lather, rinse, repeat.\n</li></ul>\n\n<img src=\"/content/images/mnistExamples.png\" alt=\"Handwriting recognition examples\"/>\n<p style=\"text-align:center;\"><i>Example handwritten digits from the MNIST dataset.</i></p>\n\nTo build a successful classification algorithm, you need *a lot* of data. For example, the famous <a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"blank_\">MNIST dataset of handwritten digits</a> contains 70,000 images of handwritten digits (0\u20139). That's 70,000 images (60,000 to train, 10,000 to test) of just ten potential output categories! (And let's say you develop an algorithm that performs at a very high 99% accuracy level. That still means that you misclassified 100 images!)\n\nLet's apply this to gang crime. In a simplified version of the algorithm, the output would be *crime committed (true/false)*, and the input would be the amount and kind of social media activity from known members of a gang. The input dataset would come from mining the social media activity of known gang members, and it would be cross-referenced with police reports, charges filed, and/or convictions won against gang members. Split the data into training and testing data, train the algorithm to recognize patterns in social media activity that strongly associates with the presence/absence of criminal activity, test those patterns on the remaining data, and determine the accuracy of the crime-predicting algorithm.\n\nPredicting crime from tweets is far more complex than recognizing handwritten digits! But there's also a lot of data that comes with those tweets: text content, user ID, other tagged individuals, timestamps, language, device used (mobile vs. desktop), and sometimes the user's location. And while there may not be 70,000 examples of crimes committed by an individual, or even a single gang, <a href=\"http://laalmanac.com/crime/cr03x.htm\" target=\"blank_\">the number of crimes committed by gangs in a large city each year</a> does number in the thousands, and they come in clear, legally delineated categories like homicide, robbery, burglary, assault, etc. So while predicting crime from tweets is not without its technical and ethical challenges, it is conceivably possible, and <a href=\"https://www.theguardian.com/cities/2014/jun/25/predicting-crime-lapd-los-angeles-police-data-analysis-algorithm-minority-report\" target=\"blank_\">law enforcement has already begun to attempt it</a>.\n\n\n## Predicting terrorism\n\nTerrorism is different. Despite the broad surveillance activity uncovered by whistleblowers like Ed Snowden, the potential dataset for predicting terror activity from things like social media activity is much smaller. Between 2001 and 2011, <a href=\"https://www.theguardian.com/news/datablog/2013/apr/17/four-decades-us-terror-attacks-listed-since-1970\" target=\"blank_\">there were 207 terror attacks in the United States</a>, far fewer than the number of gang-related crimes in one year in a large city like Los Angeles or Chicago. And there are many more known gang members than known terrorists in the US, making gang social-media activity much easier to monitor and analyze than terrorist activity.\n\nThere's a common maxim in data science: *a simple algorithm with a lot of data beats a complex algorithm with less data*. Or put another way, *big data beats sophisticated algorithms*. But what happens when you don't have enough data? You have to rely on a more sophisticated algorithm. That means you need experts to provide information and insights that can fill in the gap of what the data would tell you on its own. In academic terms, you need *theory* to stand in for lack of empirical observation. This increases the risk of human error like bias and conclusions drawn from incomplete information, which accompanies the already elevated risk of statistical error (variance due to random chance).\n\nThis increased potential for error means a higher likelihood of incorrect predictions: terror attacks not predicted and erroneous accusations directed at law-abiding individuals. Without a major breakthrough in the math, the technical solution to this problem is to gather more data. Translation: wait for more terrorist attacks. That's untenable.\n\nParsing the difference between propaganda and news is a whole other beast. Mathematically speaking, this is doable. There are copious examples of both, and there are many experts in the world who could advise Facebook (or others working on it) on features for the algorithm to hone in on. It's clear from recent events that Facebook's engineers aren't there yet. But this is conceivably possible.\n\n## The case of Napalm Girl\n\nFacebook already uses filters to detect objectionable content, though. And it clearly doesn't work as desired. Recently, Facebook censored the famous, Pulitzer-Prize-winning photograph, \"Napalm Girl\". In response to complaints, <a href=\"https://arstechnica.com/tech-policy/2016/09/facebook-napalm-girl-photo-censorship-norway/\" target=\"blank_\">Facebook told ArsTechnica</a>:\n\n> While we recognise that this photo is iconic, it\u2019s difficult to create a distinction between allowing a photograph of a nude child in one instance and not others.\n\nAt the time, Facebook treated this as a *policy* issue. But it's really a *scale* issue, and therefore a *machine learning* issue.\n\nThe problem is that for most people, there are contexts in which an image of a nude child is acceptable and those in which it is not. And legally, there are contexts in which it is felonious and contexts in which it is not. The amount of potentially objectionable images posted to Facebook is mammoth. To detect and filter objectionable images from the platform, Facebook must either hire a large number of people to view images and approve/block them (and quickly!), they must train a machine to do it automatically, or they must train a machine to do some automatically and flag others for human review. They currently do some of each, and according to Zuckerberg's recent letter, they want to automate more of the process.\n\nLet's think about this from a machine-learning perspective. If you want to train a machine to classify an image of a child as acceptable or unacceptable, the mathematically preferred way would be to assemble a large collection of both objectionable and innocuous images and use them as you train, test, and refine your algorithm. But collecting and using a large set of images of children, some of which are pornographic, would both be both illegal and reprehensible.\n\nInstead, you could use a more complex system. For example, combining a well trained classifier for children's faces with one that detects the presence/absence of clothing in an image. Such a combination could, conceivably, detect child nudity. But how can you use an algorithm to parse the nuanced boundaries between editorial journalism like \"Napalm Girl\" and child pornography? To my understanding, given the current state of machine learning, you can't.\n\n## Machine learning as stereotyping\n\nThere's another ethical issue regarding Facebook's use of machine learning to filter content for terrorism or other criminal activity. Machine-learning classifiers are meant to discover *general* trends. They are statistical approaches, not individualistic.\n\nWhen Netflix and Pandora serve up movie and music recommendations, they don't do so by getting to know you, but by using the watching/listening histories and ratings of millions of customers to identify a number of *clusters* of people with similar tastes. They identify the cluster(s) to which you belong, and then recommend that you watch or listen to things you've haven't seen or heard before that are liked by others in your cluster. While it may be more \"personalized\" to receive music and movie recommendations based on that clustering, rather than more general averages, you are still being treated as a member of a class \u2015 <a href=\"https://blog.dominodatalab.com/video-how-machine-learning-amplifies-societal-privilege/\" target=\"blank_\">a stereotype</a> \u2015 not as an individual.\n\nWhen predicting what song someone might want to hear next, this is no big deal. But when predicting crime or terrorism, it's a huge problem. If Pandora serves up a song I don't like, I tap the thumbs-down icon, the model learns, and Pandora gives me a new song. (And usually they follow a thumbs-down with a song I've already given a thumbs-up, so I'm less likely to close the app in frustration after three or four bad songs in a row.) But when the FBI predicts I'm about to commit mass murder, and therefore they increase surveillance on me, intercept my mail, tap my phone, monitor my electronic communications, visit my employer ... it's a violation of my constitutional rights and a waste of law enforcement resources, not to mention a stain on my reputation if word gets out.\n\n## \"Artificial intelligence\" or applied statistics?\n\nMachine learning can do some really cool things! That's why I like it. But it's not really \"artificial intelligence\", as many journalists \u2015 and even Mark Zuckerberg \u2015 characterize it. Machine learning is simply statistics, conducted computationally, and often applied at unprecedented scales. Like all statistics, there will always be error, and at large scales, that error, left unchecked, brings potential for enormous human and environmental consequences.\n\nSo by all means, Facebook, improve your filters and your classifiers. But we must constantly be examining the ethical implications and the unintended consequences of our technological work, especially as we use technology at large scales in new domains like this. Let's hope that's part of the \"technically difficult\" work they'll be doing over the next few years.\n\n<i>Featured image by <a href=\"https://www.flickr.com/photos/nnova/4060530995/\" target=\"blank_\">Nicolas Nova</a>.</i>\n",
                        "html": "",
                        "image": "/content/images/panopticon.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Detecting terrorism with AI",
                        "meta_description": "Can artificial intelligence identify potential terrorist activity on Facebook?",
                        "author_id": 1,
                        "created_at": "2017-02-20 13:31:00 -0500",
                        "created_by": 1,
                        "updated_at": "2017-02-20 13:31:00 -0500",
                        "updated_by": 1,
                        "published_at": "2017-02-20 13:31:00 -0500",
                        "published_by": 1,
                        "og_title": "Detecting terrorism with AI",
                        "twitter_title": "Detecting terrorism with AI",
                        "og_description": "Can artificial intelligence identify potential terrorist activity on Facebook?",
                        "twitter_description": "Can artificial intelligence identify potential terrorist activity on Facebook?",
                        "og_image": "/content/images/panopticon.jpg",
                        "twitter_image": "/content/images/panopticon.jpg"
                    }
                ]
            }
        }
    ]
}